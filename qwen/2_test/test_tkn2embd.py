#!/usr/bin/env python3
"""
í† í¬ë‚˜ì´ì € â†’ ì„ë² ë”© ë§¤í•‘ í…ŒìŠ¤íŠ¸
í•™ìŠµ ì „í›„ ë¹„êµ ê°€ëŠ¥
"""

import os
import torch
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM
# from unsloth import FastLanguageModel
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

class TokenEmbeddingTester:
    """í† í° ì„ë² ë”© ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, model_path=None, tokenizer_path=None):
        """
        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ì´ˆê¸° ìƒíƒœ í…ŒìŠ¤íŠ¸)
            tokenizer_path: í† í¬ë‚˜ì´ì € ê²½ë¡œ
        """
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.model = None
        self.tokenizer = None
    
    def load_model(self):
        """ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print("="*80)
        print("ëª¨ë¸ ë¡œë”©")
        print("="*80)
        
        if self.model_path:
            # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
            print(f"í•™ìŠµëœ ëª¨ë¸: {self.model_path}")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
        else:
            # â­ Unsloth ëŒ€ì‹  Transformers ì§ì ‘ ì‚¬ìš©
            print("ì´ˆê¸° ëª¨ë¸: Qwen (BF16 ë²„ì „)")
            
            # âš ï¸ FP8 ë²„ì „ ëŒ€ì‹  ì›ë³¸ BF16 ë²„ì „ ì‚¬ìš©
            self.model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-32B-Instruct",  # FP8 ì•„ë‹Œ ì›ë³¸
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )
            
            # í•œêµ­ì–´ ìµœì í™” í† í¬ë‚˜ì´ì €
            tokenizer_name = self.tokenizer_path or os.getenv("TOKENIZER")
            if not tokenizer_name:
                raise ValueError("TOKENIZER í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_name,
                trust_remote_code=True
            )
            
            # ì„ë² ë”© í¬ê¸° ì¡°ì •
            original_size = self.model.get_input_embeddings().weight.shape[0]
            new_size = len(self.tokenizer)
            
            if original_size != new_size:
                print(f"ì„ë² ë”© í¬ê¸° ì¡°ì •: {original_size:,} â†’ {new_size:,}")
                self.model.resize_token_embeddings(new_size)
        
        print("âœ“ ë¡œë“œ ì™„ë£Œ")
        print(f"  Vocab size: {len(self.tokenizer):,}")
        print(f"  Embedding shape: {self.model.get_input_embeddings().weight.shape}")
        print("="*80)
    
    def test_token_to_embedding(self, text_samples):
        """í† í° â†’ ì„ë² ë”© ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ” í† í° â†’ ì„ë² ë”© ë§¤í•‘ í…ŒìŠ¤íŠ¸")
        print("="*80)
        
        embed_layer = self.model.get_input_embeddings()
        
        for text in text_samples:
            print(f"\nğŸ“ í…ìŠ¤íŠ¸: '{text}'")
            print("-"*80)
            
            # í† í¬ë‚˜ì´ì§•
            tokens = self.tokenizer(text, return_tensors="pt")
            token_ids = tokens["input_ids"][0]
            
            print(f"í† í° ê°œìˆ˜: {len(token_ids)}")
            
            # ê° í† í° ì •ë³´
            for idx, token_id in enumerate(token_ids):
                token_id = token_id.item()
                token_str = self.tokenizer.decode([token_id])
                
                # ì„ë² ë”© ë²¡í„° (BFloat16 â†’ Float32 â†’ NumPy)
                embedding = embed_layer.weight[token_id].detach().cpu().float().numpy()
                
                print(f"\n  [{idx}] Token ID: {token_id}")
                print(f"      Token: '{token_str}'")
                print(f"      Embedding í†µê³„:")
                print(f"        í‰ê· : {embedding.mean():.6f}")
                print(f"        í‘œì¤€í¸ì°¨: {embedding.std():.6f}")
                print(f"        ìµœì†Œ: {embedding.min():.6f}")
                print(f"        ìµœëŒ€: {embedding.max():.6f}")
    
    def compare_token_embeddings(self, token1, token2):
        """ë‘ í† í°ì˜ ì„ë² ë”© ìœ ì‚¬ë„ ë¹„êµ"""
        print("\n" + "="*80)
        print(f"ğŸ”¬ í† í° ì„ë² ë”© ìœ ì‚¬ë„: '{token1}' vs '{token2}'")
        print("="*80)
        
        embed_layer = self.model.get_input_embeddings()
        
        # í† í° ID
        token1_id = self.tokenizer.encode(token1, add_special_tokens=False)[0]
        token2_id = self.tokenizer.encode(token2, add_special_tokens=False)[0]
        
        # ì„ë² ë”© (BFloat16 â†’ Float32 â†’ NumPy)
        emb1 = embed_layer.weight[token1_id].detach().cpu().float().numpy()
        emb2 = embed_layer.weight[token2_id].detach().cpu().float().numpy()
        
        # ìœ ì‚¬ë„
        similarity = cosine_similarity([emb1], [emb2])[0][0]
        distance = np.linalg.norm(emb1 - emb2)
        
        print(f"Token 1: '{token1}' (ID: {token1_id})")
        print(f"  í‰ê· : {emb1.mean():.6f}, í‘œì¤€í¸ì°¨: {emb1.std():.6f}")
        print(f"\nToken 2: '{token2}' (ID: {token2_id})")
        print(f"  í‰ê· : {emb2.mean():.6f}, í‘œì¤€í¸ì°¨: {emb2.std():.6f}")
        print(f"\nì½”ì‚¬ì¸ ìœ ì‚¬ë„: {similarity:.6f}")
        print(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬: {distance:.6f}")
        
        if similarity > 0.9:
            print("â†’ ë§¤ìš° ìœ ì‚¬í•¨ âœ…")
        elif similarity > 0.7:
            print("â†’ ìœ ì‚¬í•¨")
        elif similarity > 0.3:
            print("â†’ ì•½ê°„ ìœ ì‚¬í•¨")
        else:
            print("â†’ ê±°ì˜ ë‹¤ë¦„ âŒ")
    
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("\n" + "ğŸ”¬"*40)
    print("í† í° â†’ ì„ë² ë”© ë§¤í•‘ í…ŒìŠ¤íŠ¸ (í•™ìŠµ ì „)")
    print("ğŸ”¬"*40)
    
    # .env íŒŒì¼ì—ì„œ TOKENIZER í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    tester = TokenEmbeddingTester(
        model_path=None,
        tokenizer_path=None  # .envì˜ TOKENIZER ì‚¬ìš©
    )
    
    tester.load_model()
    
    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    test_samples = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ë°ì´í„° ë¶„ì„",
        "ì¸ê³µì§€ëŠ¥",
    ]
    tester.test_token_to_embedding(test_samples)
    
    # ìœ ì‚¬ë„ ë¹„êµ
    tester.compare_token_embeddings("ë°ì´í„°", "ë¶„ì„")
    tester.compare_token_embeddings("ì•ˆë…•", "ê°ì‚¬")
    
    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)


if __name__ == "__main__":
    main()