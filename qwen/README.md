# Qwen 2.5-14B Korean Fine-tuning

> **Two-Stage Training Pipeline**: General Korean â†’ WMS Domain Specialization

[![GitHub](https://img.shields.io/badge/GitHub-EnzoMH%2Fft__llm-blue)](https://github.com/EnzoMH/ft_llm)
[![Model](https://img.shields.io/badge/Model-Qwen%202.5--14B-green)](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
[![License](https://img.shields.io/badge/License-CC--BY--NC--4.0-red)](https://creativecommons.org/licenses/by-nc/4.0/)

---

## Table of Contents

- [English Documentation](#english-documentation)
  - [Overview](#overview)
  - [Key Features](#key-features)
  - [Project Structure](#project-structure)
  - [Quick Start](#quick-start)
  - [Training Details](#training-details)
- [í•œêµ­ì–´ ë¬¸ì„œ](#í•œêµ­ì–´-ë¬¸ì„œ)
  - [ê°œìš”](#ê°œìš”)
  - [ì£¼ìš” íŠ¹ì§•](#ì£¼ìš”-íŠ¹ì§•)
  - [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
  - [ë¹ ë¥¸ ì‹œìž‘](#ë¹ ë¥¸-ì‹œìž‘)
  - [í›ˆë ¨ ìƒì„¸](#í›ˆë ¨-ìƒì„¸)

---

# English Documentation

## Overview

This project fine-tunes **Qwen 2.5-14B-Instruct** with **Korean-optimized tokenizer** using a two-stage training strategy:

1. **Stage 1**: General Korean language learning (54K samples)
2. **Stage 2**: WMS domain specialization (20K samples)

---

## Key Features

### **Model**
- **Qwen 2.5-14B-Instruct**: Latest model with 14B parameters
- **INT8 Quantization**: Memory-efficient training
- **LoRA Fine-tuning**: r=128, alpha=256
- **Unsloth Optimization**: 2-5x faster training

### **Dataset**
- **Stage 1**: 54,190 high-quality Korean samples (HuggingFace)
- **Stage 2**: 20,000 WMS domain QA pairs (Local)
  - EEVE template format
  - Formal Korean style (~ìŠµë‹ˆë‹¤, ~ìž…ë‹ˆë‹¤)
  - RAG-generated answers with domain expertise

### **Training Infrastructure**
- **GPU**: 2Ã— NVIDIA H100 80GB HBM3
- **CPU**: 16 cores @ 192GB RAM
- **Framework**: Unsloth + PyTorch 2.8 + Transformers 4.57
- **Monitoring**: Real-time CPU/RAM/GPU tracking

---

## Project Structure

```
qwen/
â”œâ”€â”€ 0_qwen_ft_us.py              # Stage 1: General Korean training
â”œâ”€â”€ 1_qwen_ft_wms.py             # Stage 2: WMS domain training
â”œâ”€â”€ 2_ul_hf_tknzr.py             # Korean tokenizer uploader
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ 3_use/                       # Usage & Monitoring
â”‚   â””â”€â”€ monitor_training.py      # Log-based training monitor
â”‚
â”œâ”€â”€ z_util/                      # Utilities
â”‚   â”œâ”€â”€ cpu_mntrg.py             # CPU/RAM monitoring
â”‚   â”œâ”€â”€ gpu_mnrtg.py             # GPU/VRAM monitoring
â”‚   â””â”€â”€ local_dataset_loader.py  # Local JSON dataset loader
â”‚
â”œâ”€â”€ qwen-KR-14B-output-unsloth/  # Training output (Stage 1)
â”‚   â””â”€â”€ final/                   # Final LoRA adapters
â”‚
â””â”€â”€ training.log                 # Real-time training logs (not in Git)
```

---

## Quick Start

### Prerequisites

```bash
# Install dependencies
pip install unsloth[cu128-torch280]
pip install transformers==4.57.0
pip install datasets
pip install bitsandbytes
pip install peft
pip install trl
pip install python-dotenv

# Create .env file (DO NOT commit to Git!)
cat > .env << EOF
HF_TOKEN=your_hf_token_here
TOKENIZER=your_tokenizer_name_here
EOF
```

### Stage 1: General Korean Training

```bash
cd /home/work/tesseract/qwen
python 0_qwen_ft_us.py
```

**Training Configuration:**
- Dataset: 54,190 Korean samples (HuggingFace)
- Model: Qwen 2.5-14B-Instruct + INT8 quantization
- LoRA: r=128, alpha=256, dropout=0.0
- Batch: 4 Ã— 4 accumulation = 16 effective
- Epochs: 3, Learning Rate: 5e-5
- Time: ~13 hours (2Ã— H100)

**Output:** `qwen-KR-14B-output-unsloth/final/`

### Stage 2: WMS Domain Training

```bash
cd /home/work/tesseract/qwen
python 1_qwen_ft_wms.py
```

**Training Configuration:**
- Dataset: 20,000 WMS QA pairs (Local)
- Base: Stage 1 output model
- LoRA: r=64, alpha=128 (smaller for fine-tuning)
- Epochs: 2, Learning Rate: 2e-5 (lower)
- Time: ~6 hours (2Ã— H100)

**Output:** `qwen-WMS-14B-output-unsloth/final/`

---

## Training Details

### Stage 1: General Korean

**Objective:** Build strong Korean language foundation

```python
# Key configurations
base_model = "Qwen/Qwen2.5-14B-Instruct"
tokenizer_name = os.getenv("TOKENIZER")  # From .env file
dataset_name = "your_username/korean-quality-dataset"

# LoRA
lora_r = 128
lora_alpha = 256
lora_dropout = 0.0
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]
modules_to_save = ["embed_tokens", "lm_head"]  # Embedding layer training

# Training
num_train_epochs = 3
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 5e-5
warmup_ratio = 0.1
max_seq_length = 4096
```

**Expected Loss:**
- Initial: ~2.5-4.0
- Epoch 1: ~1.5-2.5
- Epoch 3: **0.8-1.5** âœ…

### Stage 2: WMS Domain

**Objective:** Specialize in WMS domain knowledge

```python
# Key configurations (differences from Stage 1)
base_model = "qwen-KR-14B-output-unsloth/final"  # Stage 1 output
dataset_name = "local"  # 20K WMS QA pairs

# LoRA (smaller)
lora_r = 64
lora_alpha = 128

# Training (gentler)
num_train_epochs = 2
learning_rate = 2e-5  # Lower LR
warmup_ratio = 0.05
```

---

## Monitoring

### Real-time Monitoring

```bash
# Watch training progress
tail -f training.log

# Check GPU usage
nvidia-smi -l 1

# Generate training plots (after training)
cd 3_use
python monitor_training.py --log ../training.log --output plots
```

### Key Metrics

- **Loss**: Should decrease from ~3.0 to ~1.0-1.5
- **Eval Loss**: Should be within 0.1-0.3 of train loss
- **GPU Utilization**: ~46% (INT8 quantization)
- **VRAM Usage**: ~18GB per GPU

---

## Troubleshooting

### 1. CUDA Out of Memory

```python
# Reduce batch size
per_device_train_batch_size = 2  # 4 â†’ 2
gradient_accumulation_steps = 8  # 4 â†’ 8
```

### 2. Pickle Error (dataset.map)

Already fixed in `0_qwen_ft_us.py` by using direct Python loops instead of `dataset.map()`.

### 3. Tokenizer Embedding Mismatch

Automatically handled by `resize_token_embeddings()` with smart initialization.

---

## Next Steps

### 1. Model Merging

```python
from unsloth import FastLanguageModel

model.save_pretrained_merged(
    "qwen-14B-wms-merged",
    tokenizer,
    save_method="merged_16bit"
)
```

### 2. HuggingFace Upload

```python
# Upload LoRA adapters (recommended)
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="qwen-KR-14B-output-unsloth/final",
    repo_id="your_username/qwen-14B-korean-lora",
    token="your_hf_token"
)
```

### 3. vLLM Deployment

```bash
vllm serve qwen-14B-wms-merged \
    --dtype bfloat16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

---

## License

This project is licensed under **CC-BY-NC-4.0** (Creative Commons Attribution-NonCommercial 4.0 International).

- **Allowed**: Research, Education, Personal Use
- **Prohibited**: Commercial Use
- **Required**: Attribution to original authors

Base model follows: [Qwen 2.5 License](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)

---

## Acknowledgments

- **Alibaba Qwen Team**: Qwen 2.5 model
- **Unsloth Team**: Fast training optimization
- **HuggingFace**: Transformers ecosystem

---

# í•œêµ­ì–´ ë¬¸ì„œ

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **Qwen 2.5-14B-Instruct** ëª¨ë¸ì„ **í•œêµ­ì–´ ìµœì í™” í† í¬ë‚˜ì´ì €**ì™€ í•¨ê»˜ íŒŒì¸íŠœë‹í•˜ëŠ” 2ë‹¨ê³„ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ìž…ë‹ˆë‹¤:

1. **1ë‹¨ê³„**: ì¼ë°˜ í•œêµ­ì–´ í•™ìŠµ (54K ìƒ˜í”Œ)
2. **2ë‹¨ê³„**: WMS ë„ë©”ì¸ íŠ¹í™” (20K ìƒ˜í”Œ)

---

## ì£¼ìš” íŠ¹ì§•

### ðŸš€ **ëª¨ë¸**
- **Qwen 2.5-14B-Instruct**: 140ì–µ íŒŒë¼ë¯¸í„°ì˜ ìµœì‹  ëª¨ë¸
- **INT8 ì–‘ìží™”**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í›ˆë ¨
- **LoRA íŒŒì¸íŠœë‹**: r=128, alpha=256
- **Unsloth ìµœì í™”**: 2-5ë°° ë¹ ë¥¸ í›ˆë ¨ ì†ë„

### **í† í¬ë‚˜ì´ì €**
- **í•œêµ­ì–´ ìµœì í™”**: ì»¤ìŠ¤í…€ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í†µí•©
- **í† í° íš¨ìœ¨**: Qwen ê¸°ë³¸ ëŒ€ë¹„ 10-20% í–¥ìƒ
- **ìŠ¤ë§ˆíŠ¸ ì´ˆê¸°í™”**: ìƒˆ ìž„ë² ë”©ì„ ê¸°ì¡´ í‰ê· ê°’ìœ¼ë¡œ ì´ˆê¸°í™”

### ðŸ“Š **ë°ì´í„°ì…‹**
- **1ë‹¨ê³„**: 54,190ê°œ ê³ í’ˆì§ˆ í•œêµ­ì–´ ìƒ˜í”Œ (HuggingFace)
- **2ë‹¨ê³„**: 20,000ê°œ WMS ë„ë©”ì¸ QA íŽ˜ì–´ (ë¡œì»¬)
  - EEVE í…œí”Œë¦¿ í˜•ì‹
  - ê²©ì‹ì²´ í•œêµ­ì–´ (~ìŠµë‹ˆë‹¤, ~ìž…ë‹ˆë‹¤)
  - RAG ê¸°ë°˜ ë„ë©”ì¸ ì „ë¬¸ ë‹µë³€

### ðŸ› ï¸ **í›ˆë ¨ ì¸í”„ë¼**
- **GPU**: 2Ã— NVIDIA H100 80GB HBM3
- **CPU**: 16ì½”ì–´ @ 192GB RAM
- **í”„ë ˆìž„ì›Œí¬**: Unsloth + PyTorch 2.8 + Transformers 4.57
- **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ CPU/RAM/GPU ì¶”ì 

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
qwen/
â”œâ”€â”€ 0_qwen_ft_us.py              # 1ë‹¨ê³„: ì¼ë°˜ í•œêµ­ì–´ í›ˆë ¨
â”œâ”€â”€ 1_qwen_ft_wms.py             # 2ë‹¨ê³„: WMS ë„ë©”ì¸ í›ˆë ¨
â”œâ”€â”€ 2_ul_hf_tknzr.py             # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì—…ë¡œë”
â”œâ”€â”€ README.md                    # ì´ íŒŒì¼
â”‚
â”œâ”€â”€ 3_use/                       # ì‚¬ìš© ë° ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ monitor_training.py      # ë¡œê·¸ ê¸°ë°˜ í›ˆë ¨ ëª¨ë‹ˆí„°
â”‚
â”œâ”€â”€ z_util/                      # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ cpu_mntrg.py             # CPU/RAM ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ gpu_mnrtg.py             # GPU/VRAM ëª¨ë‹ˆí„°ë§
â”‚   â””â”€â”€ local_dataset_loader.py  # ë¡œì»¬ JSON ë°ì´í„°ì…‹ ë¡œë”
â”‚
â”œâ”€â”€ qwen-KR-14B-output-unsloth/  # í›ˆë ¨ ì¶œë ¥ (1ë‹¨ê³„)
â”‚   â””â”€â”€ final/                   # ìµœì¢… LoRA ì–´ëŒ‘í„°
â”‚
â””â”€â”€ training.log                 # ì‹¤ì‹œê°„ í›ˆë ¨ ë¡œê·¸ (Git ì œì™¸)
```

---

## ë¹ ë¥¸ ì‹œìž‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install unsloth[cu128-torch280]
pip install transformers==4.57.0
pip install datasets
pip install bitsandbytes
pip install peft
pip install trl
pip install python-dotenv

# .env íŒŒì¼ ìƒì„± (Gitì— ì»¤ë°‹ ê¸ˆì§€!)
cat > .env << EOF
HF_TOKEN=your_hf_token_here
TOKENIZER=your_tokenizer_name_here
EOF
```

### 1ë‹¨ê³„: ì¼ë°˜ í•œêµ­ì–´ í›ˆë ¨

```bash
cd /home/work/tesseract/qwen
python 0_qwen_ft_us.py
```

**í›ˆë ¨ ì„¤ì •:**
- ë°ì´í„°ì…‹: 54,190ê°œ í•œêµ­ì–´ ìƒ˜í”Œ (HuggingFace)
- ëª¨ë¸: Qwen 2.5-14B-Instruct + INT8 ì–‘ìží™”
- LoRA: r=128, alpha=256, dropout=0.0
- ë°°ì¹˜: 4 Ã— 4 ëˆ„ì  = 16 ìœ íš¨ ë°°ì¹˜
- Epoch: 3, í•™ìŠµë¥ : 5e-5
- ì˜ˆìƒ ì‹œê°„: ì•½ 13ì‹œê°„ (H100 2ê°œ)

**ì¶œë ¥:** `qwen-KR-14B-output-unsloth/final/`

### 2ë‹¨ê³„: WMS ë„ë©”ì¸ í›ˆë ¨

```bash
cd /home/work/tesseract/qwen
python 1_qwen_ft_wms.py
```

**í›ˆë ¨ ì„¤ì •:**
- ë°ì´í„°ì…‹: 20,000ê°œ WMS QA íŽ˜ì–´ (ë¡œì»¬)
- ë² ì´ìŠ¤: 1ë‹¨ê³„ ì¶œë ¥ ëª¨ë¸
- LoRA: r=64, alpha=128 (íŒŒì¸íŠœë‹ìš©ìœ¼ë¡œ ìž‘ê²Œ)
- Epoch: 2, í•™ìŠµë¥ : 2e-5 (ë‚®ê²Œ)
- ì˜ˆìƒ ì‹œê°„: ì•½ 6ì‹œê°„ (H100 2ê°œ)

**ì¶œë ¥:** `qwen-WMS-14B-output-unsloth/final/`

---

## í›ˆë ¨ ìƒì„¸

### 1ë‹¨ê³„: ì¼ë°˜ í•œêµ­ì–´

**ëª©í‘œ:** ê°•ë ¥í•œ í•œêµ­ì–´ ê¸°ë°˜ êµ¬ì¶•

```python
# ì£¼ìš” ì„¤ì •
base_model = "Qwen/Qwen2.5-14B-Instruct"
tokenizer_name = os.getenv("TOKENIZER")  # .env íŒŒì¼ì—ì„œ ë¡œë“œ
dataset_name = "your_username/korean-quality-dataset"

# LoRA
lora_r = 128
lora_alpha = 256
lora_dropout = 0.0
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]
modules_to_save = ["embed_tokens", "lm_head"]  # ìž„ë² ë”© ë ˆì´ì–´ í•™ìŠµ

# í›ˆë ¨
num_train_epochs = 3
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 5e-5
warmup_ratio = 0.1
max_seq_length = 4096
```

**ì˜ˆìƒ Loss:**
- ì´ˆê¸°: ~2.5-4.0
- Epoch 1: ~1.5-2.5
- Epoch 3: **0.8-1.5** âœ…

### 2ë‹¨ê³„: WMS ë„ë©”ì¸

**ëª©í‘œ:** WMS ë„ë©”ì¸ ì§€ì‹ íŠ¹í™”

```python
# ì£¼ìš” ì„¤ì • (1ë‹¨ê³„ì™€ì˜ ì°¨ì´ì )
base_model = "qwen-KR-14B-output-unsloth/final"  # 1ë‹¨ê³„ ì¶œë ¥
dataset_name = "local"  # 20K WMS QA íŽ˜ì–´

# LoRA (ìž‘ê²Œ)
lora_r = 64
lora_alpha = 128

# í›ˆë ¨ (ë¶€ë“œëŸ½ê²Œ)
num_train_epochs = 2
learning_rate = 2e-5  # ë‚®ì€ í•™ìŠµë¥ 
warmup_ratio = 0.05
```

---

## ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```bash
# í›ˆë ¨ ì§„í–‰ ìƒí™© í™•ì¸
tail -f training.log

# GPU ì‚¬ìš©ë¥  í™•ì¸
nvidia-smi -l 1

# í›ˆë ¨ ê·¸ëž˜í”„ ìƒì„± (í›ˆë ¨ í›„)
cd 3_use
python monitor_training.py --log ../training.log --output plots
```

### ì£¼ìš” ì§€í‘œ

- **Loss**: ~3.0ì—ì„œ ~1.0-1.5ë¡œ ê°ì†Œ
- **Eval Loss**: Train lossì˜ 0.1-0.3 ì´ë‚´
- **GPU í™œìš©ë„**: ~46% (INT8 ì–‘ìží™”)
- **VRAM ì‚¬ìš©ëŸ‰**: GPUë‹¹ ~18GB

---

## ë¬¸ì œ í•´ê²°

### 1. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±

```python
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ
per_device_train_batch_size = 2  # 4 â†’ 2
gradient_accumulation_steps = 8  # 4 â†’ 8
```

### 2. Pickle ì—ëŸ¬ (dataset.map)

`0_qwen_ft_us.py`ì—ì„œ `dataset.map()` ëŒ€ì‹  Python loopë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.

### 3. í† í¬ë‚˜ì´ì € ìž„ë² ë”© ë¶ˆì¼ì¹˜

`resize_token_embeddings()`ì™€ ìŠ¤ë§ˆíŠ¸ ì´ˆê¸°í™”ë¡œ ìžë™ ì²˜ë¦¬ë©ë‹ˆë‹¤.

---

## ë‹¤ìŒ ë‹¨ê³„

### 1. ëª¨ë¸ ë³‘í•©

```python
from unsloth import FastLanguageModel

model.save_pretrained_merged(
    "qwen-14B-wms-merged",
    tokenizer,
    save_method="merged_16bit"
)
```

### 2. HuggingFace ì—…ë¡œë“œ

```python
# LoRA ì–´ëŒ‘í„° ì—…ë¡œë“œ (ê¶Œìž¥)
from huggingface_hub import HfApi

api = HfApi()
api.upload_folder(
    folder_path="qwen-KR-14B-output-unsloth/final",
    repo_id="your_username/qwen-14B-korean-lora",
    token="your_hf_token"
)
```

### 3. vLLM ë°°í¬

```bash
vllm serve qwen-14B-wms-merged \
    --dtype bfloat16 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.9
```

---

## Git ì„¤ì • (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì œì™¸)

```bash
# .gitignoreì— ì´ë¯¸ ì„¤ì •ë¨
*.log
*.safetensors
*.bin
*output*/
*cache*/
```

**ì—…ë¡œë“œ ì œì™¸ í•­ëª©:**
- ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (ìš©ëŸ‰ ì´ˆê³¼)
- ìºì‹œ íŒŒì¼
- í›ˆë ¨ ë¡œê·¸ (ë„ˆë¬´ í¼)

**Git í‘¸ì‹œ:**
```bash
git add qwen/*.py qwen/README.md qwen/1_guide/ qwen/3_use/
git commit -m "Update Qwen fine-tuning pipeline"
git push origin main
```

---

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” **CC-BY-NC-4.0** (í¬ë¦¬ì—ì´í‹°ë¸Œ ì»¤ë¨¼ì¦ˆ ì €ìž‘ìží‘œì‹œ-ë¹„ì˜ë¦¬ 4.0 êµ­ì œ) ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

- **í—ˆìš©**: ì—°êµ¬, êµìœ¡, ê°œì¸ ì‚¬ìš©
- **ê¸ˆì§€**: ìƒì—…ì  ì´ìš©
- **í•„ìˆ˜**: ì›ì €ìž‘ìž í‘œì‹œ

ë² ì´ìŠ¤ ëª¨ë¸ ë¼ì´ì„ ìŠ¤: [Qwen 2.5](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)

---

## ê°ì‚¬ì˜ ë§

- **Alibaba Qwen Team**: Qwen 2.5 ëª¨ë¸
- **Unsloth Team**: ë¹ ë¥¸ í›ˆë ¨ ìµœì í™”
- **HuggingFace**: Transformers ìƒíƒœê³„

---

**Last Updated**: 2025-10-14  
**Version**: 2.0.0  
**Training Status**: âœ… Stage 1 In Progress (Step 3/4827)

---

**Made with â¤ï¸ for Korean NLP**
