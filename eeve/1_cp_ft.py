#!/usr/bin/env python3
"""
EEVE-10.8B ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì„¸ë°€ ì¬í›ˆë ¨
- checkpoint-6250ì—ì„œ ì‹œì‘
- ë” ë‚®ì€ learning rate
- ë” ìì£¼ evaluation
- ê³¼ì í•© ë°©ì§€ ìµœì í™”
"""

import os
import torch
from datetime import datetime
from typing import Optional
from dataclasses import dataclass

from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FineTuningConfig:
    """ì„¸ë°€ íŒŒì¸íŠœë‹ ì„¤ì •"""
    
    # ëª¨ë¸ - ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œì‘!
    checkpoint_path: str = "/home/work/tesseract/eeve-korean-output-unsloth/checkpoint-6250"
    max_seq_length: int = 4096
    
    # ë°ì´í„°
    dataset_name: str = "MyeongHo0621/korean-quality-cleaned"
    
    # ì¶œë ¥
    output_dir: str = "/home/work/tesseract/eeve-korean-output-unsloth-refined"
    run_name: str = f"eeve-refined-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # LoRA (ì´ë¯¸ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì„¤ì •ë§Œ ìœ ì§€)
    lora_r: int = 128
    lora_alpha: int = 256
    lora_dropout: float = 0.0
    
    # í›ˆë ¨ ì„¤ì • - ë” ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ!
    num_train_epochs: float = 1.0  # ìµœëŒ€ 1 epoch (ì‹¤ì œë¡œëŠ” EarlyStoppingìœ¼ë¡œ ë” ì¼ì° ë©ˆì¶¤)
    per_device_train_batch_size: int = 8
    gradient_accumulation_steps: int = 2
    learning_rate: float = 3e-5  # 1e-4 â†’ 3e-5 (ë” ë‚®ê²Œ!)
    weight_decay: float = 0.01
    warmup_ratio: float = 0.03  # ë” ì§§ì€ warmup
    
    # ìµœì í™”
    use_gradient_checkpointing: str = "unsloth"
    
    # ì €ì¥ - ë” ìì£¼ ì²´í¬!
    save_steps: int = 50  # 250 â†’ 50 (5ë°° ë” ìì£¼)
    eval_steps: int = 50   # 50 ìŠ¤í…ë§ˆë‹¤ í‰ê°€
    save_total_limit: int = 10  # ë” ë§ì€ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€
    logging_steps: int = 5
    
    # Early Stopping - í•µì‹¬!
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_loss"
    greater_is_better: bool = False
    early_stopping_patience: int = 5  # 5ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    early_stopping_threshold: float = 0.001  # ìµœì†Œ ê°œì„  ì„ê³„ê°’


class CheckpointFineTuner:
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì„¸ë°€ íŒŒì¸íŠœë„ˆ"""
    
    def __init__(self, config: FineTuningConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
    
    def load_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (LoRA í¬í•¨)"""
        logger.info("="*80)
        logger.info("ì²´í¬í¬ì¸íŠ¸ ë¡œë”© (LoRA í¬í•¨)")
        logger.info("="*80)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸: {self.config.checkpoint_path}")
        logger.info(f"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {self.config.max_seq_length}")
        logger.info("="*80)
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì§ì ‘ ë¡œë“œ (LoRAê°€ ì´ë¯¸ ì ìš©ë˜ì–´ ìˆìŒ)
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config.checkpoint_path,
            max_seq_length=self.config.max_seq_length,
            dtype=None,
            load_in_4bit=False,
            trust_remote_code=True
        )
        
        # í›ˆë ¨ ëª¨ë“œë¡œ ì „í™˜
        FastLanguageModel.for_training(self.model)
        
        logger.info("âœ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        logger.info(f"âœ“ Tokenizer vocab: {len(self.tokenizer):,}")
    
    def load_data(self):
        """ë°ì´í„°ì…‹ ë¡œë“œ"""
        logger.info("="*80)
        logger.info(f"ë°ì´í„°ì…‹ ë¡œë”©: {self.config.dataset_name}")
        logger.info("="*80)
        
        dataset = load_dataset(self.config.dataset_name, split="train")
        logger.info(f"âœ“ ì „ì²´ ë°ì´í„°: {len(dataset):,}ê°œ")
        
        return dataset
    
    def format_prompts(self, examples):
        """EEVE í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        texts = []
        
        for messages in examples['messages']:
            user_content = None
            assistant_content = None
            
            if isinstance(messages, list) and len(messages) >= 2:
                for msg in messages:
                    if msg.get('role') == 'user' and user_content is None:
                        user_content = msg.get('content', '').strip()
                    elif msg.get('role') == 'assistant' and assistant_content is None:
                        assistant_content = msg.get('content', '').strip()
            
            if user_content and assistant_content:
                text = f"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nHuman: {user_content}\nAssistant: {assistant_content}{self.tokenizer.eos_token}"
                texts.append(text)
            else:
                texts.append("")
        
        return {"text": texts}
    
    def train(self, dataset):
        """ì„¸ë°€ í›ˆë ¨ ì‹¤í–‰"""
        logger.info("="*80)
        logger.info("ğŸ¯ ì„¸ë°€ íŒŒì¸íŠœë‹ ì‹œì‘")
        logger.info(f"   Run: {self.config.run_name}")
        logger.info(f"   Starting from: {self.config.checkpoint_path}")
        logger.info("="*80)
        logger.info(f"ì „ëµ:")
        logger.info(f"  1. ë” ë‚®ì€ learning rate: {self.config.learning_rate} (ê¸°ì¡´ 1e-4)")
        logger.info(f"  2. ìµœëŒ€ {self.config.num_train_epochs} epoch (EarlyStopping ì ìš©)")
        logger.info(f"  3. ìì£¼ í‰ê°€: ë§¤ {self.config.eval_steps} steps")
        logger.info(f"  4. ìì£¼ ì €ì¥: ë§¤ {self.config.save_steps} steps")
        logger.info(f"  5. EarlyStopping: {self.config.early_stopping_patience}ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ìë™ ì¤‘ë‹¨")
        logger.info(f"  6. ìµœì  ëª¨ë¸ ìë™ ì„ íƒ: eval_loss ê¸°ì¤€")
        logger.info("="*80)
        
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # ë°ì´í„° í¬ë§·íŒ…
        logger.info("ë°ì´í„° í¬ë§·íŒ… ì¤‘...")
        dataset = dataset.map(
            self.format_prompts,
            batched=True,
            remove_columns=dataset.column_names
        )
        dataset = dataset.filter(lambda x: len(x['text']) > 0)
        logger.info(f"âœ“ í¬ë§·íŒ… ì™„ë£Œ: {len(dataset):,}ê°œ")
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í•  (ê¸°ì¡´ê³¼ ë™ì¼í•œ ë¶„í•  ì‚¬ìš©)
        train_size = int(0.95 * len(dataset))
        train_dataset = dataset.select(range(train_size))
        eval_dataset = dataset.select(range(train_size, len(dataset)))
        
        logger.info(f"í›ˆë ¨: {len(train_dataset):,}ê°œ")
        logger.info(f"ê²€ì¦: {len(eval_dataset):,}ê°œ")
        logger.info(f"Epoch: {self.config.num_train_epochs}")
        logger.info(f"ë°°ì¹˜: {self.config.per_device_train_batch_size} Ã— {self.config.gradient_accumulation_steps} = {self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps}")
        logger.info(f"ì˜ˆìƒ steps: ~{int(len(train_dataset) / (self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps) * self.config.num_train_epochs)}")
        logger.info("="*80)
        
        # í›ˆë ¨ ì¸ì
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            run_name=self.config.run_name,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            
            # ìµœì í™”
            bf16=is_bfloat16_supported(),
            fp16=not is_bfloat16_supported(),
            optim="adamw_8bit",
            
            # ë¡œê¹…/ì €ì¥ - ë” ìì£¼!
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            save_total_limit=self.config.save_total_limit,
            eval_strategy="steps",
            load_best_model_at_end=self.config.load_best_model_at_end,
            metric_for_best_model=self.config.metric_for_best_model,
            greater_is_better=self.config.greater_is_better,
            
            # ê¸°íƒ€
            remove_unused_columns=False,
            report_to=[],
            seed=42,
            save_safetensors=True
        )
        
        # EarlyStopping ì½œë°± ì„¤ì •
        early_stopping_callback = EarlyStoppingCallback(
            early_stopping_patience=self.config.early_stopping_patience,
            early_stopping_threshold=self.config.early_stopping_threshold
        )
        
        # Trainer
        trainer = SFTTrainer(
            model=self.model,
            tokenizer=self.tokenizer,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            dataset_text_field="text",
            max_seq_length=self.config.max_seq_length,
            dataset_num_proc=2,
            packing=False,
            args=training_args,
            callbacks=[early_stopping_callback]  # EarlyStopping ì¶”ê°€!
        )
        
        # í›ˆë ¨ ì‹œì‘
        logger.info("="*80)
        logger.info("ğŸš€ í›ˆë ¨ ì‹œì‘!")
        logger.info("ğŸ¯ EarlyStopping í™œì„±í™”: eval_lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¤‘ë‹¨")
        logger.info("="*80)
        
        # GPU í†µê³„
        gpu_stats = torch.cuda.get_device_properties(0)
        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
        logger.info(f"GPU: {gpu_stats.name}")
        logger.info(f"VRAM ì‚¬ìš©: {start_gpu_memory} GB / {max_memory} GB")
        logger.info("="*80)
        
        trainer.train()
        
        # ìµœì¢… ì €ì¥
        final_path = os.path.join(self.config.output_dir, "final")
        trainer.save_model(final_path)
        self.tokenizer.save_pretrained(final_path)
        
        # GPU í†µê³„
        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
        used_percentage = round(used_memory / max_memory * 100, 3)
        
        logger.info("="*80)
        logger.info("âœ… ì„¸ë°€ í›ˆë ¨ ì™„ë£Œ!")
        logger.info("="*80)
        logger.info(f"ëª¨ë¸ ì €ì¥: {final_path}")
        logger.info(f"Peak VRAM: {used_memory} GB")
        logger.info(f"VRAM ì‚¬ìš©ë¥ : {used_percentage}%")
        logger.info("")
        logger.info("ğŸ“Š EarlyStopping ê²°ê³¼:")
        logger.info(f"  - ìë™ìœ¼ë¡œ ìµœì  ì§€ì ì—ì„œ ì¤‘ë‹¨ë¨")
        logger.info(f"  - ì €ì¥ëœ ëª¨ë¸ì€ ê°€ì¥ ë‚®ì€ eval_lossë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸")
        logger.info(f"  - ê³¼ì í•© ì§ì „ì˜ sweet spot!")
        logger.info("="*80)
        
        return final_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("ğŸ¯ EEVE-10.8B ì„¸ë°€ ì¬í›ˆë ¨ (checkpoint-6250 ê¸°ë°˜)")
    print("   ëª©í‘œ: ê³¼ì í•© ì§ì „ì˜ ìµœì  ì§€ì  ì°¾ê¸°")
    print("="*80)
    
    # ì„¤ì •
    config = FineTuningConfig()
    
    print(f"\n{'='*80}")
    print("ğŸ“‹ ì„¤ì • ìš”ì•½")
    print(f"{'='*80}")
    print(f"ì²´í¬í¬ì¸íŠ¸: {config.checkpoint_path}")
    print(f"ë°ì´í„°ì…‹: {config.dataset_name}")
    print(f"ì¶œë ¥: {config.output_dir}")
    print(f"Epoch: {config.num_train_epochs} (ì§§ê²Œ!)")
    print(f"Learning Rate: {config.learning_rate} (ë‚®ê²Œ!)")
    print(f"ë°°ì¹˜: {config.per_device_train_batch_size} Ã— {config.gradient_accumulation_steps}")
    print(f"í‰ê°€/ì €ì¥: ë§¤ {config.eval_steps} steps")
    print(f"{'='*80}\n")
    
    # íŒŒì¸íŠœë„ˆ
    finetuner = CheckpointFineTuner(config)
    
    # ì‹¤í–‰
    try:
        # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        finetuner.load_checkpoint()
        
        # 2. ë°ì´í„° ë¡œë“œ
        dataset = finetuner.load_data()
        
        # 3. ì„¸ë°€ í›ˆë ¨
        model_path = finetuner.train(dataset)
        
        print(f"\n{'='*80}")
        print("ğŸ‰ ë‹¤ìŒ ë‹¨ê³„")
        print(f"{'='*80}")
        print(f"1. ì²´í¬í¬ì¸íŠ¸ ë¹„êµ: test_checkpoint.pyë¡œ ìƒˆ ì²´í¬í¬ì¸íŠ¸ë“¤ í…ŒìŠ¤íŠ¸")
        print(f"2. ìµœì  ì²´í¬í¬ì¸íŠ¸ ì„ íƒ: eval_lossê°€ ê°€ì¥ ë‚®ì€ ê²ƒ")
        print(f"3. ë³‘í•© & ì—…ë¡œë“œ: merge_and_upload.py ì‚¬ìš©")
        print(f"{'='*80}\n")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

