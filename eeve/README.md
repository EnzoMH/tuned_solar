# EEVE Fine-tuning & Quantization Pipeline

> **EEVE-Korean-Instruct λ¨λΈ νμΈνλ‹ λ° μ–‘μν™” νμ΄ν”„λΌμΈ**

μ΄ λ””λ ‰ν† λ¦¬λ” EEVE-Korean-Instruct-10.8B λ¨λΈμ νμΈνλ‹, ν…μ¤νΈ, μ–‘μν™”λ¥Ό μ„ν• ν†µν•© νμ΄ν”„λΌμΈμ„ μ κ³µν•©λ‹λ‹¤.

---

## directory structure

```
eeve/
β”β”€β”€ config.py                    # νμΈνλ‹ μ„¤μ • νμΌ
β”β”€β”€ eeve_finetune.py            # νμΈνλ‹ λ©”μΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ conv_eeve.py                # λ€ν™”ν• ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β””β”€β”€ quant/                      # μ–‘μν™” κ΄€λ ¨
    β”β”€β”€ bnb_4bit.py            # 4-bit μ–‘μν™” (μ €μ‚¬μ–‘μ©)
    β””β”€β”€ bnb_8bit.py            # 8-bit μ–‘μν™” (ν”„λ΅λ•μ…μ©)
```

---

## workflow

```
1. Fine-tuning (eeve_finetune.py)
   β†“
2. Test (conv_eeve.py)
   β†“
3. Quantization (quant/bnb_*.py)
   β†“
4. Deploy (Hugging Face Hub)
```

---

## files Explanation

### 1. `config.py`

νμΈνλ‹ μ„¤μ •μ„ κ΄€λ¦¬ν•λ” μ„¤μ • νμΌμ…λ‹λ‹¤.

**μ£Όμ” μ„¤μ •**:
- **Base Model**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`
- **LoRA Config**: r=64, alpha=128, dropout=0.05
- **Training**: 2 epochs, batch_size=4, gradient_accumulation=4
- **Output**: `/home/work/eeve-korean-output`

**μμ • κ°€λ¥ν• μ£Όμ” νλΌλ―Έν„°**:
```python
base_model = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"
max_samples = 100000
lora_r = 64
lora_alpha = 128
num_train_epochs = 2
learning_rate = 1e-4
```

---

### 2. `eeve_finetune.py`

EEVE λ¨λΈμ„ ν•κµ­μ–΄ instruction λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•λ” λ©”μΈ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

**μ£Όμ” κΈ°λ¥**:
- β… Label Masking (μ‚¬μ©μ μ…λ ¥ λ¶€λ¶„μ€ loss κ³„μ‚° μ μ™Έ)
- β… EEVE μ „μ© ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
- β… 4-bit μ–‘μν™” ν›λ ¨ (QLoRA)
- β… μλ™ μ²΄ν¬ν¬μΈνΈ μ €μ¥
- β… λ°λ§β†’μ΅΄λ“λ§ μ‘λ‹µ ν•™μµ

**μ‚¬μ©λ²•**:

```bash
# κΈ°λ³Έ μ‹¤ν–‰ (config.py μ„¤μ • μ‚¬μ©)
cd /home/work/tesseract/eeve
python eeve_finetune.py

# λ°±κ·ΈλΌμ΄λ“ μ‹¤ν–‰
nohup python eeve_finetune.py > training.log 2>&1 &

# ν›λ ¨ μƒνƒ λ¨λ‹ν„°λ§
tail -f training.log
```

**μ¶λ ¥**:
- `/home/work/eeve-korean-output/checkpoint-{N}/`
- `/home/work/eeve-korean-output/final/`

---

### 3. `conv_eeve.py`

νμΈνλ‹λ EEVE λ¨λΈκ³Ό ν„°λ―Έλ„μ—μ„ λ€ν™”ν•  μ μλ” ν…μ¤νΈ μ¤ν¬λ¦½νΈμ…λ‹λ‹¤.

**μ£Όμ” κΈ°λ¥**:
- β… λ€ν™”ν• μΈν„°νμ΄μ¤
- β… μμ—°μ¤λ¬μ΄ μ‘λ‹µ μƒμ„±
- β… LoRA μ–΄λ‘ν„° μλ™ λ΅λ“
- β… λ€ν™” νμ¤ν† λ¦¬ κ΄€λ¦¬

**μ‚¬μ©λ²•**:

```bash
# κΈ°λ³Έ μ‹¤ν–‰ (μµμ‹  μ²΄ν¬ν¬μΈνΈ μ‚¬μ©)
python conv_eeve.py

# νΉμ • μ²΄ν¬ν¬μΈνΈ ν…μ¤νΈ
python conv_eeve.py --checkpoint /home/work/eeve-korean-output/checkpoint-500

# λ² μ΄μ¤ λ¨λΈλ§ ν…μ¤νΈ (μ–΄λ‘ν„° μ—†μ΄)
python conv_eeve.py --no-adapter
```

**μμ‹ λ€ν™”**:
```
User: WMSκ°€ λ­μ•Ό?
Assistant: WMS(Warehouse Management System)λ” μ°½κ³  κ΄€λ¦¬ μ‹μ¤ν…μΌλ΅, 
λ¬Όλ¥ μ„Όν„°μ μ…μ¶κ³ , μ¬κ³  κ΄€λ¦¬, ν”Όν‚Ή, ν¨ν‚Ή λ“±μ μ‘μ—…μ„ ν¨μ¨μ μΌλ΅ 
κ΄€λ¦¬ν•λ” μ‹μ¤ν…μ…λ‹λ‹¤...

User: quit  # μΆ…λ£
```

---

### 4. `quant/bnb_4bit.py`

**4-bit μ–‘μν™” μ¤ν¬λ¦½νΈ (μ €μ‚¬μ–‘ GPUμ©)**

**νΉμ§•**:
- VRAM: ~3.5GB
- ν’μ§: μ›λ³Έμ 98%
- μ©λ„: κ°λ°/ν…μ¤νΈ, μ €μ‚¬μ–‘μ»΄ν“¨ν„° λ° Ondeviceμ©

**μ‚¬μ©λ²•**:

```bash
cd /home/work/tesseract/eeve/quant

# κΈ°λ³Έ μ‹¤ν–‰
python bnb_4bit.py

# μ»¤μ¤ν…€ μ„¤μ •
python bnb_4bit.py \
    --model /home/work/eeve-merged-checkpoint-500 \
    --output /home/work/tesseract/eeve/quant/eeve-bnb-4bit
```

**μ¶λ ¥**:
- `eeve-bnb-4bit/` (μ•½ 5.5GB)

---

### 5. `quant/bnb_8bit.py`

**8-bit μ–‘μν™” μ¤ν¬λ¦½νΈ (ν”„λ΅λ•μ…μ©)** β­

**νΉμ§•**:
- VRAM: ~10GB
- ν’μ§: μ›λ³Έμ 99.5%
- μ©λ„: ν”„λ΅λ•μ… μ„λΉ„μ¤, RTX 3060+

**μ‚¬μ©λ²•**:

```bash
cd /home/work/tesseract/eeve/quant

# κΈ°λ³Έ μ‹¤ν–‰
python bnb_8bit.py

# μ»¤μ¤ν…€ μ„¤μ •
python bnb_8bit.py \
    --model /home/work/eeve-merged-checkpoint-500 \
    --output /home/work/tesseract/eeve/quant/eeve-bnb-8bit \
    --threshold 6.0
```

**μ¶λ ¥**:
- `eeve-bnb-8bit/` (μ•½ 10.5GB)

---

## μ „μ²΄ νμ΄ν”„λΌμΈ μ‹¤ν–‰ κ°€μ΄λ“

### Step 1: νμΈνλ‹

```bash
# 1. μ„¤μ • ν™•μΈ
vim config.py

# 2. ν›λ ¨ μ‹μ‘
cd /home/work/tesseract/eeve
nohup python eeve_finetune.py > training.log 2>&1 &

# 3. μ§„ν–‰ μƒν™© λ¨λ‹ν„°λ§
tail -f training.log

# 4. ν›λ ¨ μƒνƒ ν™•μΈ
ps aux | grep eeve_finetune.py
nvidia-smi
```

**μμƒ μ‹κ°„**: H100 κΈ°μ¤€ μ•½ 2-3μ‹κ°„ (100K μƒν”, 2 epochs)

---

### Step 2: μ²΄ν¬ν¬μΈνΈ ν…μ¤νΈ

```bash
# 1. μ²« μ²΄ν¬ν¬μΈνΈ ν…μ¤νΈ
python conv_eeve.py

# 2. λ€ν™” ν…μ¤νΈ
User: λ°λ§λ΅ μ§λ¬Έν•΄λ„ μ΅΄λ“λ§λ΅ λ‹µλ³€ν•λ‚μ”?
Assistant: λ„¤, κ·Έλ ‡μµλ‹λ‹¤. μ‚¬μ©μκ»μ„ λ°λ§λ΅ μ§λ¬Έν•μ‹λ”λΌλ„...

# 3. μ—¬λ¬ μ²΄ν¬ν¬μΈνΈ λΉ„κµ
python conv_eeve.py --checkpoint /home/work/eeve-korean-output/checkpoint-500
python conv_eeve.py --checkpoint /home/work/eeve-korean-output/checkpoint-1000
```

---

### Step 3: λ¨λΈ λ³‘ν•© (μ„ νƒμ‚¬ν•­)

```bash
# LoRA μ–΄λ‘ν„°λ¥Ό λ² μ΄μ¤ λ¨λΈμ— λ³‘ν•©
cd /home/work/tesseract

# λ³‘ν•© μ¤ν¬λ¦½νΈ μ‹¤ν–‰ (ν•„μ”μ‹ λ³„λ„ μ‘μ„±)
# λλ” Hugging Faceμ— μ–΄λ‘ν„°λ§ μ—…λ΅λ“ κ°€λ¥
```

---

### Step 4: μ–‘μν™”

```bash
cd /home/work/tesseract/eeve/quant

# 4-bit μ–‘μν™” (μ €μ‚¬μ–‘μ©)
python bnb_4bit.py \
    --model /home/work/eeve-merged-checkpoint-500 \
    --output ./eeve-bnb-4bit

# 8-bit μ–‘μν™” (ν”„λ΅λ•μ…μ©)
python bnb_8bit.py \
    --model /home/work/eeve-merged-checkpoint-500 \
    --output ./eeve-bnb-8bit
```

**μμƒ μ‹κ°„**: κ° 5-10λ¶„

---

### Step 5: Hugging Face μ—…λ΅λ“

```bash
# μ–‘μν™” λ¨λΈ μ—…λ΅λ“
cd /home/work/tesseract/eeve/quant

# 4-bit μ—…λ΅λ“
huggingface-cli upload MyeongHo0621/eeve-vss-smh-bnb-4bit \
    ./eeve-bnb-4bit \
    --repo-type model

# 8-bit μ—…λ΅λ“
huggingface-cli upload MyeongHo0621/eeve-vss-smh-bnb-8bit \
    ./eeve-bnb-8bit \
    --repo-type model
```

---

## π“ μ„±λ¥ λΉ„κµ

| λ²„μ „ | VRAM | ν’μ§ | μ†λ„ | μ©λ„ |
|------|------|------|------|------|
| **FP16 μ›λ³Έ** | 21GB | 100% | β΅β΅β΅β΅ | κ³ μ‚¬μ–‘ GPU |
| **8-bit** β­ | 10GB | 99.5% | β΅β΅β΅β΅ | ν”„λ΅λ•μ… |
| **4-bit** | 3.5GB | 98% | β΅β΅β΅ | κ°λ°/ν…μ¤νΈ |

---

## π”§ λ¬Έμ  ν•΄κ²°

### 1. CUDA Out of Memory

**λ¬Έμ **: ν›λ ¨ μ¤‘ CUDA OOM μ—λ¬

**ν•΄κ²°**:
```python
# config.py μμ •
per_device_train_batch_size = 2  # 4 β†’ 2
gradient_accumulation_steps = 8  # 4 β†’ 8
max_length = 1024  # 2048 β†’ 1024
```

---

### 2. μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨

**λ¬Έμ **: `conv_eeve.py`μ—μ„ μ–΄λ‘ν„° λ΅λ“ μ‹¤ν¨

**ν•΄κ²°**:
```bash
# μ²΄ν¬ν¬μΈνΈ κ²½λ΅ ν™•μΈ
ls -la /home/work/eeve-korean-output/

# μ¬λ°”λ¥Έ κ²½λ΅ μ§€μ •
python conv_eeve.py --checkpoint /home/work/eeve-korean-output/checkpoint-XXX
```

---

### 3. μ–‘μν™” μ¤‘ μ—λ¬

**λ¬Έμ **: `bitsandbytes` κ΄€λ ¨ μ—λ¬

**ν•΄κ²°**:
```bash
# bitsandbytes μ¬μ„¤μΉ
pip install bitsandbytes --upgrade

# CUDA λ²„μ „ ν™•μΈ
nvidia-smi
```

---

## π“ κ΄€λ ¨ λ¦¬μ†μ¤

### λ°°ν¬λ λ¨λΈ

| λ¨λΈ | ν¬κΈ° | λ§ν¬ |
|------|------|------|
| **FP16 μ›λ³Έ** | 21GB | [eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh) |
| **8-bit** β­ | 10GB | [eeve-vss-smh-bnb-8bit](https://huggingface.co/MyeongHo0621/eeve-vss-smh-bnb-8bit) |
| **4-bit** | 5.5GB | [eeve-vss-smh-bnb-4bit](https://huggingface.co/MyeongHo0621/eeve-vss-smh-bnb-4bit) |

### λ°μ΄ν„°μ…‹

| λ°μ΄ν„°μ…‹ | μƒν” μ | λ§ν¬ |
|----------|---------|------|
| **Korean Quality** | 54,190 | [korean-quality-cleaned](https://huggingface.co/datasets/MyeongHo0621/korean-quality-cleaned) |

### λ¬Έμ„

| λ¬Έμ„ | μ„¤λ… |
|------|------|
| `../README.md` | ν”„λ΅μ νΈ μ „μ²΄ κ°μ” |
| `../NATURAL_LLM_STRATEGY.md` | μμ—°μ¤λ¬μ΄ LLM μƒμ„± μ „λµ |
| `../test_perform.py` | λ¨λΈ μ„±λ¥ ν‰κ°€ μ¤ν¬λ¦½νΈ |

---

## Best Practices

### 1. ν›λ ¨ μ „ μ²΄ν¬λ¦¬μ¤νΈ

- [ ] `config.py` μ„¤μ • ν™•μΈ
- [ ] λ°μ΄ν„°μ…‹ κ²½λ΅ ν™•μΈ
- [ ] GPU λ©”λ¨λ¦¬ ν™•μΈ (`nvidia-smi`)
- [ ] μ¶λ ¥ λ””λ ‰ν† λ¦¬ ν™•μΈ
- [ ] λ””μ¤ν¬ κ³µκ°„ ν™•μΈ (μµμ† 50GB)

### 2. ν¨μ¨μ μΈ ν›λ ¨

```python
# config.py κ¶μ¥ μ„¤μ • (H100 κΈ°μ¤€)
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
max_length = 2048
num_train_epochs = 2
learning_rate = 1e-4
```

### 3. μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬

```bash
# ν›λ ¨ μ¤‘ μ£ΌκΈ°μ μΌλ΅ μ €μ¥ (κΈ°λ³Έ: 250 steps)
save_steps = 250

# λ””μ¤ν¬ κ³µκ°„ μ μ•½μ„ μ„ν•΄ μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ 
rm -rf /home/work/eeve-korean-output/checkpoint-{old}
```

### 4. μ–‘μν™” μ „λµ

```
κ°λ°/ν…μ¤νΈ β†’ 4-bit (λΉ λ¥Έ λ°λ³µ)
         β†“
ν”„λ΅λ•μ… λ°°ν¬ β†’ 8-bit (μ•μ •μ„± & ν’μ§)
         β†“
κ³ μ„±λ¥ ν•„μ” β†’ FP16 μ›λ³Έ
```

---

## μ£Όμ” νΉμ§•

### 1. Label Masking

- μ‚¬μ©μ μ…λ ¥ λ¶€λ¶„μ€ loss κ³„μ‚°μ—μ„ μ μ™Έ
- Assistant μ‘λ‹µλ§ ν•™μµ
- λ” μμ—°μ¤λ¬μ΄ λ€ν™” μƒμ„±

### 2. EEVE μ „μ© ν…ν”λ¦Ώ

```python
template = """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: {assistant_output}"""
```

---

## π“ μ°Έκ³ μ‚¬ν•­

### μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

| κµ¬μ„± μ”μ† | μµμ† μ‚¬μ–‘ | κ¶μ¥ μ‚¬μ–‘ |
|----------|----------|----------|
| **GPU** | A100 (40GB) | H100 (80GB) |
| **RAM** | 32GB | 64GB+ |
| **Disk** | 100GB | 500GB+ |
| **CUDA** | 11.8+ | 12.0+ |

### λΌμ΄μ„ μ¤

- **Base Model**: [EEVE-Korean-Instruct-10.8B](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **Training Data**: CC-BY-NC-SA-4.0
- **Code**: MIT License

### Citation

```bibtex
@misc{eeve-vss-smh-2025,
  author = {MyeongHo0621},
  title = {EEVE-VSS-SMH: Fine-tuned EEVE for Korean Instructions},
  year = {2025},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/MyeongHo0621/eeve-vss-smh}}
}
```

---

## Attribute

μ΄μ λ° κ°μ„  μ μ•μ€ GitHub λλ” Hugging Faceλ¥Ό ν†µν•΄ μ μ¶ν•΄μ£Όμ„Έμ”.

---

**Last Updated**: 2025-10-11  
**Version**: 1.0  
**Status**: Production-Ready 

