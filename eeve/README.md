# EEVE-Korean-Custom-10.8B

> ğŸ‡°ğŸ‡· **Korean Custom Fine-tuning** - Responds politely in formal Korean even to casual questions

## English Documentation

### Model Overview

This model is based on [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0), fine-tuned with high-quality Korean instruction data using LoRA, and subsequently merged into a standalone model.

**Key Features:**
- High-quality Korean language processing trained on 100K+ instruction samples
- Extended context support up to 8K tokens
- Bilingual capabilities supporting both Korean and English

### Quick Start

**Installation:**
```bash
pip install transformers torch accelerate
```

**Basic Usage:**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model (no PEFT required)
model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",  
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")

# Prompt template (EEVE format)
def create_prompt(user_input):
    return f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """

# Generate response
user_input = "Implement Fibonacci sequence in Python"
prompt = create_prompt(user_input)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.85,
    repetition_penalty=1.0,
    do_sample=True
)

response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(response)
```

**Streaming Generation:**
```python
from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
generation_kwargs = {
    **inputs,
    "max_new_tokens": 512,
    "temperature": 0.3,
    "top_p": 0.85,
    "streamer": streamer
}

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

for text in streamer:
    print(text, end="", flush=True)
```

### Training Details

**Dataset Configuration:**
- Size: Approximately 100,000 samples
- Sources: Combination of high-quality Korean instruction datasets including KoAlpaca, Ko-Ultrachat, KoInstruct, Kullm-v2, Smol Korean Talk, and Korean Wiki QA
- Preprocessing: Length filtering, deduplication, language verification, and special character removal

**LoRA Configuration:**
```yaml
r: 128                    # Higher rank for stronger learning
lora_alpha: 256           # alpha = 2 * r
lora_dropout: 0.0         # No dropout (Unsloth optimization)
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
bias: none
task_type: CAUSAL_LM
use_rslora: false
```

**Training Hyperparameters:**

| Parameter | Value | Description |
|-----------|-------|-------------|
| Framework | **Unsloth** | 2-5x faster than standard transformers |
| Epochs | 3 (stopped at 1.94) | Early stopping at optimal point |
| Batch Size | 8 per device | Maximizing H100E memory |
| Gradient Accumulation | 2 | Effective batch size of 16 |
| Learning Rate | 1e-4 | Balanced learning rate |
| Max Sequence Length | **4096** | Extended context support |
| Warmup Ratio | 0.05 | Quick warmup |
| Weight Decay | 0.01 | Regularization |
| Optimizer | AdamW 8-bit (Unsloth) | Memory optimized |
| LR Scheduler | Cosine | Smooth decay |
| Gradient Checkpointing | Unsloth optimized | Memory efficient |

**Checkpoint Selection Strategy:**

The model was trained for 3 epochs, but we selected **checkpoint-6250 (Epoch 1.94)** based on evaluation loss analysis:

| Checkpoint | Epoch | Training Loss | Eval Loss | Status |
|-----------|-------|--------------|-----------|--------|
| 6250 | 1.94 | 0.9986 | **1.4604** | âœ… Selected (Best) |
| 6500 | 2.02 | 0.561 | 1.5866 | âŒ Overfitting |

**Key Insight:** Training loss continued to decrease, but evaluation loss started increasing after checkpoint-6250, indicating overfitting. We selected the checkpoint with the **lowest evaluation loss** for optimal generalization.

**Memory Optimization:**
- Full precision training (no 4-bit quantization needed on H100E)
- Unsloth gradient checkpointing
- BF16 training optimized for H100E
- Peak VRAM usage: ~26GB during training

**Training Infrastructure:**
- GPU: NVIDIA H100 80GB HBM3
- Framework: Unsloth + PyTorch 2.6, Transformers 4.46.3
- Training time: ~3 hours (6,250 steps with Unsloth acceleration)
- Final checkpoint: Step 6250 (Epoch 1.94), merged to full model

### Performance Examples

**Casual to Formal Korean Conversion:**

Input (casual Korean): "WMSê°€ ë­ì•¼?"

Output (formal Korean): "WMSëŠ” Warehouse Management Systemì˜ ì•½ìë¡œ, ì°½ê³  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¬ê³  ì¶”ì , ì…ì¶œê³  ê´€ë¦¬, í”¼í‚¹, íŒ¨í‚¹ ë“±ì˜ ë¬¼ë¥˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•˜ê³  ìµœì í™”í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ì°½ê³  ìš´ì˜ì„ ìœ„í•´ ì‚¬ìš©ë˜ë©°, ì‹¤ì‹œê°„ ì¬ê³  ê°€ì‹œì„±ê³¼ ì‘ì—… ìƒì‚°ì„± í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤."

**Code Generation:**

Input: "íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜"

Output: Provides three different Python methods for list reversal with detailed explanations of each approach, including reverse() method, slicing, and reversed() function, along with their differences.

### Prompt Template

This model uses the standard EEVE template format:

```python
template = """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_message}
Assistant: """
```

Using this exact template is essential for optimal performance.

### Recommended Generation Parameters

```python
generation_config = {
    "max_new_tokens": 512,
    "temperature": 0.3,
    "top_p": 0.85,
    "repetition_penalty": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "eos_token_id": tokenizer.eos_token_id,
}
```

**Parameter Tuning Guide:**

| Use Case | Temperature | Top P | Repetition Penalty | Notes |
|----------|-------------|-------|--------------------|-------|
| Precise answers | 0.1-0.3 | 0.8-0.9 | 1.0 | Best for factual Q&A |
| Balanced responses | 0.5-0.7 | 0.85-0.95 | 1.0 | **Recommended default** |
| Creative outputs | 0.8-1.0 | 0.9-1.0 | 1.05-1.1 | For creative writing |

**Important Notes on Repetition Penalty:**

- **Default (1.0):** No penalty, natural repetition allowed
- **Light (1.05-1.1):** Reduces minor repetition in creative tasks
- **Moderate (1.1-1.2):** Good for reducing repetitive phrases
- **Strong (1.2+):** May affect output quality, use with caution

âš ï¸ **Warning:** Setting repetition_penalty > 1.2 can degrade Korean text quality. For this model, **1.0-1.1 is optimal** for most use cases.

**Advanced Configuration Example:**

```python
# For code generation
code_gen_config = {
    "max_new_tokens": 1024,
    "temperature": 0.2,
    "top_p": 0.9,
    "repetition_penalty": 1.0,
    "do_sample": True,
}

# For conversational responses
conversation_config = {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.05,
    "do_sample": True,
}

# For precise factual answers
factual_config = {
    "max_new_tokens": 256,
    "temperature": 0.1,
    "top_p": 0.85,
    "repetition_penalty": 1.0,
    "do_sample": True,
}
```

### Limitations

This model has been released for research and educational purposes. Commercial use requires compliance with the CC-BY-NC-SA-4.0 license. While optimized for Korean language, the model provides partial support for other languages. Performance may improve with additional training beyond checkpoint 500.

### License

- Model License: CC-BY-NC-SA-4.0
- Base Model: Complies with [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0) license
- Commercial Use: Restricted (refer to license)

### Citation

```bibtex
@misc{eeve-vss-smh-2024,
  author = {MyeongHo0621},
  title = {EEVE-VSS-SMH: Korean Custom Fine-tuned Model},
  year = {2024},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/MyeongHo0621/eeve-vss-smh}},
  note = {LoRA fine-tuned and merged model based on EEVE-Korean-Instruct-10.8B-v1.0}
}
```

### Acknowledgments

- Base Model: [Yanolja](https://huggingface.co/yanolja) - EEVE-Korean-Instruct-10.8B-v1.0
- Training Infrastructure: KT Cloud H100E
- Framework: Hugging Face Transformers, PEFT

### Contact

- GitHub: [MyeongHo0621](https://github.com/MyeongHo0621)
- Model Repository: [tesseract](https://github.com/MyeongHo0621/tuned_solar)

---

## í•œêµ­ì–´ ë¬¸ì„œ

### ëª¨ë¸ ì†Œê°œ

ì´ ëª¨ë¸ì€ [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)ì„ ë² ì´ìŠ¤ë¡œ, ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„°ë¡œ LoRA íŒŒì¸íŠœë‹í•œ í›„ ë³‘í•©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- 100K+ ê³ í’ˆì§ˆ instruction ë°ì´í„°ë¡œ í›ˆë ¨ëœ í•œêµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥
- ìµœëŒ€ 8K í† í°ê¹Œì§€ í™•ì¥ëœ ë¬¸ë§¥ ì§€ì›
- í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ì´ì¤‘ì–¸ì–´ ê¸°ëŠ¥

### ë¹ ë¥¸ ì‹œì‘

**ì„¤ì¹˜:**
```bash
pip install transformers torch accelerate
```

**ê¸°ë³¸ ì‚¬ìš©:**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ëª¨ë¸ ë¡œë“œ (PEFT ë¶ˆí•„ìš”)
model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",  
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (EEVE í˜•ì‹)
def create_prompt(user_input):
    return f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """

# ì‘ë‹µ ìƒì„±
user_input = "íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ êµ¬í˜„í•´ì¤˜"
prompt = create_prompt(user_input)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.85,
    repetition_penalty=1.0,
    do_sample=True
)

response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print(response)
```

**ìŠ¤íŠ¸ë¦¬ë° ìƒì„±:**
```python
from transformers import TextIteratorStreamer
from threading import Thread

streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
generation_kwargs = {
    **inputs,
    "max_new_tokens": 512,
    "temperature": 0.3,
    "top_p": 0.85,
    "streamer": streamer
}

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

for text in streamer:
    print(text, end="", flush=True)
```

### í›ˆë ¨ ì„¸ë¶€ì‚¬í•­

**ë°ì´í„°ì…‹ êµ¬ì„±:**
- í¬ê¸°: ì•½ 100,000ê°œ ìƒ˜í”Œ
- ì¶œì²˜: KoAlpaca, Ko-Ultrachat, KoInstruct, Kullm-v2, Smol Korean Talk, Korean Wiki QA ë“± ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„°ì…‹ ì¡°í•©
- ì „ì²˜ë¦¬: ê¸¸ì´ í•„í„°ë§, ì¤‘ë³µ ì œê±°, ì–¸ì–´ í™•ì¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°

**LoRA ì„¤ì •:**
```yaml
r: 128                    # ë” ë†’ì€ rank (ê°•ë ¥í•œ í•™ìŠµ)
lora_alpha: 256           # alpha = 2 * r
lora_dropout: 0.0         # Dropout ì—†ìŒ (Unsloth ìµœì í™”)
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
bias: none
task_type: CAUSAL_LM
use_rslora: false
```

**í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°:**

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| í”„ë ˆì„ì›Œí¬ | **Unsloth** | ê¸°ì¡´ ëŒ€ë¹„ 2-5ë°° ë¹ ë¥¸ í›ˆë ¨ |
| Epochs | 3 (1.94ì—ì„œ ì¤‘ë‹¨) | ìµœì  ì§€ì ì—ì„œ ì¡°ê¸° ì¢…ë£Œ |
| Batch Size | 8 per device | H100E ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš© |
| Gradient Accumulation | 2 | ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸° 16 |
| Learning Rate | 1e-4 | ê· í˜•ì¡íŒ í•™ìŠµë¥  |
| Max Sequence Length | **4096** | í™•ì¥ëœ ë¬¸ë§¥ ì§€ì› |
| Warmup Ratio | 0.05 | ë¹ ë¥¸ ì›Œë°ì—… |
| Weight Decay | 0.01 | ì •ê·œí™” |
| Optimizer | AdamW 8-bit (Unsloth) | ë©”ëª¨ë¦¬ ìµœì í™” |
| LR Scheduler | Cosine | ë¶€ë“œëŸ¬ìš´ ê°ì†Œ |
| Gradient Checkpointing | Unsloth ìµœì í™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ |

**ì²´í¬í¬ì¸íŠ¸ ì„ íƒ ì „ëµ:**

3 epoch í›ˆë ¨ì„ ì§„í–‰í–ˆì§€ë§Œ, í‰ê°€ ì†ì‹¤(evaluation loss) ë¶„ì„ì„ í†µí•´ **checkpoint-6250 (Epoch 1.94)**ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤:

| ì²´í¬í¬ì¸íŠ¸ | Epoch | Training Loss | Eval Loss | ìƒíƒœ |
|-----------|-------|--------------|-----------|------|
| 6250 | 1.94 | 0.9986 | **1.4604** | âœ… ì„ íƒ (ìµœì ) |
| 6500 | 2.02 | 0.561 | 1.5866 | âŒ ê³¼ì í•© |

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸:** Training lossëŠ” ê³„ì† ê°ì†Œí–ˆì§€ë§Œ, checkpoint-6250 ì´í›„ evaluation lossê°€ ì¦ê°€í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê³¼ì í•©ì˜ ì‹ í˜¸ì…ë‹ˆë‹¤. **ê°€ì¥ ë‚®ì€ evaluation loss**ë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ìµœì ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

**ë©”ëª¨ë¦¬ ìµœì í™”:**
- Full precision í›ˆë ¨ (H100Eì—ì„œ 4-bit ì–‘ìí™” ë¶ˆí•„ìš”)
- Unsloth gradient checkpointing
- H100E ìµœì í™” BF16 í›ˆë ¨
- í›ˆë ¨ ì¤‘ Peak VRAM ì‚¬ìš©ëŸ‰: ~26GB

**í›ˆë ¨ í™˜ê²½:**
- GPU: NVIDIA H100 80GB HBM3
- í”„ë ˆì„ì›Œí¬: Unsloth + PyTorch 2.6, Transformers 4.46.3
- í›ˆë ¨ ì‹œê°„: ~3ì‹œê°„ (Unsloth ê°€ì†ìœ¼ë¡œ 6,250 steps)
- ìµœì¢… ì²´í¬í¬ì¸íŠ¸: Step 6250 (Epoch 1.94), ì „ì²´ ëª¨ë¸ë¡œ ë³‘í•©

### ì„±ëŠ¥ ì˜ˆì‹œ

**ë°˜ë§ì—ì„œ ì¡´ëŒ“ë§ ë³€í™˜:**

ì…ë ¥ (ë°˜ë§): "WMSê°€ ë­ì•¼?"

ì¶œë ¥ (ì¡´ëŒ“ë§): "WMSëŠ” Warehouse Management Systemì˜ ì•½ìë¡œ, ì°½ê³  ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¬ê³  ì¶”ì , ì…ì¶œê³  ê´€ë¦¬, í”¼í‚¹, íŒ¨í‚¹ ë“±ì˜ ë¬¼ë¥˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•˜ê³  ìµœì í™”í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ì°½ê³  ìš´ì˜ì„ ìœ„í•´ ì‚¬ìš©ë˜ë©°, ì‹¤ì‹œê°„ ì¬ê³  ê°€ì‹œì„±ê³¼ ì‘ì—… ìƒì‚°ì„± í–¥ìƒì„ ì œê³µí•©ë‹ˆë‹¤."

**ì½”ë“œ ìƒì„±:**

ì…ë ¥: "íŒŒì´ì¬ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜"

ì¶œë ¥: reverse() ë©”ì„œë“œ, ìŠ¬ë¼ì´ì‹±, reversed() í•¨ìˆ˜ ë“± ì„¸ ê°€ì§€ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ì—­ìˆœ ë³€í™˜ ë°©ë²•ì„ ê° ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ í•¨ê»˜ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

ì´ ëª¨ë¸ì€ í‘œì¤€ EEVE í…œí”Œë¦¿ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
template = """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_message}
Assistant: """
```

ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì´ í…œí”Œë¦¿ì„ ì •í™•íˆ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.

### ê¶Œì¥ ìƒì„± íŒŒë¼ë¯¸í„°

```python
generation_config = {
    "max_new_tokens": 512,
    "temperature": 0.3,
    "top_p": 0.85,
    "repetition_penalty": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "eos_token_id": tokenizer.eos_token_id,
}
```

**íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ì´ë“œ:**

| ìš©ë„ | Temperature | Top P | Repetition Penalty | ë¹„ê³  |
|------|-------------|-------|--------------------|------|
| ì •í™•í•œ ë‹µë³€ | 0.1-0.3 | 0.8-0.9 | 1.0 | ì‚¬ì‹¤ ê¸°ë°˜ Q&Aì— ìµœì  |
| ê· í˜•ì¡íŒ ë‹µë³€ | 0.5-0.7 | 0.85-0.95 | 1.0 | **ê¶Œì¥ ê¸°ë³¸ê°’** |
| ì°½ì˜ì  ë‹µë³€ | 0.8-1.0 | 0.9-1.0 | 1.05-1.1 | ì°½ì‘ ê¸€ì“°ê¸°ìš© |

**Repetition Penalty ì¤‘ìš” ì°¸ê³ ì‚¬í•­:**

- **ê¸°ë³¸ê°’ (1.0):** í˜ë„í‹° ì—†ìŒ, ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©
- **ì•½í•¨ (1.05-1.1):** ì°½ì‘ ì‘ì—…ì—ì„œ ë¯¸ì„¸í•œ ë°˜ë³µ ê°ì†Œ
- **ì¤‘ê°„ (1.1-1.2):** ë°˜ë³µì ì¸ êµ¬ë¬¸ ê°ì†Œì— íš¨ê³¼ì 
- **ê°•í•¨ (1.2+):** ì¶œë ¥ í’ˆì§ˆ ì €í•˜ ê°€ëŠ¥, ì£¼ì˜í•´ì„œ ì‚¬ìš©

âš ï¸ **ì£¼ì˜:** repetition_penaltyë¥¼ 1.2 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ë©´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ í’ˆì§ˆì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ê²½ìš° ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš© ì‚¬ë¡€ì—ì„œ **1.0-1.1ì´ ìµœì **ì…ë‹ˆë‹¤.

**ê³ ê¸‰ ì„¤ì • ì˜ˆì‹œ:**

```python
# ì½”ë“œ ìƒì„±ìš©
code_gen_config = {
    "max_new_tokens": 1024,
    "temperature": 0.2,
    "top_p": 0.9,
    "repetition_penalty": 1.0,
    "do_sample": True,
}

# ëŒ€í™”í˜• ì‘ë‹µìš©
conversation_config = {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "repetition_penalty": 1.05,
    "do_sample": True,
}

# ì •í™•í•œ ì‚¬ì‹¤ ë‹µë³€ìš©
factual_config = {
    "max_new_tokens": 256,
    "temperature": 0.1,
    "top_p": 0.85,
    "repetition_penalty": 1.0,
    "do_sample": True,
}
```

### ì œí•œì‚¬í•­

ì´ ëª¨ë¸ì€ ì—°êµ¬ ë° êµìœ¡ ëª©ì ìœ¼ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì—…ì  ì‚¬ìš© ì‹œ CC-BY-NC-SA-4.0 ë¼ì´ì„ ìŠ¤ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤. í•œêµ­ì–´ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ë„ ë¶€ë¶„ì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

### ë¼ì´ì„ ìŠ¤

- ëª¨ë¸ ë¼ì´ì„ ìŠ¤: CC-BY-NC-SA-4.0
- ë² ì´ìŠ¤ ëª¨ë¸: [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0) ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜
- ìƒì—…ì  ì‚¬ìš©: ì œí•œì  (ë¼ì´ì„ ìŠ¤ ì°¸ì¡°)

### ì¸ìš©

```bibtex
@misc{eeve-vss-smh-2024,
  author = {MyeongHo0621},
  title = {EEVE-VSS-SMH: Korean Custom Fine-tuned Model},
  year = {2024},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/MyeongHo0621/eeve-vss-smh}},
  note = {LoRA fine-tuned and merged model based on EEVE-Korean-Instruct-10.8B-v1.0}
}
```

### ê°ì‚¬ì˜ ê¸€

- ë² ì´ìŠ¤ ëª¨ë¸: [Yanolja](https://huggingface.co/yanolja) - EEVE-Korean-Instruct-10.8B-v1.0
- í›ˆë ¨ ì¸í”„ë¼: KT Cloud H100E
- í”„ë ˆì„ì›Œí¬: Hugging Face Transformers, PEFT

### ì—°ë½ì²˜

- **Github** : [tuned_solar](https://github.com/EnzoMH/tuned_solar/tree/main/eeve)

---

**Last Updated**: 2025-10-12  
**Checkpoint**: 6250 steps (Epoch 1.94)  
**Training Method**: Unsloth (2-5x faster)  
**Selection Criteria**: Lowest Evaluation Loss (1.4604)  
**Status**: Merged & Ready for Deployment