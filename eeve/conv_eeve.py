#!/usr/bin/env python3
"""
EEVE ëª¨ë¸ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤
- EEVE ê³µì‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel
import argparse


class EEVEConversation:
    def __init__(
        self,
        base_model_name="yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
        lora_model_path=None
    ):
        """
        EEVE ëŒ€í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            base_model_name: ë² ì´ìŠ¤ ëª¨ë¸ (EEVE-10.8B)
            lora_model_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ì˜µì…˜, ë¡œì»¬ ë˜ëŠ” HuggingFace)
        """
        self.base_model_name = base_model_name
        self.lora_model_path = lora_model_path
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì¤‘: {self.base_model_name}")
        if self.lora_model_path:
            print(f"LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {self.lora_model_path}")
        
        # 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True
        )
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ (ìˆë‹¤ë©´)
        if self.lora_model_path:
            try:
                self.model = PeftModel.from_pretrained(
                    self.model,
                    self.lora_model_path,
                    is_trainable=False
                )
                print("âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        if self.lora_model_path:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.lora_model_path,
                    trust_remote_code=True
                )
                print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (LoRA ê²½ë¡œ)")
            except:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.base_model_name,
                    trust_remote_code=True
                )
                print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (ë² ì´ìŠ¤ ëª¨ë¸)")
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True
            )
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ (ë² ì´ìŠ¤ ëª¨ë¸)")
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        print("=" * 60)
        print("ğŸš€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        print(f"ğŸ“ Vocab í¬ê¸°: {len(self.tokenizer):,}")
        print("=" * 60)
        
    def generate_response(
        self,
        user_input,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.1
    ):
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        EEVE ìµœì  ì„¤ì •:
        - temperature=0.7 (ì ì ˆí•œ ë‹¤ì–‘ì„±)
        - top_p=0.9 (ìì—°ìŠ¤ëŸ¬ìš´ sampling)
        - max_new_tokens=512 (8K ì§€ì›)
        
        Args:
            user_input: ì‚¬ìš©ì ë©”ì‹œì§€
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„± (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì )
            top_p: Nucleus sampling
            top_k: Top-k sampling
            repetition_penalty: ë°˜ë³µ íŒ¨ë„í‹°
        """
        # EEVE ê³µì‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
        
        # í† í°í™”
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        
        # ì…ë ¥ ê¸¸ì´ ê³„ì‚° (GPU ì´ë™ ì „!)
        input_length = inputs.input_ids.shape[1]
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=True if temperature > 0 else False,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”© (ì…ë ¥ ì œì™¸)
        response = self.tokenizer.decode(
            outputs[0][input_length:],
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def chat(self):
        """ëŒ€í™” ë£¨í”„"""
        print("\nğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œ: 'quit', 'exit', 'q')")
        print("ğŸ“Œ ë°˜ë§ë¡œ ì§ˆë¬¸í•´ë„ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
        print("-" * 60)
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
        print("\ní…ŒìŠ¤íŠ¸ ì˜ˆì‹œ:")
        print("  - í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?")
        print("  - íŒŒì´ì¬ ì½”ë“œ ì§œì¤˜")
        print("  - í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ì„¤ëª…í•´ë´")
        print("-" * 60)
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                user_input = input("\nì‚¬ìš©ì: ").strip()
                
                # ì¢…ë£Œ ëª…ë ¹
                if user_input.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                    print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                # ë¹ˆ ì…ë ¥
                if not user_input:
                    continue
                
                # ì‘ë‹µ ìƒì„±
                print("\nì–´ì‹œìŠ¤í„´íŠ¸: ", end="", flush=True)
                response = self.generate_response(user_input)
                print(response)
                print("-" * 60)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue


def main():
    parser = argparse.ArgumentParser(
        description="EEVE ëª¨ë¸ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤"
    )
    parser.add_argument(
        "--base-model",
        type=str,
        default="yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
        help="ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="LoRA ëª¨ë¸ ê²½ë¡œ (ì˜µì…˜, ë¡œì»¬ ë˜ëŠ” HuggingFace)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ìƒì„± temperature (ê¸°ë³¸: 0.7)"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=512,
        help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜ (ê¸°ë³¸: 512)"
    )
    
    args = parser.parse_args()
    
    # ëŒ€í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    conv = EEVEConversation(
        base_model_name=args.base_model,
        lora_model_path=args.model_path
    )
    
    # ëª¨ë¸ ë¡œë“œ
    conv.load_model()
    
    # ëŒ€í™” ì‹œì‘
    conv.chat()


if __name__ == "__main__":
    main()

