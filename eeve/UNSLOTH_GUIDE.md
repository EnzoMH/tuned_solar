# Unsloth ê³ ì† íŒŒì¸íŠœë‹ ê°€ì´ë“œ

## ì™œ Unslothì¸ê°€?

### ì†ë„ ë¹„êµ

| ë°©ë²• | ì†ë„/step | ì´ ì‹œê°„ (9,654 steps) | ê°€ì†ë¹„ |
|------|-----------|----------------------|--------|
| **Transformers + PEFT** | 10.97ì´ˆ | ~29ì‹œê°„ | 1x |
| **Unsloth** | 4-6ì´ˆ | **10-16ì‹œê°„** | **2-5ë°°** |

### Unsloth ìµœì í™”

1. **Flash Attention 2** - Attention ì—°ì‚° ìµœì í™”
2. **RoPE Scaling** - ìœ„ì¹˜ ì„ë² ë”© ìµœì í™”
3. **Triton Kernels** - ì»¤ìŠ¤í…€ CUDA ì»¤ë„
4. **8-bit Optimizer** - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì €
5. **Gradient Checkpointing** - Unsloth ì „ìš© ìµœì í™”

---

## ì¦‰ì‹œ ì‹¤í–‰

### 1. ê¸°ë³¸ ì‹¤í–‰ (ê¶Œì¥)

```bash
cd /home/work/tesseract

# ì§ì ‘ ì‹¤í–‰
python eeve/eeve_finetune_unsloth.py

# ë˜ëŠ” nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python eeve/eeve_finetune_unsloth.py > eeve/training_unsloth.log 2>&1 &

# ë¡œê·¸ í™•ì¸
tail -f eeve/training_unsloth.log
```

### 2. tmuxë¡œ ì‹¤í–‰

```bash
# tmux ì„¸ì…˜ ìƒì„±
tmux new -s eeve_unsloth

# í›ˆë ¨ ì‹œì‘
cd /home/work/tesseract
python eeve/eeve_finetune_unsloth.py

# Detach: Ctrl+B, D
# ì¬ì ‘ì†: tmux attach -t eeve_unsloth
```

---

## ì£¼ìš” ë³€ê²½ ì‚¬í•­

### Transformers â†’ Unsloth

| í•­ëª© | Transformers | Unsloth |
|------|-------------|---------|
| **ëª¨ë¸ ë¡œë“œ** | `AutoModelForCausalLM` | `FastLanguageModel` |
| **LoRA ì ìš©** | `get_peft_model()` | `FastLanguageModel.get_peft_model()` |
| **Trainer** | `Trainer` | `SFTTrainer` |
| **Optimizer** | `adamw_torch_fused` | `adamw_8bit` |
| **Gradient Checkpointing** | `True` | `"unsloth"` |

### ìë™ ìµœì í™”

- âœ… **ìë™ dtype ì„ íƒ** (bfloat16/float16)
- âœ… **ìë™ Flash Attention 2**
- âœ… **ìë™ RoPE Scaling**
- âœ… **ë ˆì´ë¸” ë§ˆìŠ¤í‚¹** (SFTTrainer ìë™ ì²˜ë¦¬)

---

## ì„¤ì • ìƒì„¸

### ëª¨ë¸ ì„¤ì •

```python
base_model = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"
max_seq_length = 2048
load_in_4bit = True
```

### ë°ì´í„° ì„¤ì •

```python
dataset_name = "MyeongHo0621/korean-quality-cleaned"
# 54,190ê°œ ê³ í’ˆì§ˆ í•œêµ­ì–´ ë°ì´í„°
```

### LoRA ì„¤ì •

```python
lora_r = 64
lora_alpha = 128
lora_dropout = 0.1
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

### í›ˆë ¨ ì„¤ì •

```python
num_train_epochs = 3
per_device_train_batch_size = 4
gradient_accumulation_steps = 4  # íš¨ê³¼ì  ë°°ì¹˜ = 16
learning_rate = 5e-5
warmup_ratio = 0.1
```

### ìµœì í™” ì„¤ì •

```python
optimizer = "adamw_8bit"  # Unsloth 8-bit optimizer
use_gradient_checkpointing = "unsloth"  # Unsloth ì „ìš©
bf16 = True (if supported)
```

---

## ì¶œë ¥ êµ¬ì¡°

```
/home/work/tesseract/eeve-korean-output-unsloth/
â”œâ”€â”€ checkpoint-250/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-750/
â”œâ”€â”€ ...
â””â”€â”€ final/
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â”œâ”€â”€ config.json
    â””â”€â”€ tokenizer files
```

---

## í›ˆë ¨ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f eeve/training_unsloth.log

# loss ì¶”ì¶œ
grep "loss" eeve/training_unsloth.log

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
watch -n 1 nvidia-smi
```

### ì˜ˆìƒ ì§„í–‰ ìƒí™©

```
Step 10:   loss ~1.38, 4-6ì´ˆ/step
Step 100:  loss ~1.35, ì•ˆì •í™”
Step 500:  loss ~1.2-1.3
Step 1000: loss ~1.1-1.2
...
ìµœì¢…:      loss ~0.9-1.0 (ëª©í‘œ)
```

---

## í›ˆë ¨ ì™„ë£Œ í›„

### 1. LoRA ë³‘í•©

```bash
python -c "
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name='/home/work/tesseract/eeve-korean-output-unsloth/final',
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=False
)

# ë³‘í•©
model.save_pretrained_merged(
    '/home/work/tesseract/eeve-unsloth-merged',
    tokenizer,
    save_method='merged_16bit'
)
print('âœ“ ë³‘í•© ì™„ë£Œ!')
"
```

### 2. vLLM í…ŒìŠ¤íŠ¸

```bash
python test_vllm_speed.py \
  --model /home/work/tesseract/eeve-unsloth-merged \
  --repetition-penalty 1.15
```

### 3. Hugging Face ì—…ë¡œë“œ

```bash
huggingface-cli login
huggingface-cli upload \
  MyeongHo0621/eeve-vss-smh-v2 \
  /home/work/tesseract/eeve-unsloth-merged
```

---

## ë¬¸ì œ í•´ê²°

### OOM (Out of Memory)

```python
# eeve_finetune_unsloth.pyì—ì„œ ìˆ˜ì •
per_device_train_batch_size = 2  # 4 â†’ 2
gradient_accumulation_steps = 8  # 4 â†’ 8
```

### ì†ë„ê°€ ëŠë¦° ê²½ìš°

```python
# max_seq_length ì¤„ì´ê¸°
max_seq_length = 1024  # 2048 â†’ 1024
```

### í›ˆë ¨ ì¤‘ë‹¨ ì‹œ

```bash
# í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
ps aux | grep eeve_finetune_unsloth

# ì¤‘ë‹¨
kill <PID>
```

---

## Unsloth vs Transformers ë¹„êµ

### ì¥ì  âœ…

- âœ… **2-5ë°° ë¹ ë¥¸ ì†ë„**
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨** (8-bit optimizer)
- âœ… **ìë™ ìµœì í™”** (Flash Attention 2, RoPE ë“±)
- âœ… **ê°„ë‹¨í•œ API**
- âœ… **SFTTrainer ìë™ ë ˆì´ë¸” ë§ˆìŠ¤í‚¹**

### ì œì•½ ì‚¬í•­ âš ï¸

- âš ï¸ **íŠ¹ì • ëª¨ë¸ë§Œ ì§€ì›** (Llama, Mistral ë“±)
- âš ï¸ **LoRAë§Œ ì§€ì›** (Full Fine-tuning ë¶ˆê°€)
- âš ï¸ **Checkpoint ì´ì–´ì„œ í•™ìŠµ ì–´ë ¤ì›€**

---

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… **ì§€ê¸ˆ**: Unsloth í›ˆë ¨ ì‹œì‘ (10-16ì‹œê°„)
2. â³ **í›ˆë ¨ ì™„ë£Œ í›„**: LoRA ë³‘í•©
3. â³ **ë³‘í•© í›„**: vLLMìœ¼ë¡œ ì†ë„ í…ŒìŠ¤íŠ¸
4. â³ **ìµœì¢…**: RAG QA ìƒì„± (vLLM ì„œë²„ ì‚¬ìš©)

---

## ëª…ë ¹ì–´ ìš”ì•½

```bash
# 1. í›ˆë ¨ ì‹œì‘
nohup python eeve/eeve_finetune_unsloth.py > eeve/training_unsloth.log 2>&1 &

# 2. ë¡œê·¸ í™•ì¸
tail -f eeve/training_unsloth.log

# 3. GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# 4. í›ˆë ¨ ì™„ë£Œ í›„ ë³‘í•©
python unsloth_merge.py

# 5. vLLM í…ŒìŠ¤íŠ¸
python test_vllm_speed.py --model eeve-unsloth-merged
```

---

**Unslothë¡œ 14-19ì‹œê°„ ì ˆì•½í•˜ì„¸ìš”!** ğŸš€

