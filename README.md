# EEVE-Korean-Instruct Custom Fine-tuning 

**EEVE-Korean-Instruct-10.8B** ëª¨ë¸, í•œêµ­ì–´ ì»¤ìŠ¤í…€ instruction ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•œ í”„ë¡œì íŠ¸

**Training Complete & HuggingFace Deployment complete**

## Project Outline

- **40,960 vocab** 
- **í•œì˜ balanced** 
- **8K context** ì§€ì›
- **Unsloth ê°€ì†** 

## Deployed Model

**HuggingFace**: [MyeongHo0621/eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")
```

## Model Information

- **Base Model**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **How to fine-tune**: LoRA (r=128, alpha=256) + Unsloth
- **Data**: ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„° (~100K ìƒ˜í”Œ)

## Train envrionment & configuration

### H/W info
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: Unsloth + PyTorch 2.8, Transformers 4.56.2

### LoRA configuration 
- **r**: 128 
- **alpha**: 256 (alpha = 2 * r)
- **dropout**: 0.0 (Only 0.0)
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

### Training Hyper Parameter 
- **Framework**: Unsloth 
- **Epochs**: 3 
- **Batch Size**: 8 
- **Gradient Accumulation**: 2 
- **Learning Rate**: 1e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

### Memory Optimization
- **Full Precision Training**
- **Unsloth Gradient Checkpointing**
- **BF16 Training**
- **Peak VRAM**

## Directory tree

```
tesseract/
â”œâ”€â”€ eeve/                         
â”‚   â”œâ”€â”€ README.md                 
â”‚   â”œâ”€â”€ 0_unsl_ft.py            # main script
â”‚   â”œâ”€â”€ 1_cp_ft.py              # CheckPoint training resume
â”‚   â”œâ”€â”€ 2_merg_uplod.py         # Merging and Huggingfacehub upload
â”‚   â”œâ”€â”€ 3_test_checkpoint.py    # Checkpoint Test
â”‚   â”œâ”€â”€ UNSLOTH_GUIDE.md        # Unsloth Guid
â”‚   â””â”€â”€ quant/                  # Quantizatio Script
â”œâ”€â”€ datageneration/             # Data generator
â”‚   â””â”€â”€ inst_eeve/              # EEVE instruction data
â””â”€â”€ solar/                      # Project Solar
```

## How to use

### 1. HuggingFace (recommended)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# model load
model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")

# prompt Template
def create_prompt(user_input):
    return f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """

# generating response
prompt = create_prompt("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.85,
    do_sample=True
)
response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
print(response)
```

### 2. Re-Training from Checkpoint(Optional)

```bash
cd eeve

# from start
python eeve_finetune_unsloth.py

# from check_point
python 1_eeve_finetune_from_checkpoint.py

# checkpoint test
python 3_test_checkpoint.py --compare \
  /path/to/checkpoint-1 \
  /path/to/checkpoint-2
```

### 3. Model Load (Python API)

#### ê¸°ë³¸ ë¡œë“œ (4-bit ì–‘ìí™”)
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# 4bit Quantization Configuration 
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# Base Model Load
base_model = AutoModelForCausalLM.from_pretrained(
    "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# LoRA Adaptor load
model = PeftModel.from_pretrained(
    base_model, 
    "/home/work/eeve-korean-output/final",
    is_trainable=False
)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "/home/work/eeve-korean-output/final",
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### Text Generation (EEVE Prompt Template)
```python
def generate_response(user_input, max_tokens=512):
    # EEVE Official Prompt Template
    prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    )
    
    input_length = inputs.input_ids.shape[1]
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,           # ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„±
            top_p=0.9,                # Nucleus sampling
            top_k=50,
            repetition_penalty=1.1,    # ë°˜ë³µ ë°©ì§€
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][input_length:], 
        skip_special_tokens=True
    ).strip()
    
    return response

# example
print(generate_response("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?"))
print(generate_response("í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ì„¤ëª…í•´ë´"))
```

## Strategy and Output

### Strategy
- **Label Masking**
- **Prompt Template**
- **Early Stopping**
- **Memory Efficiency**

### Output
- **Training time**: ~3 hours (H100E, Unsloth, 6,250 steps)
- **Memory Usage**: ~26GB VRAM (Peak)
- **Checkpoint**: 250 steps
- **Assessment**: 250 steps, eval_loss 


## ê¸°ìˆ  ìƒì„¸

### EEVE Prompt Template
```python
prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
```

ì´ í…œí”Œë¦¿ì€:
- EEVE ê³µì‹ í…œí”Œë¦¿ (í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ê²ƒê³¼ ë™ì¼)
- ì •ì¤‘í•œ ë‹µë³€ ìŠ¤íƒ€ì¼ ìœ ë„
- ì¼ê´€ëœ ì„±ëŠ¥ ë³´ì¥

### Label Masking ì „ëµ
```python
# í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (loss ê³„ì‚° ì œì™¸)
labels = input_ids.clone()
labels[:prompt_length] = -100  # í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹
labels[labels == pad_token_id] = -100  # íŒ¨ë”© ë§ˆìŠ¤í‚¹
```

**ì™œ Label Masking?**
- ì‚¬ìš©ì ì§ˆë¬¸ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ
- ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ í•™ìŠµ
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìŠ¤íƒ€ì¼ í˜•ì„±

### ë©”ëª¨ë¦¬ ìµœì í™”
1. **4-bit Quantization (NF4)**: ëª¨ë¸ í¬ê¸° 1/4ë¡œ ì¶•ì†Œ
2. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
3. **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ~0.5%ë§Œ í•™ìŠµ
4. **BF16 Training**: H100E í•˜ë“œì›¨ì–´ ìµœì í™”

**ê²°ê³¼**: 80GB GPUì—ì„œ 11GBë§Œ ì‚¬ìš©!

## ê´€ë ¨ í”„ë¡œì íŠ¸

### WMS Instruction Dataset Generator
`datageneration/Instruction/` ë””ë ‰í† ë¦¬ì— WMS(ì°½ê³  ê´€ë¦¬) ë„ë©”ì¸ íŠ¹í™” instruction ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- **RAG ê¸°ë°˜**: FAISS vectorstore í™œìš©
- **ìë™ ìƒì„±**: ì§ˆë¬¸-ë‹µë³€ í˜ì–´ ìë™ ìƒì„±
- **ë„ë©”ì¸ íŠ¹í™”**: WMS ê´€ë ¨ ì „ë¬¸ ìš©ì–´ ë° ì‹œë‚˜ë¦¬ì˜¤

### ì´ì „ SOLAR í”„ë¡œì íŠ¸
`solar/` ë””ë ‰í† ë¦¬ì— ì´ì „ SOLAR-10.7B íŒŒì¸íŠœë‹ ê²°ê³¼ê°€ ë³´ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ TODO & ë¡œë“œë§µ

### âœ… ì™„ë£Œ
- [x] EEVE ëª¨ë¸ ì„ ì •
- [x] ë°ì´í„° ì¤€ë¹„ ë° ì •ì œ (191K â†’ 100K)
- [x] Unsloth ê¸°ë°˜ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] Label masking êµ¬í˜„
- [x] í›ˆë ¨ ì™„ë£Œ (3ì‹œê°„, Unsloth ê°€ì†)
- [x] ì²´í¬í¬ì¸íŠ¸ ë¹„êµ í…ŒìŠ¤íŠ¸ (6250 vs 6500)
- [x] Eval Loss ê¸°ì¤€ ìµœì  ì²´í¬í¬ì¸íŠ¸ ì„ íƒ
- [x] ëª¨ë¸ ë³‘í•© (LoRA â†’ Full model)
- [x] **HuggingFace Hub ì—…ë¡œë“œ ì™„ë£Œ** âœ…
- [x] ìƒì„¸ README ì‘ì„± (í›ˆë ¨ ì„¸ë¶€ì‚¬í•­)

### ğŸ“‹ í–¥í›„ ê³„íš
- [ ] ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ (KoBEST, KLUE ë“±)
- [ ] ì–‘ìí™” ë²„ì „ ì—…ë¡œë“œ (4-bit, 8-bit)
- [ ] WMS ë„ë©”ì¸ ë°ì´í„° ì¶”ê°€ í•™ìŠµ
- [ ] RAG íŒŒì´í”„ë¼ì¸ í†µí•©
- [ ] ì¶”ë¡  ìµœì í™” (vLLM, TensorRT-LLM)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ì¸ [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## Acknowledgments

- **[Yanolja (EEVE Team)](https://huggingface.co/yanolja)**: EEVE-Korean-Instruct-10.8B ë² ì´ìŠ¤ ëª¨ë¸
- **[LG AI Research (EXAONE)](https://huggingface.co/LGAI-EXAONE)**: EXAONE í† í¬ë‚˜ì´ì € (EEVEì— í†µí•©)
- **[Upstage](https://huggingface.co/upstage)**: SOLAR-10.7B ê¸°ë°˜ ëª¨ë¸
- **KT Cloud**: H100E GPU ì¸í”„ë¼ ì œê³µ
- **Hugging Face**: Transformers, PEFT, Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬
- **í•œêµ­ì–´ ë°ì´í„°ì…‹ ê¸°ì—¬ìë“¤**: KoAlpaca, Kullm-v2, Smol Korean Talk ë“±

---

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 1. Eval Lossê°€ ìµœìš°ì„  ì§€í‘œ
- Training Lossê°€ ë‚®ì•„ë„ Eval Lossê°€ ì¦ê°€í•˜ë©´ **ê³¼ì í•©**
- checkpoint-6500ì€ Training Loss 0.561ë¡œ ë§¤ìš° ë‚®ì•˜ì§€ë§Œ, Eval Loss 1.5866ìœ¼ë¡œ ì¦ê°€
- checkpoint-6250ì´ Eval Loss 1.4604ë¡œ **ìµœì  ì¼ë°˜í™” ì§€ì **

### 2. Unslothì˜ ìœ„ë ¥
- ê¸°ì¡´ ëŒ€ë¹„ **2-5ë°° ë¹ ë¥¸ í›ˆë ¨**
- 6,250 stepsë¥¼ **3ì‹œê°„**ë§Œì— ì™„ë£Œ (H100E)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì : Full precisionì—ë„ 26GBë§Œ ì‚¬ìš©

### 3. ì²´í¬í¬ì¸íŠ¸ ì„ íƒì˜ ì¤‘ìš”ì„±
- ë¬´ì¡°ê±´ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ê°€ ì¢‹ì€ ê²ƒì€ ì•„ë‹˜
- Early Stopping ë˜ëŠ” Eval Loss ëª¨ë‹ˆí„°ë§ í•„ìˆ˜
- ê³¼ì í•© ì „ ì§€ì ì„ ì°¾ëŠ” ê²ƒì´ í•µì‹¬

## í”„ë¡œì íŠ¸ ì •ë³´

- **ì‹œì‘ì¼**: 2025-10-11
- **ì™„ë£Œì¼**: 2025-10-12
- **í˜„ì¬ ìƒíƒœ**: âœ… **í›ˆë ¨ ì™„ë£Œ & ë°°í¬ ì™„ë£Œ**
- **í›ˆë ¨ í™˜ê²½**: KT Cloud H100E (80GB HBM3, 24 cores, 192GB RAM)
- **í›ˆë ¨ ì‹œê°„**: ~3ì‹œê°„ (Unsloth ê°€ì†)
- **ë°°í¬**: [HuggingFace Hub](https://huggingface.co/MyeongHo0621/eeve-vss-smh)
- **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-12

---

**Made with â¤ï¸ for Korean NLP Community**
