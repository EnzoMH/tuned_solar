# EEVE-Korean-Instruct Custom Fine-tuning

**EEVE-Korean-Instruct-10.8B** ëª¨ë¸ì„ í•œêµ­ì–´ ì»¤ìŠ¤í…€ instruction ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. 
ë°˜ë§ ì§ˆë¬¸ì—ë„ ì¡´ëŒ“ë§ë¡œ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ë„ë¡ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ ê°œìš”

EEVEëŠ” ì´ë¯¸ **í•œêµ­ì–´ì™€ ì˜ì–´ì— ìµœì í™”**ë˜ì–´ ìˆì–´, Light CPT(Continued Pre-training) ì—†ì´ ë°”ë¡œ Instruction Tuningì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- âœ… **40,960 vocab** (EXAONE í† í¬ë‚˜ì´ì € í†µí•©)
- âœ… **í•œì˜ balanced** (ì´ë¯¸ ìµœì í™”ë¨)
- âœ… **8K context** ì§€ì›
- âœ… **ë¹ ë¥¸ í•™ìŠµ** (2 epochë©´ ì¶©ë¶„)

## ëª¨ë¸ ì •ë³´

- **ë² ì´ìŠ¤ ëª¨ë¸**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **íŒŒì¸íŠœë‹ ë°©ë²•**: LoRA (Low-Rank Adaptation)
- **í›ˆë ¨ ë°ì´í„°**: ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„° (~100K ìƒ˜í”Œ)
- **ëª©í‘œ**: ë°˜ë§ ì§ˆë¬¸ â†’ ì¡´ëŒ“ë§ ë‹µë³€ (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)
- **í›ˆë ¨ í™˜ê²½**: KT Cloud H100E (80GB HBM3)

## í›ˆë ¨ í™˜ê²½ & ì„¤ì •

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: PyTorch 2.6, Transformers, PEFT

### LoRA ì„¤ì •
- **r**: 64 (rank)
- **alpha**: 128
- **dropout**: 0.05 (ë‚®ê²Œ ì„¤ì •, ì´ë¯¸ instruction-tuned)
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

### í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- **Epochs**: 2
- **Batch Size**: 4 (per device)
- **Gradient Accumulation**: 4 (effective batch = 16)
- **Learning Rate**: 1e-4 (ë‚®ê²Œ, ì´ë¯¸ ì˜ í•™ìŠµëœ ëª¨ë¸)
- **Max Length**: 2048 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

### ë©”ëª¨ë¦¬ ìµœì í™”
- **4-bit Quantization**: NF4
- **Gradient Checkpointing**: í™œì„±í™”
- **BF16 Training**: H100E ìµœì í™”
- **ì˜ˆìƒ ë©”ëª¨ë¦¬**: ~11GB VRAM

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
tesseract/
â”œâ”€â”€ eeve_finetune.py              # ğŸ”¥ ë©”ì¸ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ conv_eeve.py                  # ğŸ’¬ ëŒ€í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ config.py                     # âš™ï¸ ì„¤ì • íŒŒì¼
â”‚
â”œâ”€â”€ korean_large_data/            # ğŸ“Š í›ˆë ¨ ë°ì´í„° (191K)
â”‚   â””â”€â”€ korean_large_dataset.json
â”‚
â”œâ”€â”€ eeve-korean-output/           # ğŸ’¾ í›ˆë ¨ ì¶œë ¥
â”‚   â”œâ”€â”€ checkpoint-250/           # ì²« ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ checkpoint-500/           # ...
â”‚   â””â”€â”€ final/                    # ìµœì¢… ëª¨ë¸
â”‚
â”œâ”€â”€ datageneration/               # ğŸ­ WMS Instruction ìƒì„±
â”‚   â””â”€â”€ Instruction/
â”‚
â”œâ”€â”€ solar/                        # ğŸ“¦ ì´ì „ SOLAR í”„ë¡œì íŠ¸
â””â”€â”€ NATURAL_LLM_STRATEGY.md       # ğŸ“– ì „ëµ ë¬¸ì„œ
```

## ì‚¬ìš© ë°©ë²•

### 1. í›ˆë ¨ ì‹¤í–‰

```bash
# ë°±ê·¸ë¼ìš´ë“œë¡œ í›ˆë ¨ ì‹œì‘
nohup python eeve_finetune.py > training_eeve.log 2>&1 &

# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f training_eeve.log

# í›ˆë ¨ ìƒíƒœ í™•ì¸
ps aux | grep eeve_finetune
```

### 2. ëŒ€í™” í…ŒìŠ¤íŠ¸ (ì²´í¬í¬ì¸íŠ¸)

```bash
# ì²« ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (ë°˜ë§â†’ì¡´ëŒ“ë§ ê²€ì¦)
python conv_eeve.py --model-path /home/work/eeve-korean-output/checkpoint-250

# ìµœì¢… ëª¨ë¸ í…ŒìŠ¤íŠ¸
python conv_eeve.py --model-path /home/work/eeve-korean-output/final

# ë² ì´ìŠ¤ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
python conv_eeve.py
```

### 3ï¸âƒ£ ìˆ˜ë™ ëª¨ë¸ ë¡œë“œ (Python API)

#### ê¸°ë³¸ ë¡œë“œ (4-bit ì–‘ìí™”)
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# 4bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(
    base_model, 
    "/home/work/eeve-korean-output/final",
    is_trainable=False
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    "/home/work/eeve-korean-output/final",
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### í…ìŠ¤íŠ¸ ìƒì„± (EEVE í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)
```python
def generate_response(user_input, max_tokens=512):
    # EEVE ê³µì‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    )
    
    input_length = inputs.input_ids.shape[1]
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,           # ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„±
            top_p=0.9,                # Nucleus sampling
            top_k=50,
            repetition_penalty=1.1,    # ë°˜ë³µ ë°©ì§€
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][input_length:], 
        skip_special_tokens=True
    ).strip()
    
    return response

# ì‚¬ìš© ì˜ˆì‹œ (ë°˜ë§ ì§ˆë¬¸ â†’ ì¡´ëŒ“ë§ ë‹µë³€)
print(generate_response("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?"))
print(generate_response("í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ì„¤ëª…í•´ë´"))
```

## í›ˆë ¨ ëª©í‘œ ë° íŠ¹ì§•

### ì£¼ìš” ëª©í‘œ
1. **ë°˜ë§ ì§ˆë¬¸ â†’ ì¡´ëŒ“ë§ ë‹µë³€**: ì‚¬ìš©ìê°€ ë°˜ë§ë¡œ ì§ˆë¬¸í•´ë„ í•­ìƒ ì •ì¤‘í•œ ì¡´ëŒ“ë§ë¡œ ë‹µë³€
2. **ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´**: ë²ˆì—­ì²´ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„
3. **ì¼ê´€ëœ í’ˆì§ˆ**: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë‚®ì€ learning rateì™€ dropout

### ë°ì´í„° íŠ¹ì„±
- **ì´ ìƒ˜í”Œ**: ~191K (100K ìƒ˜í”Œë§)
- **ì†ŒìŠ¤**: KoAlpaca, Kullm-v2, Smol Korean Talk, Korean Wiki QA
- **í’ˆì§ˆ í•„í„°ë§**: ê¸¸ì´, íŠ¹ìˆ˜ë¬¸ì, ë°˜ë³µ, ì–¸ì–´ ë¹„ìœ¨ ê²€ì¦
- **í˜•ì‹**: `{"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}`

### í›ˆë ¨ ì „ëµ
- **Label Masking**: ì‚¬ìš©ì ì§ˆë¬¸ ë¶€ë¶„ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸, ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ í•™ìŠµ
- **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿**: EEVE ê³µì‹ í…œí”Œë¦¿ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
- **Early Stopping**: eval_loss ê¸°ì¤€ best model ì €ì¥
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 4-bit ì–‘ìí™” + gradient checkpointing

### ì˜ˆìƒ ì„±ëŠ¥ (í›ˆë ¨ ì¤‘)
- **í›ˆë ¨ ì‹œê°„**: 6-10ì‹œê°„ (H100E, 100K ìƒ˜í”Œ, 2 epoch)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ~11GB VRAM
- **ì²´í¬í¬ì¸íŠ¸**: 250 stepsë§ˆë‹¤ ì €ì¥
- **í‰ê°€**: 250 stepsë§ˆë‹¤ eval_loss ì¸¡ì •

## ğŸ” ê¸°ìˆ  ìƒì„¸

### EEVE í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```python
prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
```

ì´ í…œí”Œë¦¿ì€:
- EEVE ê³µì‹ í…œí”Œë¦¿ (í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ê²ƒê³¼ ë™ì¼)
- ì •ì¤‘í•œ ë‹µë³€ ìŠ¤íƒ€ì¼ ìœ ë„
- ì¼ê´€ëœ ì„±ëŠ¥ ë³´ì¥

### Label Masking ì „ëµ
```python
# í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (loss ê³„ì‚° ì œì™¸)
labels = input_ids.clone()
labels[:prompt_length] = -100  # í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹
labels[labels == pad_token_id] = -100  # íŒ¨ë”© ë§ˆìŠ¤í‚¹
```

**ì™œ Label Masking?**
- ì‚¬ìš©ì ì§ˆë¬¸ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ
- ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ í•™ìŠµ
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìŠ¤íƒ€ì¼ í˜•ì„±

### ë©”ëª¨ë¦¬ ìµœì í™”
1. **4-bit Quantization (NF4)**: ëª¨ë¸ í¬ê¸° 1/4ë¡œ ì¶•ì†Œ
2. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
3. **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ~0.5%ë§Œ í•™ìŠµ
4. **BF16 Training**: H100E í•˜ë“œì›¨ì–´ ìµœì í™”

**ê²°ê³¼**: 80GB GPUì—ì„œ 11GBë§Œ ì‚¬ìš©!

## ğŸ“¦ ê´€ë ¨ í”„ë¡œì íŠ¸

### WMS Instruction Dataset Generator
`datageneration/Instruction/` ë””ë ‰í† ë¦¬ì— WMS(ì°½ê³  ê´€ë¦¬) ë„ë©”ì¸ íŠ¹í™” instruction ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- **RAG ê¸°ë°˜**: FAISS vectorstore í™œìš©
- **ìë™ ìƒì„±**: ì§ˆë¬¸-ë‹µë³€ í˜ì–´ ìë™ ìƒì„±
- **ë„ë©”ì¸ íŠ¹í™”**: WMS ê´€ë ¨ ì „ë¬¸ ìš©ì–´ ë° ì‹œë‚˜ë¦¬ì˜¤

### ì´ì „ SOLAR í”„ë¡œì íŠ¸
`solar/` ë””ë ‰í† ë¦¬ì— ì´ì „ SOLAR-10.7B íŒŒì¸íŠœë‹ ê²°ê³¼ê°€ ë³´ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ TODO & ë¡œë“œë§µ

### âœ… ì™„ë£Œ
- [x] EEVE ëª¨ë¸ ì„ ì •
- [x] ë°ì´í„° ì¤€ë¹„ ë° ì •ì œ (191K â†’ 100K)
- [x] í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (ë©”ëª¨ë¦¬ ìµœì í™”)
- [x] ëŒ€í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- [x] Label masking êµ¬í˜„
- [x] í›ˆë ¨ ì‹œì‘ (ì§„í–‰ ì¤‘)

### ğŸ”„ ì§„í–‰ ì¤‘
- [ ] í›ˆë ¨ ì™„ë£Œ ëŒ€ê¸° (6-10ì‹œê°„ ì˜ˆìƒ)
- [ ] ì²« ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (ë°˜ë§â†’ì¡´ëŒ“ë§ ê²€ì¦)
- [ ] ìµœì¢… ëª¨ë¸ í’ˆì§ˆ í‰ê°€

### ğŸ“‹ í–¥í›„ ê³„íš
- [ ] Hugging Face Hub ì—…ë¡œë“œ
- [ ] ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ (KoBEST, KLUE ë“±)
- [ ] WMS ë„ë©”ì¸ ë°ì´í„° ì¶”ê°€ í•™ìŠµ
- [ ] RAG íŒŒì´í”„ë¼ì¸ í†µí•©
- [ ] ì„±ëŠ¥ ìµœì í™” (ì¶”ë¡  ì†ë„)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ì¸ [EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## Acknowledgments

- **[Yanolja (EEVE Team)](https://huggingface.co/yanolja)**: EEVE-Korean-Instruct-10.8B ë² ì´ìŠ¤ ëª¨ë¸
- **[LG AI Research (EXAONE)](https://huggingface.co/LGAI-EXAONE)**: EXAONE í† í¬ë‚˜ì´ì € (EEVEì— í†µí•©)
- **[Upstage](https://huggingface.co/upstage)**: SOLAR-10.7B ê¸°ë°˜ ëª¨ë¸
- **KT Cloud**: H100E GPU ì¸í”„ë¼ ì œê³µ
- **Hugging Face**: Transformers, PEFT, Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬
- **í•œêµ­ì–´ ë°ì´í„°ì…‹ ê¸°ì—¬ìë“¤**: KoAlpaca, Kullm-v2, Smol Korean Talk ë“±

---

## í”„ë¡œì íŠ¸ ì •ë³´

- **ì‹œì‘ì¼**: 2025-10-11
- **í˜„ì¬ ìƒíƒœ**: í›ˆë ¨ ì§„í–‰ ì¤‘ (2% ì™„ë£Œ)
- **í›ˆë ¨ í™˜ê²½**: KT Cloud H100E (80GB HBM3, 24 cores, 192GB RAM)
- **ì˜ˆìƒ ì™„ë£Œ**: 2025-10-12
- **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-11

---

**Made with â¤ï¸ for Korean NLP Community**
