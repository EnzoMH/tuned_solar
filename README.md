# SOLAR-10.7B Korean Fine-tuned Model

ν•κµ­μ–΄λ΅ νμΈνλ‹λ SOLAR-10.7B λ¨λΈμ…λ‹λ‹¤. LoRA κΈ°λ²•μ„ ν™μ©ν•μ—¬ ν¨μ¨μ μΌλ΅ ν›λ ¨λμ—μΌλ©°, κ³Όμ ν•© λ¶„μ„μ„ ν†µν•΄ μµμ  μ²΄ν¬ν¬μΈνΈλ¥Ό μ„ μ •ν–μµλ‹λ‹¤.

## λ¨λΈ μ •λ³΄

- **λ² μ΄μ¤ λ¨λΈ**: [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)
- **νμΈνλ‹ λ°©λ²•**: LoRA (Low-Rank Adaptation)
- **ν›λ ¨ λ°μ΄ν„°**: ν•κµ­μ–΄ instruction following λ°μ΄ν„°μ…‹ (~300,000κ° μƒν”)
- **ν›λ ¨ μ‹κ°„**: μ•½ 20μ‹κ°„ 42λ¶„ (KT Cloud H100E ν™κ²½, 3 μ—ν¬ν¬)
- **μµμΆ… Train Loss**: 0.87 (μ‹μ‘: 1.56)
- **μµμ  μ²΄ν¬ν¬μΈνΈ**: checkpoint-1000 (epoch 1.03, eval_loss: 0.089)

## ν›λ ¨ ν™κ²½ & κ²°κ³Ό

- **GPU**: NVIDIA H100 80GB HBM3
- **Framework**: PyTorch 2.6, Transformers, PEFT
- **λ°°μΉ ν¬κΈ°**: 2 (gradient accumulation steps: 4, effective batch size: 8)
- **ν•™μµλ¥ **: 2e-5 (warmup_ratio: 0.1)
- **LoRA μ„¤μ •**: r=16, alpha=32, dropout=0.1, target_modules=["gate_proj", "q_proj", "o_proj", "v_proj", "down_proj", "up_proj", "k_proj"]

### κ³Όμ ν•© λ¶„μ„ κ²°κ³Ό
- **μµμ μ **: epoch 2.06 (eval_loss: 0.073) 
- **checkpoint-1000**: epoch 1.03 β†’ μ•μ •μ  μ„±λ¥, κ³Όμ ν•© μ—†μ β…
- **checkpoint-1385**: epoch 1.37 β†’ μ•½κ°„μ μ„±λ¥ μ €ν•
- **final**: epoch 3.0 β†’ κ³Όμ ν•©μΌλ΅ μΈν• ν’μ§ μ €ν• β

## νμΌ κµ¬μ΅°

```
tesseract/
β”β”€β”€ solar-korean-output/
β”‚   β””β”€β”€ checkpoint-1000/              # μµμ  μ„±λ¥ μ²΄ν¬ν¬μΈνΈ
β”‚       β”β”€β”€ adapter_model.safetensors  # LoRA κ°€μ¤‘μΉ (241MB)
β”‚       β”β”€β”€ adapter_config.json       # LoRA μ„¤μ •
β”‚       β”β”€β”€ tokenizer.json            # ν† ν¬λ‚μ΄μ € (3.4MB)
β”‚       β”β”€β”€ tokenizer.model           # SentencePiece λ¨λΈ
β”‚       β”β”€β”€ tokenizer_config.json     # ν† ν¬λ‚μ΄μ € μ„¤μ •
β”‚       β””β”€β”€ special_tokens_map.json   # νΉμ ν† ν° λ§¤ν•‘
β”‚
β”β”€β”€ test_checkpoint_1385_cuda_turbo.py  # CUDA μµμ ν™” ν…μ¤νΈ (κ¶μ¥)
β”β”€β”€ dataset_strategy_tester.py         # λ°μ΄ν„°μ…‹ λ¶„μ„ λ„κµ¬
β”β”€β”€ solar.py                          # μ›λ³Έ ν›λ ¨ μ¤ν¬λ¦½νΈ
β”‚
β”β”€β”€ checkpoint_comparison.json        # μ²΄ν¬ν¬μΈνΈ λΉ„κµ λ¶„μ„
β”β”€β”€ dataset_strategy_analysis.json    # λ°μ΄ν„°μ…‹ μ „λµ λ¶„μ„
β””β”€β”€ download-package-huggingface.tar.gz  # HF μ—…λ΅λ“μ© (223MB)
```

## μ‚¬μ© λ°©λ²•

### κ¶μ¥: CUDA ν„°λ³΄ ν…μ¤νΈ (6λ°° λΉ λ¦„)
```bash
# λ°”λ΅ μ‹¤ν–‰ κ°€λ¥ν• μµμ ν™” μ½”λ“
python test_checkpoint_1385_cuda_turbo.py

# λλ” λ€ν™”ν• λ¨λ“
python test_checkpoint_1385_cuda_turbo.py chat
```

### μλ™ λ¨λΈ λ΅λ“ (κ³ κΈ‰ μ‚¬μ©μμ©)

#### 1. κΈ°λ³Έ λ΅λ“ (λ©”λ¨λ¦¬ 24GB+ ν•„μ”)
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# λ² μ΄μ¤ λ¨λΈ λ΅λ“
base_model = AutoModelForCausalLM.from_pretrained(
    "upstage/SOLAR-10.7B-v1.0",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# LoRA μ–΄λ‘ν„° λ΅λ“ (checkpoint-1000 κ¶μ¥)
model = PeftModel.from_pretrained(
    base_model, 
    "./solar-korean-output/checkpoint-1000",
    torch_dtype=torch.bfloat16
)

# ν† ν¬λ‚μ΄μ € λ΅λ“
tokenizer = AutoTokenizer.from_pretrained("./solar-korean-output/checkpoint-1000")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### 2. 4bit μ–‘μν™” (λ©”λ¨λ¦¬ 12GB+ κ¶μ¥)
```python
from transformers import BitsAndBytesConfig

# 4bit μ–‘μν™” μ„¤μ •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# λ² μ΄μ¤ λ¨λΈ λ΅λ“ (μ–‘μν™” μ μ©)
base_model = AutoModelForCausalLM.from_pretrained(
    "upstage/SOLAR-10.7B-v1.0",
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=True
)

model = PeftModel.from_pretrained(base_model, "./solar-korean-output/checkpoint-1000")
```

#### 3. ν…μ¤νΈ μƒμ„± (μµμ ν™”λ νλΌλ―Έν„°)
```python
def generate_korean_response(question, max_tokens=200):
    prompt = f"μ§λ¬Έ: {question}\nλ‹µλ³€:"
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.3,        # μΌκ΄€μ„± μλ” λ‹µλ³€
            top_p=0.7,             # μ μ ν• λ‹¤μ–‘μ„±  
            top_k=25,              # ν† ν° μ„ νƒ μ ν•
            do_sample=True,
            repetition_penalty=1.2, # λ°λ³µ λ°©μ§€
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True
        )
    
    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:], 
        skip_special_tokens=True
    ).strip()
    
    return response

# μ‚¬μ© μμ‹
question = "ν•κµ­μ μλ„μ— λ€ν•΄ μ„¤λ…ν•΄μ£Όμ„Έμ”."
answer = generate_korean_response(question)
print(answer)
```

## μ„±λ¥ λ¶„μ„ κ²°κ³Ό

### μ²΄ν¬ν¬μΈνΈλ³„ ν’μ§ λΉ„κµ
- **checkpoint-1000**: 9.6/10 (κ³Όμ ν•© μ—†μ, μ•μ •μ  μ„±λ¥) 
- **checkpoint-1385**: 9.6/10 (μ•½κ°„μ μ„±λ¥ μ €ν•)  
- **final**: 9.8/10 (κ³Όμ ν•©μΌλ΅ μΈν• μ‹¤μ  ν’μ§ μ €ν•) 

### ν¨ν„΄λ³„ μ„±λ¥ (checkpoint-1000 κΈ°μ¤€)
- **λ€ν™”ν•**: 7.0/10 - μμ—°μ¤λ¬μ΄ ν•κµ­μ–΄ λ€ν™” 
- **μ „λ¬Έμ  μ§λ¬Έ**: 7.0/10 - κΈ°μ /κ³Όν•™ μ„¤λ… μ°μ
- **μ°½μμ  μ§λ¬Έ**: 7.0/10 - μƒμƒλ ¥ κΈ°λ° λ‹µλ³€
- **λ‹¨λ‹µν• QA**: 6.8/10 - ν©νΈ κΈ°λ° μ§λ¬Έ λ‹µλ³€
- **μ„¤λ…ν• QA**: 6.6/10 - μƒμ„Έ μ„¤λ… (κ°μ„  ν•„μ”)

### CUDA μµμ ν™” μ„±λ¥
- **κ°λ³„ μ²λ¦¬**: 6.84μ΄/μ§λ¬Έ
- **λ°°μΉ μ²λ¦¬**: 0.87μ΄/μ§λ¬Έ (6λ°° ν–¥μƒ!)
- **μµμ  λ°°μΉ ν¬κΈ°**: 5-10κ° μ§λ¬Έ

### κ¶μ¥ μƒμ„± νλΌλ―Έν„° (κ²€μ¦λ¨)
```python
temperature=0.3          # μΌκ΄€μ„± μλ” λ‹µλ³€
top_p=0.7               # μ μ ν• λ‹¤μ–‘μ„±
top_k=25                # μ•μ •μ  ν† ν° μ„ νƒ  
repetition_penalty=1.2   # λ°λ³µ λ°©μ§€
```

## ν›λ ¨ κ³Όμ • & κµν›

### ν›λ ¨ μ„Έλ¶€μ‚¬ν•­
1. **λ°μ΄ν„°**: ~300,000κ° ν•κµ­μ–΄ instruction μƒν”
2. **ν›λ ¨ μ‹κ°„**: 20μ‹κ°„ 42λ¶„ (3 μ—ν¬ν¬)
3. **Loss λ³€ν™”**: 1.56 β†’ 0.87 (44% κ°μ„ )
4. **ν‰κ°€ μ§€ν‘**: eval_loss 0.089 (epoch 1.03μ—μ„ μµμ )

### ν•µμ‹¬ λ°κ²¬μ‚¬ν•­
- **κ³Όμ ν•© λ°μƒ**: epoch 2.06 μ΄ν›„ μ„±λ¥ μ €ν•
- **μµμ  μ¤‘λ‹¨μ **: epoch 1.03 (checkpoint-1000)
- **early stopping ν•„μ”**: ν–¥ν›„ ν›λ ¨ μ‹ μ μ© κ¶μ¥
- **λ°μ΄ν„° λ…Έμ΄μ¦**: URL, νΉμλ¬Έμ μ •μ  ν•„μ”

### λ‹¤μ λ²„μ „ κ°μ„  κ³„ν
1. **Early Stopping**: epoch 2.0 κ·Όμ²μ—μ„ ν›λ ¨ μ¤‘λ‹¨
2. **λ°μ΄ν„° μ •μ **: URL, HTML νƒκ·Έ μ κ±°  
3. **ν•™μµλ¥  μ¤μΌ€μ¤„λ§**: 2 μ—ν¬ν¬ ν›„ ν•™μµλ¥  κ°μ†
4. **μ •κ·ν™” κ°•ν™”**: dropout, weight decay μ¦κ°€

## λ¶„μ„ λ„κµ¬

ν”„λ΅μ νΈμ— ν¬ν•¨λ λ¶„μ„ λ„κµ¬λ“¤:

- **`test_checkpoint_1385_cuda_turbo.py`**: CUDA μµμ ν™”λ μ¶”λ΅  ν…μ¤νΈ (κ¶μ¥)
- **`dataset_strategy_tester.py`**: λ°μ΄ν„°μ…‹ ν¨ν„΄λ³„ μ„±λ¥ λ¶„μ„
- **`solar.py`**: μ›λ³Έ ν›λ ¨ μ¤ν¬λ¦½νΈ (LoRA + H100 μµμ ν™”)
- **λ¶„μ„ κ²°κ³Ό JSON**: μ²΄ν¬ν¬μΈνΈ λΉ„κµ λ° λ°μ΄ν„° μ „λµ κ²°κ³Ό

## λ°°ν¬ κ°€μ΄λ“

### Hugging Face μ—…λ΅λ“
```bash
# μ—…λ΅λ“μ© ν¨ν‚¤μ§€ μ‚¬μ©
tar -xzf download-package-huggingface.tar.gz
# μ΄ν›„ Hugging Face Hubμ— μ—…λ΅λ“
```

### λ΅μ»¬ μ„λΉ™
```bash
# CUDA ν„°λ³΄λ΅ λΉ λ¥Έ λ΅μ»¬ μ„λΉ„μ¤
python test_checkpoint_1385_cuda_turbo.py chat
```

## λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” λ² μ΄μ¤ λ¨λΈμΈ [SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)μ λΌμ΄μ„ μ¤λ¥Ό λ”°λ¦…λ‹λ‹¤.

## κΈ°μ—¬

μ΄μ, κ°μ„  μ μ•, PRμ€ μ–Έμ λ“  ν™μμ…λ‹λ‹¤!

νΉν λ‹¤μ μμ—­μ—μ„μ κΈ°μ—¬λ¥Ό ν™μν•©λ‹λ‹¤:
- λ°μ΄ν„° μ •μ  λ° ν’μ§ κ°μ„ 
- μƒλ΅μ΄ ν•κµ­μ–΄ ν‰κ°€ λ²¤μΉλ§ν¬
- μ¶”λ΅  μµμ ν™” λ° μ„±λ¥ κ°μ„ 
- λ‹¤μ–‘ν• λ„λ©”μΈλ³„ ν…μ¤νΈ μΌ€μ΄μ¤

## Acknowledgments

- **[Upstage](https://huggingface.co/upstage)**: SOLAR-10.7B-v1.0 λ² μ΄μ¤ λ¨λΈ
- **KT Cloud**: H100E GPU μΈν”„λΌ μ κ³µ  
- **Hugging Face**: Transformers, PEFT λΌμ΄λΈλ¬λ¦¬

---
**π“… ν”„λ΅μ νΈ μ •λ³΄**
- *ν›λ ¨ κΈ°κ°„*: 2025-09-25 ~ 2025-09-27
- *μ΄ ν›λ ¨ μ‹κ°„*: 20μ‹κ°„ 42λ¶„
- *ν›λ ¨ ν™κ²½*: KT Cloud H100E (80GB HBM3)
- *μµμΆ… μ—…λ°μ΄νΈ*: 2025-09-30
