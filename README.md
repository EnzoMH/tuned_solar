# SOLAR-10.7B Korean Fine-tuned Model

í•œêµ­ì–´ë¡œ íŒŒì¸íŠœë‹ëœ SOLAR-10.7B ëª¨ë¸ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª¨ë¸ ì •ë³´

- **ë² ì´ìŠ¤ ëª¨ë¸**: [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)
- **íŒŒì¸íŠœë‹ ë°©ë²•**: LoRA (Low-Rank Adaptation)
- **í›ˆë ¨ ë°ì´í„°**: í•œêµ­ì–´ instruction following ë°ì´í„°ì…‹ 100,000ê°œ ìƒ˜í”Œ
- **í›ˆë ¨ ì‹œê°„**: ì•½ 1ì‹œê°„ 4ë¶„ (KT Cloud H100E í™˜ê²½)
- **ìµœì¢… Loss**: 0.99

## ğŸš€ í›ˆë ¨ í™˜ê²½

- **GPU**: NVIDIA H100 80GB HBM3
- **Framework**: PyTorch 2.6, Transformers, PEFT
- **ë°°ì¹˜ í¬ê¸°**: 2 (gradient accumulation steps: 4)
- **í•™ìŠµë¥ **: 2e-5
- **LoRA ì„¤ì •**: r=16, alpha=32, dropout=0.1

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
solar-korean-final.tar.gz  # ì••ì¶•ëœ ëª¨ë¸ íŒŒì¼ (223MB)
â”œâ”€â”€ adapter_config.json    # LoRA ì„¤ì •
â”œâ”€â”€ adapter_model.safetensors  # LoRA ê°€ì¤‘ì¹˜ (252MB)
â”œâ”€â”€ tokenizer.json         # í† í¬ë‚˜ì´ì €
â”œâ”€â”€ tokenizer.model        # SentencePiece ëª¨ë¸
â”œâ”€â”€ tokenizer_config.json  # í† í¬ë‚˜ì´ì € ì„¤ì •
â””â”€â”€ special_tokens_map.json
```

## ğŸ”§ ì‚¬ìš© ë°©ë²•

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
```bash
# ì••ì¶• í•´ì œ
tar -xzf solar-korean-final.tar.gz
```

### 2. ëª¨ë¸ ë¡œë“œ
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "upstage/SOLAR-10.7B-v1.0",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(
    base_model, 
    "./final",  # ì••ì¶• í•´ì œëœ í´ë” ê²½ë¡œ
    torch_dtype=torch.bfloat16
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("./final")
```

### 3. í…ìŠ¤íŠ¸ ìƒì„±
```python
# ì¶”ë¡  ëª¨ë“œ
model.eval()

# ì…ë ¥ í…ìŠ¤íŠ¸
messages = [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."}]

# ìƒì„±
inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors="pt")
with torch.no_grad():
    outputs = model.generate(
        inputs,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
print(response)
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

í›ˆë ¨ëœ ëª¨ë¸ì˜ í•œêµ­ì–´ ëŠ¥ë ¥ í‰ê°€ ê²°ê³¼:

- **ì–¸ì–´ ëŠ¥ë ¥**: â­â­â­â­â­ (5/5) - ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ êµ¬ì‚¬
- **ê¸°ë³¸ ì§€ì‹**: â­â­â­ (3/5) - ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë‹µë³€ ê°€ëŠ¥  
- **ì½”ë”© ëŠ¥ë ¥**: â­â­â­â­ (4/5) - íŒŒì´ì¬ ì½”ë“œ ìƒì„± ê°€ëŠ¥
- **ì°½ì˜ì„±**: â­â­â­ (3/5) - ìš”ë¦¬ë²•, ì¶”ì²œ ë“± ì°½ì˜ì  ë‹µë³€

## ğŸ”„ í›ˆë ¨ ê³¼ì •

1. **ë°ì´í„° ì „ì²˜ë¦¬**: 100,000ê°œ í•œêµ­ì–´ instruction ìƒ˜í”Œ
2. **ëª¨ë¸ ì„¤ì •**: LoRAë¥¼ í™œìš©í•œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹
3. **í›ˆë ¨**: 1 ì—í¬í¬, H100 í™˜ê²½ì—ì„œ 1ì‹œê°„ 4ë¶„
4. **ê²€ì¦**: Loss 1.47 â†’ 0.99ë¡œ ê°ì†Œ (35% ê°œì„ )

## ğŸ“ ë¼ì´ì„¼ìŠ¤

ë² ì´ìŠ¤ ëª¨ë¸ì¸ SOLAR-10.7B-v1.0ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆë‚˜ ê°œì„  ì œì•ˆì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!

---
*í›ˆë ¨ ë‚ ì§œ: 2025-09-25*  
*í›ˆë ¨ í™˜ê²½: KT Cloud H100E*
