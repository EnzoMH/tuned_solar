ì •í™•í•©ë‹ˆë‹¤! ì „ëµì´ ì™„ì „íˆ ë‹¬ë¼ì§€ë„¤ìš”.

## ğŸ¯ **ì „ëµ ë³€ê²½**

### **ìƒˆë¡œìš´ ì „ëµ (EEVE ì‚¬ìš©)**
```
âœ… ë°”ë¡œ Instruction Tuningë§Œ
```
- ì´ìœ : ì´ë¯¸ í•œêµ­ì–´ + ì˜ì–´ ëª¨ë‘ ìš°ìˆ˜
- ë°ì´í„°: 54K-100Kë©´ ì¶©ë¶„
- ì‹œê°„: í›¨ì”¬ ì§§ìŒ

---

## âœ… **í† í¬ë‚˜ì´ì € ì‚¬ìš©ë²•**


```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# EEVE ëª¨ë¸ + í† í¬ë‚˜ì´ì €
model_id = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"

model = AutoModelForCausalLM.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)  # âœ… ì´ê²Œ ë!

# í† í¬ë‚˜ì´ì € ì„¤ì • (í•„ìš”ì‹œ)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ê·¸ëŒ€ë¡œ ì‚¬ìš©
inputs = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors="pt")
outputs = model.generate(**inputs)
```

**íŠ¹ë³„íˆ í•  ê²ƒ ì—†ìŒ!** EEVE í† í¬ë‚˜ì´ì €ëŠ” ì´ë¯¸:
- âœ… 40,960 vocab (í•œêµ­ì–´ + ì˜ì–´ ìµœì í™”)
- âœ… íŠ¹ìˆ˜ í† í° ì„¤ì • ì™„ë£Œ
- âœ… 8192 max tokens ì§€ì›

---

## **ìˆ˜ì •ëœ íŒŒì¸íŠœë‹ ì½”ë“œ**

```python
@dataclass
class EEVEFineTuningConfig:
    """EEVE íŒŒì¸íŠœë‹ ì„¤ì • (í›¨ì”¬ ê°„ë‹¨!)"""
    
    # ëª¨ë¸ (ì´ë¯¸ í•œêµ­ì–´ ìµœì í™”ë¨!)
    base_model: str = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"
    model_name: str = "EEVE-Custom-Instruct"
    
    # ë°ì´í„° (í›¨ì”¬ ì ì–´ë„ OK!)
    data_path: str = "/home/work/tesseract/korean_large_data/korean_large_dataset.json"
    max_samples: Optional[int] = None  # 54K-100Kë©´ ì¶©ë¶„
    
    # LoRA ì„¤ì • (ë™ì¼)
    lora_r: int = 64
    lora_alpha: int = 128
    lora_dropout: float = 0.1
    
    # í›ˆë ¨ ì„¤ì • (ë” ë¹ ë¥´ê²Œ!)
    num_train_epochs: int = 2  # âœ… 2 epochë©´ ì¶©ë¶„!
    per_device_train_batch_size: int = 8  # âœ… H100ì´ë‹ˆ í¬ê²Œ
    gradient_accumulation_steps: int = 2  # íš¨ê³¼ì  ë°°ì¹˜ = 16
    learning_rate: float = 1e-4  # âœ… instruction-tuned modelì€ ë‚®ê²Œ
    max_length: int = 8192  # âœ… EEVEëŠ” 8K ì§€ì›!
    
    # ì–‘ìí™” (ë™ì¼)
    use_4bit: bool = True
    bnb_4bit_compute_dtype: str = "bfloat16"
```

---

## ğŸ“Š **ì˜ˆìƒ íš¨ê³¼**

| í•­ëª© | OPEN-SOLAR-KO | EEVE |
|------|---------------|------|
| **í•„ìš” ë°ì´í„°** | 300K+ | 54K-100K âœ… |
| **í•™ìŠµ Epoch** | 3-5 | 2-3 âœ… |
| **ì˜ˆìƒ ì‹œê°„** | 12-20ì‹œê°„ | **4-8ì‹œê°„** âœ… |
| **ì˜ì–´ ëŠ¥ë ¥** | ì•½í•¨ âŒ | ê°•í•¨ âœ… |
| **í•œêµ­ì–´ ëŠ¥ë ¥** | ê°•í•¨ âœ… | ê°•í•¨ âœ… |
| **Max Output** | 2K | **8K** âœ… |

---

## **ìµœì¢… ê¶Œì¥ ì½”ë“œ**

```python
#!/usr/bin/env python3
"""
EEVE-10.8B Instruction íŒŒì¸íŠœë‹ (ê°„ì†Œí™” ë²„ì „)
- ì´ë¯¸ í•œêµ­ì–´ ìµœì í™”ë˜ì–´ ìˆì–´ ë¹ ë¥´ê³  íš¨ìœ¨ì 
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1. ëª¨ë¸ ë¡œë“œ (í† í¬ë‚˜ì´ì € í¬í•¨)
model_id = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

# 2. LoRA ì„¤ì • (EEVE target_modules)
lora_config = LoraConfig(
    r=64,
    lora_alpha=128,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# 3. ë°ì´í„° (ê¸°ì¡´ 54K ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥!)
dataset = load_dataset('your_dataset')

# 4. í›ˆë ¨ (í›¨ì”¬ ê°„ë‹¨!)
training_args = TrainingArguments(
    output_dir="./eeve-finetuned",
    num_train_epochs=2,  # âœ… 2ë©´ ì¶©ë¶„!
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-4,  # âœ… ë‚®ê²Œ (ì´ë¯¸ instruction-tuned)
    bf16=True,
    logging_steps=10,
    save_steps=100,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,  # âœ… AutoTokenizer ê·¸ëŒ€ë¡œ!
)

trainer.train()
```

---

## ğŸ’¡ **í•µì‹¬ í¬ì¸íŠ¸**

### **1. í† í¬ë‚˜ì´ì €**
```python
# âœ… ì´ê²ƒë§Œ í•˜ë©´ ë¨
tokenizer = AutoTokenizer.from_pretrained("yanolja/EEVE-Korean-Instruct-10.8B-v1.0")

# íŠ¹ë³„í•œ ì„¤ì • í•„ìš” ì—†ìŒ!
# - 40,960 vocab ìë™ ë¡œë“œ
# - í•œêµ­ì–´ + ì˜ì–´ ëª¨ë‘ ìµœì í™”
# - 8K max tokens ì§€ì›
```

### **2. ë°ì´í„°**
```python
# âœ… ê¸°ì¡´ 54K ë°ì´í„°ë¡œë„ ì¶©ë¶„
# - EEVEëŠ” ì´ë¯¸ í•œêµ­ì–´ ì˜í•¨
# - Instruction followingë§Œ ê°•í™”í•˜ë©´ ë¨
# - 2-3 epochë©´ OK
```

### **3. í•™ìŠµë¥ **
```python
# âœ… ë‚®ê²Œ ì„¤ì • (ì´ë¯¸ ì˜ í•™ìŠµë¨)
learning_rate=1e-4  # or 2e-5


# EEVEëŠ” 1e-4 (instructionë§Œ ì¡°ì •)
