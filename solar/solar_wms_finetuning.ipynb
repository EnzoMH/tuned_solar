{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLAR-10.7B í•œêµ­ì–´ WMS íŒŒì¸íŠœë‹ (KT Cloud H100E ìµœì í™”)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Upstageì˜ SOLAR-10.7B ëª¨ë¸ì„ KT Cloud H100E í™˜ê²½ì—ì„œ í•œêµ­ì–´ WMS(ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ) ë„ë©”ì¸ì— íŠ¹í™”í•˜ì—¬ íŒŒì¸íŠœë‹í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” ë‹¨ê³„\n",
    "1. **í™˜ê²½ ì„¤ì •**: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™˜ê²½ ì²´í¬\n",
    "2. **ë°ì´í„° ì¤€ë¹„**:\n",
    "   - ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "   - Gemini 1.5 Flash APIë¥¼ í™œìš©í•œ WMS ì „ë¬¸ ë°ì´í„° ìƒì„±\n",
    "3. **ëª¨ë¸ ì„¤ì •**: SOLAR-10.7B ëª¨ë¸ ë¡œë”© ë° LoRA ì„¤ì •\n",
    "4. **í›ˆë ¨**: KT Cloud H100Eì— ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ íŒŒì¸íŠœë‹\n",
    "5. **í‰ê°€ ë° ì—…ë¡œë“œ**: (ì˜µì…˜) í›ˆë ¨ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° Hugging Face Hubì— ì—…ë¡œë“œ\n",
    "\n",
    "### ìµœì í™” í™˜ê²½\n",
    "- **GPU**: NVIDIA H100E\n",
    "- **Framework**: PyTorch 2.6, CUDA 12.8\n",
    "- **Techniques**: Flash Attention 2, BF16, Fused AdamW, LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í™˜ê²½ ì•ˆì •í™” ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# <<< ë°˜ë“œì‹œ ê°€ì¥ ì²« ì…€ì— ì‹¤í–‰ >>>\n",
    "import os, sys\n",
    "base = \"/home/work\"  # ì“°ê¸° ê°€ëŠ¥í•œ ê²½ë¡œ\n",
    "\n",
    "pkg_root = os.path.join(base, \"transformer_engine\")\n",
    "pkg_common = os.path.join(pkg_root, \"common\")\n",
    "\n",
    "os.makedirs(pkg_common, exist_ok=True)\n",
    "\n",
    "# ë¹ˆ ëª¨ë“ˆë“¤ ìƒì„±: __init__.py, pytorch.py, common/__init__.py\n",
    "open(os.path.join(pkg_root, \"__init__.py\"), \"w\").close()\n",
    "open(os.path.join(pkg_root, \"pytorch.py\"), \"w\").close()\n",
    "open(os.path.join(pkg_common, \"__init__.py\"), \"w\").close()\n",
    "\n",
    "# ì´ ê²½ë¡œë¥¼ íŒŒì´ì¬ ê²€ìƒ‰ ê²½ë¡œ ìµœìš°ì„ ìœ¼ë¡œ (ì‹œìŠ¤í…œ TE ëŒ€ì‹  ìš°ë¦¬ê°€ ë§Œë“  ë”ë¯¸ë¥¼ ì¡ê²Œ)\n",
    "if base not in sys.path:\n",
    "    sys.path.insert(0, base)\n",
    "\n",
    "# (ê¶Œì¥) device_map í™˜ê²½ë³€ìˆ˜ ì˜¤ì—¼ ë°©ì§€ + ë‹¨ì¼ GPU ê³ ì •\n",
    "for k in (\"ACCELERATE_USE_DEVICE_MAP\", \"HF_ACCELERATE_USE_DEVICE_MAP\"):\n",
    "    os.environ.pop(k, None)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"í™˜ê²½ ì•ˆì •í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ ì„¤ì • ë° ì„í¬íŠ¸\n",
    "\n",
    "íŒŒì¸íŠœë‹ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³ , ê¸°ë³¸ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "- `KTCloudH100Config` í´ë˜ìŠ¤ë¥¼ í†µí•´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "- Gemini API í‚¤, Hugging Face ì‚¬ìš©ìëª… ë“±ì€ ì´ ë‹¨ê³„ì—ì„œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers>=4.45.0 in /home/work/.local/lib/python3.12/site-packages (4.56.2)\n",
      "Requirement already satisfied: peft==0.17.1 in /home/work/.local/lib/python3.12/site-packages (0.17.1)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/work/.local/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.1 in /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg (0.45.4.dev0)\n",
      "Collecting bitsandbytes>=0.43.1\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.4 in /home/work/.local/lib/python3.12/site-packages (0.6.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/work/.local/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.16.0 in /home/work/.local/lib/python3.12/site-packages (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/work/.local/lib/python3.12/site-packages (from peft==0.17.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/work/.local/lib/python3.12/site-packages (from peft==0.17.1) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (2.6.0a0+ecf3bae40a.nv25.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (3.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/work/.local/lib/python3.12/site-packages (from transformers>=4.45.0) (0.22.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/work/.local/lib/python3.12/site-packages (from datasets>=2.0.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/work/.local/lib/python3.12/site-packages (from huggingface_hub>=0.16.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/work/.local/lib/python3.12/site-packages (from huggingface_hub>=0.16.0) (1.1.10)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (3.11.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.17.1) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.0.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.1) (2.0.1)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.47.0\n",
      "íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! ì»¤ë„ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install --user -U \\\n",
    "\"transformers>=4.45.0\" \\\n",
    "\"peft==0.17.1\" \\\n",
    "\"accelerate>=0.34.0\" \\\n",
    "\"bitsandbytes>=0.43.1\" \\\n",
    "\"safetensors>=0.4.4\" \\\n",
    "\"datasets>=2.0.0\" \\\n",
    "\"huggingface_hub>=0.16.0\"\n",
    "\n",
    "print(\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ! ì»¤ë„ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\n",
      "PyTorch ë²„ì „: 2.6.0a0+ecf3bae40a.nv25.01\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "GPU ë””ë°”ì´ìŠ¤: NVIDIA H100 80GB HBM3\n",
      "GPU ë©”ëª¨ë¦¬: 79.2GB\n",
      "H100 ê°ì§€! ìµœì í™” ëª¨ë“œ í™œì„±í™”\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "print(\"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # H100 í™•ì¸\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"H100\" in gpu_name:\n",
    "        print(\"H100 ê°ì§€! ìµœì í™” ëª¨ë“œ í™œì„±í™”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„¤ì • ì™„ë£Œ!\n",
      "ê¸°ë°˜ ëª¨ë¸: upstage/SOLAR-10.7B-v1.0\n",
      "ì¶œë ¥ ë””ë ‰í† ë¦¬: /home/work/solar-korean-output\n",
      "ë°°ì¹˜ í¬ê¸°: 8 x 2 = 16\n",
      "LoRA ì„¤ì •: r=32, alpha=64\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SOLARKoreanConfig:\n",
    "    \"\"\"SOLAR-10.7B í•œêµ­ì–´ íŒŒì¸íŠœë‹ ì„¤ì •\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    base_model: str = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "    model_name: str = \"SOLAR-10.7B-Korean-Instruct\"\n",
    "    \n",
    "    # Training settings\n",
    "    output_dir: str = \"/home/work/solar-korean-output\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 8  # H100Eì—ì„œ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸°\n",
    "    gradient_accumulation_steps: int = 2  # effective batch size = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.03\n",
    "    max_length: int = 2048\n",
    "    \n",
    "    # LoRA settings - SOLARì— ìµœì í™”\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # Hardware settings\n",
    "    use_4bit: bool = False  # H100ì€ ë©”ëª¨ë¦¬ ì¶©ë¶„\n",
    "    use_bf16: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "    \n",
    "    # Performance settings\n",
    "    dataloader_num_workers: int = 8\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "# ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "config = SOLARKoreanConfig()\n",
    "\n",
    "print(\"ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"ê¸°ë°˜ ëª¨ë¸: {config.base_model}\")\n",
    "print(f\"ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {config.per_device_train_batch_size} x {config.gradient_accumulation_steps} = {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"LoRA ì„¤ì •: r={config.lora_r}, alpha={config.lora_alpha}\")\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: /home/work/tesseract/korean_base_dataset.json\n",
      "ì´ ë°ì´í„° ê°œìˆ˜: 173,785ê°œ\n",
      "\n",
      "ë°ì´í„° êµ¬ì¡°:\n",
      "- instruction: 32\n",
      "- output: 342\n",
      "- messages: 2\n",
      "\n",
      "ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n",
      "í›ˆë ¨ ë°ì´í„°: 156,406ê°œ\n",
      "ê²€ì¦ ë°ì´í„°: 17,379ê°œ\n",
      "\n",
      "ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° íŒŒì¼ í™•ì¸ ë° ë¡œë“œ\n",
    "data_path = \"/home/work/tesseract/korean_base_dataset.json\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {data_path}\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        korean_data = json.load(f)\n",
    "    \n",
    "    print(f\"ì´ ë°ì´í„° ê°œìˆ˜: {len(korean_data):,}ê°œ\")\n",
    "    \n",
    "    # ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
    "    sample = korean_data[0]\n",
    "    print(\"\\në°ì´í„° êµ¬ì¡°:\")\n",
    "    print(f\"- instruction: {len(sample.get('instruction', ''))}\")\n",
    "    print(f\"- output: {len(sample.get('output', ''))}\")\n",
    "    print(f\"- messages: {len(sample.get('messages', []))}\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    def preprocess_korean_data(data_list):\n",
    "        \"\"\"í•œêµ­ì–´ ë°ì´í„°ë¥¼ chat í˜•íƒœë¡œ ì „ì²˜ë¦¬\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in data_list:\n",
    "            # messages í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ instruction-outputìœ¼ë¡œ ìƒì„±\n",
    "            if 'messages' in item and item['messages']:\n",
    "                messages = item['messages']\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": item['instruction']},\n",
    "                    {\"role\": \"assistant\", \"content\": item['output']}\n",
    "                ]\n",
    "            \n",
    "            processed.append({\n",
    "                \"messages\": messages,\n",
    "                \"source\": \"korean_base\"\n",
    "            })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "    print(\"\\në°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "    processed_data = preprocess_korean_data(korean_data)\n",
    "    \n",
    "    # í›ˆë ¨/ê²€ì¦ ë¶„í•  (90%/10%)\n",
    "    train_size = int(0.9 * len(processed_data))\n",
    "    train_data = processed_data[:train_size]\n",
    "    eval_data = processed_data[train_size:]\n",
    "    \n",
    "    print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ\")\n",
    "    print(f\"ê²€ì¦ ë°ì´í„°: {len(eval_data):,}ê°œ\")\n",
    "    \n",
    "    # Dataset ê°ì²´ ìƒì„±\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    print(\"\\në°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}\")\n",
    "    train_dataset = None\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KT Cloud í™˜ê²½ ì²´í¬\n",
    "\n",
    "í˜„ì¬ ì‹¤í–‰ í™˜ê²½ì´ H100Eì— ìµœì í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "- PyTorch, CUDA ë²„ì „ ì²´í¬\n",
    "- GPU ì¢…ë¥˜ ë° ë©”ëª¨ë¦¬ í™•ì¸\n",
    "- Flash Attention ì§€ì› ì—¬ë¶€ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR ëª¨ë¸ ë¡œë”©: upstage/SOLAR-10.7B-v1.0\n",
      "í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb512cc87684b028787c09d407ee109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Full precision)\n",
      "ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: 10,731,524,096\n"
     ]
    }
   ],
   "source": [
    "print(f\"SOLAR ëª¨ë¸ ë¡œë”©: {config.base_model}\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì •\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (H100ì€ ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë¯€ë¡œ ì–‘ìí™” ì—†ì´)\n",
    "if not config.use_4bit:\n",
    "    # Full precision ë¡œë“œ\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model,\n",
    "        device_map=config.device_map,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False  # í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    )\n",
    "    print(\"SOLAR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Full precision)\")\n",
    "else:\n",
    "    # ì–‘ìí™” ë²„ì „ (í•„ìš”ì‹œ)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=config.device_map,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"SOLAR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (4-bit ì–‘ìí™”)\")\n",
    "\n",
    "print(f\"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gemini APIë¥¼ í™œìš©í•œ WMS ë°ì´í„° ìƒì„±\n",
    "\n",
    "Gemini 1.5 Flash ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ WMS ë„ë©”ì¸ì— íŠ¹í™”ëœ ê³ í’ˆì§ˆ ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- **API í‚¤ ë¡œí…Œì´ì…˜**: 4ê°œì˜ API í‚¤ë¥¼ ë²ˆê°ˆì•„ ì‚¬ìš©í•˜ì—¬ API ì œí•œ(rate limit)ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
    "- **ë¹„ë™ê¸° ì²˜ë¦¬**: `asyncio`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìƒì„±ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê³  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
    "- **ì „ë¬¸ í”„ë¡¬í”„íŠ¸**: ì¬ê³ ê´€ë¦¬, ë¬¼ë¥˜ìš´ì˜, WMS ê¸°ìˆ  ë“± ì„¸ë¶„í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ì „ë¬¸ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 125,829,120 || all params: 10,857,353,216 || trainable%: 1.1589\n",
      "LoRA ì„¤ì • ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ì¼ê³± ë²ˆì§¸ ì…€ ìˆ˜ì •ë²„ì „\n",
    "# QLoRA/LoRA ì¤€ë¹„ - gradient checkpointing ë¹„í™œì„±í™” ë²„ì „\n",
    "if config.use_4bit:\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "    print(\"4-bit ëª¨ë¸ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "# SOLARì— ìµœì í™”ëœ LoRA ì„¤ì •  \n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    use_rslora=False  # RSLoRAë„ ë¹„í™œì„±í™”í•´ì„œ ì•ˆì •ì„± í™•ë³´\n",
    ")\n",
    "\n",
    "print(\"LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient for LoRA parameters explicitly\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param.retain_grad()  # ëª…ì‹œì ìœ¼ë¡œ gradient ìœ ì§€\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "print(\"LoRA ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•œêµ­ì–´ ë°ì´í„° ë¡œë”\n",
    "\n",
    "SOLAR ëª¨ë¸ íŒŒì¸íŠœë‹ì— ì‚¬ìš©í•  ëŒ€ê·œëª¨ í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "- **ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í™œìš©**: KoAlpaca (Instruction), Ko-Ultrachat (ëŒ€í™”), KULLM (ê³ í’ˆì§ˆ ëŒ€í™”) ë“± ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ í†µí•©í•˜ì—¬ ëª¨ë¸ì˜ í•œêµ­ì–´ ëŠ¥ë ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "- **ë³‘ë ¬ ì²˜ë¦¬**: `num_proc` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
    "- **ì¼ê´€ëœ í˜•ì‹**: ëª¨ë“  ë°ì´í„°ë¥¼ `messages` í˜•ì‹ìœ¼ë¡œ í†µì¼í•˜ì—¬ í›ˆë ¨ ë°ì´í„° êµ¬ì¡°ë¥¼ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654e2fe17286442eab9dbc3fd48ab449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/156406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979fdf6935064db88263dd668edb4b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/17379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\n",
      "í›ˆë ¨ ìƒ˜í”Œ: 156406\n",
      "ê²€ì¦ ìƒ˜í”Œ: 17379\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\"\"\"\n",
    "    batch_input_ids = []\n",
    "    batch_attention_masks = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for messages in examples['messages']:\n",
    "        # Chat template ì ìš©\n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        except:\n",
    "            # Chat templateì´ ì—†ìœ¼ë©´ ì§ì ‘ í¬ë§·\n",
    "            formatted_text = \"\"\n",
    "            for msg in messages:\n",
    "                if msg['role'] == 'user':\n",
    "                    formatted_text += f\"ì‚¬ìš©ì: {msg['content']}\\n\"\n",
    "                elif msg['role'] == 'assistant':\n",
    "                    formatted_text += f\"ì–´ì‹œìŠ¤í„´íŠ¸: {msg['content']}\\n\"\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì§•\n",
    "        tokenized = tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        # ë ˆì´ë¸” = input_ids (causal LM)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        batch_input_ids.append(input_ids)\n",
    "        batch_attention_masks.append(attention_mask)\n",
    "        batch_labels.append(labels)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": batch_input_ids,\n",
    "        \"attention_mask\": batch_attention_masks,\n",
    "        \"labels\": batch_labels\n",
    "    }\n",
    "\n",
    "print(\"ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ì¤‘...\")\n",
    "if train_dataset is not None:\n",
    "    # í† í¬ë‚˜ì´ì§• ì‹¤í–‰\n",
    "    train_tokenized = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    eval_tokenized = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    print(f\"í† í¬ë‚˜ì´ì§• ì™„ë£Œ!\")\n",
    "    print(f\"í›ˆë ¨ ìƒ˜í”Œ: {len(train_tokenized)}\")\n",
    "    print(f\"ê²€ì¦ ìƒ˜í”Œ: {len(eval_tokenized)}\")\n",
    "else:\n",
    "    print(\"ë°ì´í„°ì…‹ì´ ì—†ì–´ í† í¬ë‚˜ì´ì§•ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SOLAR ìµœì í™” íŠ¸ë ˆì´ë„ˆ\n",
    "\n",
    "ë³¸ê²©ì ì¸ ëª¨ë¸ í›ˆë ¨ì„ ë‹´ë‹¹í•˜ëŠ” `SOLARTrainer` í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "- **ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •**: `setup_model`\n",
    "  - H100E í™˜ê²½ì— ë§ì¶° `bfloat16`ê³¼ `Flash Attention 2`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "  - `torch.compile`ì„ ì ìš©í•˜ì—¬ PyTorch 2.xì˜ ìµœì‹  ì„±ëŠ¥ ìµœì í™”ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
    "  - `LoRA` ì„¤ì •ì„ í†µí•´ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "- **ë°ì´í„°ì…‹ ì „ì²˜ë¦¬**: `process_dataset`\n",
    "  - ëª¨ë“  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- **í›ˆë ¨ ì‹¤í–‰**: `train`\n",
    "  - `TrainingArguments`ë¥¼ í†µí•´ H100Eì— ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "  - `Trainer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í›ˆë ¨ì„ ì§„í–‰í•˜ê³ , ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸...\n",
      "í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: 125,829,120 / 10,857,353,216 (1.16%)\n",
      "ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ forward pass í…ŒìŠ¤íŠ¸...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Forward pass ì„±ê³µ! Loss: 1.9100\n",
      "SOLAR í•œêµ­ì–´ íŒŒì¸íŠœë‹ ì‹œì‘!\n",
      "ì´ ìŠ¤í…: 29325\n",
      "ì˜ˆìƒ ì‹œê°„: ì•½ 6-8ì‹œê°„ (H100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='29328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   87/29328 12:38 < 72:26:29, 0.11 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_dataset is not None:\n",
    "    # ëª¨ë¸ trainable íŒŒë¼ë¯¸í„° í™•ì¸ ë° ìˆ˜ì •\n",
    "    print(\"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸...\")\n",
    "    \n",
    "    # PEFT ëª¨ë¸ì˜ ê²½ìš° í›ˆë ¨ ëª¨ë“œ ëª…ì‹œì  ì„¤ì •\n",
    "    model.train()\n",
    "    \n",
    "    # LoRA íŒŒë¼ë¯¸í„°ê°€ ì œëŒ€ë¡œ í›ˆë ¨ ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for param in model.parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {all_params:,} ({trainable_params/all_params*100:.2f}%)\")\n",
    "    \n",
    "    if trainable_params == 0:\n",
    "        print(\"âŒ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤! LoRA ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        # LoRA íŒŒë¼ë¯¸í„° ê°•ì œ í™œì„±í™”\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora_' in name:\n",
    "                param.requires_grad = True\n",
    "                print(f\"LoRA íŒŒë¼ë¯¸í„° í™œì„±í™”: {name}\")\n",
    "    \n",
    "    # í›ˆë ¨ ì¸ì ì„¤ì • (API ë³€ê²½ì‚¬í•­ ë°˜ì˜)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        \n",
    "        # H100 ìµœì í™”\n",
    "        bf16=config.use_bf16,\n",
    "        gradient_checkpointing=config.gradient_checkpointing,\n",
    "        dataloader_num_workers=config.dataloader_num_workers,\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        # ë¡œê¹… ë° ì €ì¥\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        \n",
    "        # ì„±ëŠ¥ ìµœì í™”\n",
    "        remove_unused_columns=False,\n",
    "        optim=\"adamw_torch_fused\",  # PyTorch 2.6 ìµœì í™”\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # ë¡œê¹…\n",
    "        report_to=[],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •\n",
    "        logging_dir=f\"{config.output_dir}/logs\",\n",
    "        \n",
    "        # ì¶”ê°€ ì•ˆì •ì„± ì„¤ì •\n",
    "        save_safetensors=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # ë°ì´í„° ì½œë ˆì´í„°\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LMì´ë¯€ë¡œ MLM ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
    "    )\n",
    "    \n",
    "    # íŠ¸ë ˆì´ë„ˆ ìƒì„± (API ë³€ê²½ì‚¬í•­ ë°˜ì˜)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        processing_class=tokenizer,  # tokenizer ëŒ€ì‹  processing_class ì‚¬ìš©\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # ì¶”ê°€ ê²€ì¦: ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ forward pass í…ŒìŠ¤íŠ¸\n",
    "    print(\"ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ forward pass í…ŒìŠ¤íŠ¸...\")\n",
    "    try:\n",
    "        sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "        sample_batch = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in sample_batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**sample_batch)\n",
    "            print(f\"âœ… Forward pass ì„±ê³µ! Loss: {outputs.loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Forward pass ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ë°ì´í„° ë˜ëŠ” ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    print(\"SOLAR í•œêµ­ì–´ íŒŒì¸íŠœë‹ ì‹œì‘!\")\n",
    "    print(f\"ì´ ìŠ¤í…: {len(train_tokenized) // (config.per_device_train_batch_size * config.gradient_accumulation_steps) * config.num_train_epochs}\")\n",
    "    print(f\"ì˜ˆìƒ ì‹œê°„: ì•½ 6-8ì‹œê°„ (H100)\")\n",
    "    \n",
    "    # í›ˆë ¨ ì‹¤í–‰\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    except Exception as e:\n",
    "        print(f\"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ëª¨ë¸ ìƒíƒœì™€ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        \n",
    "else:\n",
    "    print(\"ë°ì´í„°ì…‹ì´ ì—†ì–´ í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ìœ„ì—ì„œ ì •ì˜í•œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸(ë°ì´í„° ë¡œë”, ë°ì´í„° ìƒì„±ê¸°, íŠ¸ë ˆì´ë„ˆ)ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ ì „ì²´ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **ì‹¤í–‰ ìˆœì„œ**:\n",
    "  1. í™˜ê²½ ì²´í¬\n",
    "  2. í•œêµ­ì–´ ë°ì´í„° ë¡œë“œ\n",
    "  3. (API í‚¤ê°€ ìˆì„ ê²½ìš°) Geminië¡œ WMS ë°ì´í„° ìƒì„±\n",
    "  4. ë°ì´í„°ì…‹ í†µí•©\n",
    "  5. ëª¨ë¸ ì„¤ì • ë° í›ˆë ¨\n",
    "\n",
    "- **ì‹¤í–‰ ë°©ë²•**:\n",
    "  - ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ë©´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ë™ì‘í•©ë‹ˆë‹¤.\n",
    "  - `config` ê°ì²´ì˜ ê°’ì„ ìˆ˜ì •í•˜ì—¬ í›ˆë ¨ ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset is not None:\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    final_path = os.path.join(config.output_dir, \"final\")\n",
    "    print(f\"ëª¨ë¸ ì €ì¥ ì¤‘: {final_path}\")\n",
    "    \n",
    "    trainer.save_model(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    \n",
    "    print(f\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}\")\n",
    "    \n",
    "    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "    def generate_korean_response(prompt, max_length=200):\n",
    "        \"\"\"í•œêµ­ì–´ ì‘ë‹µ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "        # Chat formatìœ¼ë¡œ ì…ë ¥ êµ¬ì„±\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except:\n",
    "            formatted_text = f\"ì‚¬ìš©ì: {prompt}\\nì–´ì‹œìŠ¤í„´íŠ¸:\"\n",
    "        \n",
    "        inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°í•˜ê³  ì‘ë‹µë§Œ ì¶”ì¶œ\n",
    "        if \"ì–´ì‹œìŠ¤í„´íŠ¸:\" in response:\n",
    "            return response.split(\"ì–´ì‹œìŠ¤í„´íŠ¸:\")[-1].strip()\n",
    "        else:\n",
    "            return response[len(formatted_text):].strip()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤\n",
    "    test_prompts = [\n",
    "        \"ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.\",\n",
    "        \"Pythonê³¼ JavaScriptì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "        \"ê±´ê°•í•œ ì‹ë‹¨ì„ ìœ„í•œ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.\",\n",
    "        \"íš¨ê³¼ì ì¸ í•™ìŠµ ë°©ë²•ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== SOLAR í•œêµ­ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n[í…ŒìŠ¤íŠ¸ {i}]\")\n",
    "        print(f\"ì§ˆë¬¸: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            response = generate_korean_response(prompt)\n",
    "            print(f\"ë‹µë³€: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\nâœ… SOLAR-10.7B í•œêµ­ì–´ íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {final_path}\")\n",
    "    print(f\"ğŸ¯ í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ í•œêµ­ì–´ ìƒ˜í”Œ\")\n",
    "    \n",
    "else:\n",
    "    print(\"ë°ì´í„°ì…‹ì´ ì—†ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (ì„ íƒ) Hugging Face ì—…ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "\n",
    "í›ˆë ¨ì´ ì™„ë£Œëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œí•˜ê³ , ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_test(model_path):\n",
    "    # Hugging Face ë¡œê·¸ì¸\n",
    "    if config.hf_token:\n",
    "        login(token=config.hf_token)\n",
    "    \n",
    "    # ëª¨ë¸ ë³‘í•© ë° ì—…ë¡œë“œ\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(config.base_model, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    peft_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    repo_name = f\"{config.hf_username}/{config.model_name}\"\n",
    "    merged_model.push_to_hub(repo_name)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.push_to_hub(repo_name)\n",
    "    \n",
    "    logger.info(f\"âœ… ëª¨ë¸ì„ {repo_name}ì— ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "    logger.info(\"ğŸ” ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "    pipe = pipeline(\"text-generation\", model=repo_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": \"ì°½ê³ ì—ì„œ í”¼í‚¹ íš¨ìœ¨ì„ ë†’ì´ëŠ” ë°©ë²• 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\"}]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    \n",
    "    print(\"\\n--- ì¶”ë¡  ê²°ê³¼ ---\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"-----------------\")\n",
    "\n",
    "# í›ˆë ¨ ì™„ë£Œ í›„ ì‹¤í–‰\n",
    "# final_model_path = os.path.join(config.output_dir, \"final\")\n",
    "# upload_and_test(final_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.6 (NGC 25.01/Python 3.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
