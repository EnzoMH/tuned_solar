#!/usr/bin/env python3
"""
SOLAR-10.7B í•œêµ­ì–´ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
KT Cloud H100E í™˜ê²½ ìµœì í™” ë²„ì „ (5ì‹œê°„ ì œí•œìš©)

í™˜ê²½ ì‚¬ì–‘:
- GPU: H100E 80GB
- RAM: 192GiB
- CPU: 24 Cores
- PyTorch: 2.6
- Python: 3.12
- CUDA: 12.8
"""

import os
import json
import torch
import random
import logging
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass

# Transformers and PEFT
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    prepare_model_for_kbit_training,
    TaskType,
    PeftModel
)

# Datasets
from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets
from torch.utils.data import DataLoader

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# KT Cloud H100E ìµœì í™” Configuration (5ì‹œê°„ ì œí•œìš©)
# =============================================================================

@dataclass
class QLoRAConfig:
    # ëª¨ë¸
    base_model: str = "upstage/SOLAR-10.7B-v1.0"
    
    # ì–‘ìí™”
    load_in_4bit: bool = True
    bnb_4bit_compute_dtype: str = "bfloat16"
    bnb_4bit_quant_type: str = "nf4"
    use_nested_quant: bool = True
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 16
    lora_dropout: float = 0.1
    
    # í›ˆë ¨
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    max_length: int = 2048
    output_dir: str = "./solar-qlora-output"

@dataclass
class KTCloudH100Config:
    """KT Cloud H100E í™˜ê²½ì— ìµœì í™”ëœ ì„¤ì • (5ì‹œê°„ ì œí•œ)"""
    
    # Model settings
    base_model: str = "upstage/SOLAR-10.7B-v1.0"
    model_name: str = "SOLAR-10.7B-Korean-Instruct-Fast"
    
    # KT Cloud H100E ìµœì í™” ì„¤ì • (ë¹ ë¥¸ ì‹¤í–‰)
    output_dir: str = "/home/work/solar-korean-output"
    num_train_epochs: int = 3  # 16ì‹œê°„ í™œìš©: 100ë§Œ ë°ì´í„° Ã— 3ì—í¬í¬
    per_device_train_batch_size: int = 8   # Gradient checkpointingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    gradient_accumulation_steps: int = 2   # íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° = 8 ìœ ì§€
    learning_rate: float = 1.5e-5  # ë” ë³´ìˆ˜ì ì¸ í•™ìŠµë¥ ë¡œ ì•ˆì •ì„± í–¥ìƒ
    weight_decay: float = 0.01
    warmup_ratio: float = 0.05  # warmup ì¦ê°€ë¡œ ì•ˆì •ì„± í–¥ìƒ
    max_length: int = 2048  # ìµœëŒ€ì¶œë ¥ê¸¸ì´
    
    # LoRA settings - SOLARì— ìµœì í™” (16ì‹œê°„ ìµœê³  í’ˆì§ˆ)
    lora_r: int = 64  # 16ì‹œê°„ í™œìš©ìœ¼ë¡œ íŒŒë¼ë¯¸í„° 4ë°° í™•ëŒ€
    lora_alpha: int = 128  # rì— ë¹„ë¡€í•˜ì—¬ ì¡°ì •
    lora_dropout: float = 0.1
    
    # Dataset settings - 16ì‹œê°„ ìµœì í™”ìš©
    max_samples: int = 300000  # ì „ì²´ ë°ì´í„° ì¤‘ 30ë§Œê°œ ì‚¬ìš© (ìµœê³  í’ˆì§ˆ)
    
    # KT Cloud H100E í•˜ë“œì›¨ì–´ ì„¤ì •
    use_4bit: bool = True  # H100EëŠ” ë©”ëª¨ë¦¬ ì¶©ë¶„
    use_bf16: bool = False   # H100Eì—ì„œ BF16 ìµœì 
    use_flash_attention: bool = False  # fp16/bf16 í˜¸í™˜ì„± ë¬¸ì œë¡œ ë¹„í™œì„±í™”
    device_map: str = "auto"
    bnb_4bit_compute_dtype: str = "bfloat16"
    bnb_4bit_quant_type: str = "nf4"
    use_nested_quant: bool = True
    
    # H100E ì„±ëŠ¥ ìµœì í™”
    dataloader_num_workers: int = 8  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¶•ì†Œ
    dataloader_pin_memory: bool = True
    gradient_checkpointing: bool = True  # enable_input_require_grads()ë¡œ í˜¸í™˜ì„± í•´ê²°
    torch_compile: bool = False  # ì•ˆì •ì„± ìœ„í•´ ë¹„í™œì„±í™”
    
    # ì €ì¥ ì„¤ì • (ìì£¼ ì €ì¥)
    save_steps: int = 500
    save_total_limit: int = 10
    logging_steps: int = 50

config = KTCloudH100Config()

# =============================================================================
# KT Cloud H100E í™˜ê²½ ìµœì í™” ì²´í¬
# =============================================================================

def check_kt_cloud_environment():
    """KT Cloud H100E í™˜ê²½ ìµœì í™” ì²´í¬"""
    # Tokenizer ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë°©ì§€
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    logger.info("KT Cloud H100E í™˜ê²½ ì²´í¬ ì‹œì‘")
    
    # PyTorch ë²„ì „ í™•ì¸
    pytorch_version = torch.__version__
    logger.info(f"PyTorch ë²„ì „: {pytorch_version}")
    
    # CUDA ë²„ì „ í™•ì¸
    if torch.cuda.is_available():
        cuda_version = torch.version.cuda
        logger.info(f"CUDA ë²„ì „: {cuda_version}")
        
        # GPU ì •ë³´ í™•ì¸
        gpu_count = torch.cuda.device_count()
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            logger.info(f"GPU {i}: {props.name}, ë©”ëª¨ë¦¬: {memory_gb:.1f}GB")
            
            # H100 í™•ì¸
            if "H100" in props.name:
                logger.info("H100 ê°ì§€! KT Cloud ìµœì í™” ëª¨ë“œ í™œì„±í™”")
    
    # CPU ì •ë³´
    cpu_count = os.cpu_count()
    logger.info(f"CPU ì½”ì–´: {cpu_count}ê°œ")
    
    logger.info("KT Cloud í™˜ê²½ ì²´í¬ ì™„ë£Œ")
    return True

# =============================================================================
# í•œêµ­ì–´ ë°ì´í„° ë¡œë” (ë¹ ë¥¸ ì‹¤í–‰ìš©)
# =============================================================================

class SOLARKoreanDataLoader:
    """SOLAR í•œêµ­ì–´ ë°ì´í„° ë¡œë” (ë¹ ë¥¸ ì‹¤í–‰ìš©)"""
    
    def __init__(self, config: KTCloudH100Config):
        self.config = config
        
    def load_korean_datasets(self) -> Dataset:
        """í•œêµ­ì–´ ë°ì´í„° ë¡œë”© (5ì‹œê°„ ì œí•œìš©)"""
        logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„° ë¡œë”© ì‹œì‘ (ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ)...")
        
        # 1. ì‚¬ì „ ì¤€ë¹„ëœ í•œêµ­ì–´ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ìš°ì„  ì‹œë„
        try:
            data_path = "/home/work/tesseract/korean_data/korean_base_dataset.json"
            if os.path.exists(data_path):
                logger.info("ì‚¬ì „ ì¤€ë¹„ëœ í•œêµ­ì–´ ë°ì´í„° ë°œê²¬!")
                with open(data_path, "r", encoding="utf-8") as f:
                    korean_data = json.load(f)
                
                # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ìƒ˜í”Œë§
                if len(korean_data) > self.config.max_samples:
                    korean_data = random.sample(korean_data, self.config.max_samples)
                    logger.info(f"ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ {self.config.max_samples:,}ê°œ ìƒ˜í”Œë§")
                
                logger.info(f"í•œêµ­ì–´ ê¸°ë³¸ ë°ì´í„° ë¡œë“œ: {len(korean_data):,}ê°œ")
                return Dataset.from_list(korean_data)
                
        except Exception as e:
            logger.warning(f"ì‚¬ì „ ì¤€ë¹„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. ì‹¤ì‹œê°„ ë¡œë“œ (fallback)
        logger.info("ì‹¤ì‹œê°„ í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë”©...")
        datasets_loaded = []
        
        # KoAlpaca - ë¹ ë¥¸ ë¡œë”©
        try:
            logger.info("KoAlpaca ë¡œë”©...")
            koalpaca = load_dataset("beomi/KoAlpaca-v1.1a", split="train")
            koalpaca_sample = koalpaca.select(range(min(30000, len(koalpaca))))
            koalpaca_formatted = self._format_instruction_dataset(koalpaca_sample, "koalpaca")
            datasets_loaded.append(koalpaca_formatted)
            logger.info(f"KoAlpaca: {len(koalpaca_formatted):,}ê°œ")
        except Exception as e:
            logger.warning(f"KoAlpaca ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Ko-Ultrachat - ë¹ ë¥¸ ë¡œë”©
        try:
            logger.info("Ko-Ultrachat ë¡œë”©...")
            ultrachat = load_dataset("maywell/ko_Ultrachat_200k", split="train")
            ultrachat_sample = ultrachat.select(range(min(30000, len(ultrachat))))
            ultrachat_formatted = self._format_chat_dataset(ultrachat_sample)
            datasets_loaded.append(ultrachat_formatted)
            logger.info(f"Ko-Ultrachat: {len(ultrachat_formatted):,}ê°œ")
        except Exception as e:
            logger.warning(f"Ko-Ultrachat ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        if datasets_loaded:
            combined = concatenate_datasets(datasets_loaded)
            
            # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
            if len(combined) > self.config.max_samples:
                indices = random.sample(range(len(combined)), self.config.max_samples)
                combined = combined.select(indices)
                logger.info(f"ë¹ ë¥¸ ì‹¤í–‰ìš© ë°ì´í„°: {len(combined):,}ê°œ")
                
            return combined
        else:
            logger.error("í•œêµ­ì–´ ë°ì´í„° ë¡œë“œ ì™„ì „ ì‹¤íŒ¨")
            return None
    
    def _format_instruction_dataset(self, dataset: Dataset, source: str) -> Dataset:
        """Instruction following í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        def format_example(example):
            instruction = example.get('instruction', '')
            input_text = example.get('input', '')
            output = example.get('output', '')
            
            if input_text and input_text.strip():
                user_content = f"{instruction}\n\n{input_text}"
            else:
                user_content = instruction
                
            return {
                'messages': [
                    {'role': 'user', 'content': user_content},
                    {'role': 'assistant', 'content': output}
                ],
                'source': source
            }
        
        return dataset.map(format_example, num_proc=None)  # ë©€í‹°í”„ë¡œì„¸ì‹± ì™„ì „ ë¹„í™œì„±í™”
    
    def _format_chat_dataset(self, dataset: Dataset) -> Dataset:
        """ì±„íŒ… í˜•ì‹ ë°ì´í„° ì²˜ë¦¬"""
        def format_example(example):
            messages = example.get('messages', [])
            if isinstance(messages, list) and len(messages) >= 2:
                return {'messages': messages, 'source': 'ultrachat'}
            return None
        
        formatted = dataset.map(format_example, num_proc=None)  # ë©€í‹°í”„ë¡œì„¸ì‹± ì™„ì „ ë¹„í™œì„±í™”
        return formatted.filter(lambda x: x is not None)

# =============================================================================
# SOLAR ìµœì í™” íŠ¸ë ˆì´ë„ˆ (5ì‹œê°„ ì œí•œìš©)
# =============================================================================

class SOLARTrainer:
    """SOLAR-10.7Bì— ìµœì í™”ëœ KT Cloud H100E íŠ¸ë ˆì´ë„ˆ (ë¹ ë¥¸ ì‹¤í–‰)"""
    
    def __init__(self, config: KTCloudH100Config):
        self.config = config
        self.model = None
        self.tokenizer = None
        
    def setup_model(self):
        """SOLAR ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •"""
        logger.info(f"SOLAR ëª¨ë¸ ë¡œë”©: {self.config.base_model}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.base_model,
            trust_remote_code=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 1. BitsAndBytes ì„¤ì • ì¶”ê°€
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # KT Cloud H100E ìµœì í™” ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model,
            quantization_config=bnb_config,
            device_map=self.config.device_map,
            trust_remote_code=True,
            use_cache=False,  # í›ˆë ¨ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½
            low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
            attn_implementation="flash_attention_2" if self.config.use_flash_attention else None
        )
        
        logger.info("SOLAR ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Gradient checkpointingê³¼ LoRA í˜¸í™˜ì„± í•´ê²°
        if self.config.gradient_checkpointing:
            self.model.enable_input_require_grads()
            logger.info("enable_input_require_grads() ì ìš© ì™„ë£Œ")
            
            # ì¶”ê°€ ì•ˆì •ì„±ì„ ìœ„í•´ prepare_model_for_kbit_trainingë„ ì ìš©
            self.model = prepare_model_for_kbit_training(
                self.model, 
                use_gradient_checkpointing=True
            )
            logger.info("prepare_model_for_kbit_training ì ìš© ì™„ë£Œ")
        
        # SOLARì— ìµœì í™”ëœ LoRA ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
            use_rslora=False  # ì•ˆì •ì„± ìœ„í•´ ë¹„í™œì„±í™”
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        # í›ˆë ¨ ëª¨ë“œ ì„¤ì •
        self.model.train()
        
        # LoRA íŒŒë¼ë¯¸í„° ê°•ì œ í™œì„±í™” (ë” ê°•ë ¥í•œ ë°©ë²•)
        logger.info("LoRA íŒŒë¼ë¯¸í„° ê°•ì œ í™œì„±í™” ì‹œì‘...")
        lora_params_activated = 0
        
        for name, param in self.model.named_parameters():
            # LoRA ê´€ë ¨ íŒŒë¼ë¯¸í„° ì‹ë³„ ë° ê°•ì œ í™œì„±í™”
            if any(lora_key in name.lower() for lora_key in ['lora_', 'lora_a', 'lora_b']):
                param.requires_grad = True
                param.retain_grad()  # gradientë¥¼ ìœ ì§€
                lora_params_activated += 1
                logger.debug(f"LoRA íŒŒë¼ë¯¸í„° í™œì„±í™”: {name}")
            else:
                # ë² ì´ìŠ¤ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ëª…ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”
                param.requires_grad = False
        
        logger.info(f"LoRA íŒŒë¼ë¯¸í„° í™œì„±í™” ì™„ë£Œ: {lora_params_activated}ê°œ")
        
        # ìµœì¢… ê²€ì¦: í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}ê°œ / ì „ì²´ {total_params:,}ê°œ")
        
        if trainable_params == 0:
            logger.error("í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            raise RuntimeError("No trainable parameters found!")
        
        logger.info("SOLAR ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
    
    def process_dataset(self, dataset: Dataset) -> Dataset:
        """SOLARìš© ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (ë…¸íŠ¸ë¶ ë°©ì‹ ì ìš©)"""
        logger.info("SOLARìš© ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        def tokenize_function(examples):
            """ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ (ì•ˆì „ì„± ê°•í™”)"""
            batch_input_ids = []
            batch_attention_masks = []
            batch_labels = []
            
            for messages in examples['messages']:
                # None ê°’ê³¼ íƒ€ì… ì²´í¬
                if messages is None or not isinstance(messages, list):
                    continue
                
                # Chat template ì ìš© ì‹œë„
                try:
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=False
                    )
                except:
                    # Chat templateì´ ì—†ìœ¼ë©´ ì§ì ‘ í¬ë§·
                    formatted_text = ""
                    for msg in messages:
                        if isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                            if msg['role'] == 'user':
                                formatted_text += f"ì‚¬ìš©ì: {msg['content']}\n"
                            elif msg['role'] == 'assistant':
                                formatted_text += f"Solar: {msg['content']}\n"
                
                # ë¹ˆ í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
                if not formatted_text or not formatted_text.strip():
                    continue
                
                # í† í¬ë‚˜ì´ì§•
                tokenized = self.tokenizer(
                    formatted_text,
                    truncation=True,
                    padding="max_length",
                    max_length=self.config.max_length,
                    return_tensors="pt"
                )
                
                input_ids = tokenized["input_ids"].squeeze(0)
                attention_mask = tokenized["attention_mask"].squeeze(0)
                
                # ë ˆì´ë¸” = input_ids (causal LM)
                labels = input_ids.clone()
                
                batch_input_ids.append(input_ids)
                batch_attention_masks.append(attention_mask)
                batch_labels.append(labels)
            
            # ë¹ˆ ë°°ì¹˜ ì²˜ë¦¬
            if not batch_input_ids:
                # ë”ë¯¸ ë°ì´í„°ë¡œ ë¹ˆ ë°°ì¹˜ ì²˜ë¦¬
                dummy_input = self.tokenizer(
                    "ì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”\nSolar: ì•ˆë…•í•˜ì„¸ìš”!",
                    truncation=True,
                    padding="max_length", 
                    max_length=self.config.max_length,
                    return_tensors="pt"
                )
                return {
                    "input_ids": [dummy_input["input_ids"].squeeze(0)],
                    "attention_mask": [dummy_input["attention_mask"].squeeze(0)],
                    "labels": [dummy_input["input_ids"].squeeze(0)]
                }
            
            return {
                "input_ids": batch_input_ids,
                "attention_mask": batch_attention_masks,
                "labels": batch_labels
            }
        
        processed_dataset = dataset.map(
            tokenize_function,
            batched=True,
            batch_size=32,
            num_proc=None,  # ì™„ì „íˆ ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
            remove_columns=dataset.column_names
        )
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_dataset)}ê°œ")
        return processed_dataset
    
    def train(self, dataset: Dataset):
        """KT Cloud H100E ìµœì í™” í›ˆë ¨ (5ì‹œê°„ ì œí•œ)"""
        logger.info(" KT Cloud H100E ìµœì í™” í›ˆë ¨ ì‹œì‘ (ë¹ ë¥¸ ì‹¤í–‰ ëª¨ë“œ)...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í•  (90%/10%)
        train_size = int(0.9 * len(dataset))
        train_dataset = dataset.select(range(train_size))
        eval_dataset = dataset.select(range(train_size, len(dataset)))
        
        logger.info(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset):,}ê°œ")
        logger.info(f"ê²€ì¦ ë°ì´í„°: {len(eval_dataset):,}ê°œ")
        
        # KT Cloud H100E ìµœì í™” í›ˆë ¨ ì¸ì (5ì‹œê°„ ì œí•œ)
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            
            # H100 ìµœì í™”
            bf16=False,
            gradient_checkpointing=self.config.gradient_checkpointing,
            dataloader_num_workers=self.config.dataloader_num_workers,
            dataloader_pin_memory=self.config.dataloader_pin_memory,
            
            # ë¡œê¹… ë° ì €ì¥ (ìì£¼ ì €ì¥)
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            save_total_limit=self.config.save_total_limit,
            eval_strategy="steps",
            eval_steps=500,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            
            # ì„±ëŠ¥ ìµœì í™”
            remove_unused_columns=False,
            report_to=[],  # ë¡œê¹… ë„êµ¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            logging_dir=f"{self.config.output_dir}/logs",
            
            # PyTorch 2.6 ìµœì í™”
            optim="adamw_torch_fused",
            max_grad_norm=1.0,
            ddp_find_unused_parameters=False,
            save_safetensors=True,
            seed=42
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=self.tokenizer,
            data_collator=data_collator,
        )
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        logger.info("ì²« ë²ˆì§¸ ë°°ì¹˜ forward pass í…ŒìŠ¤íŠ¸...")
        try:
            sample_batch = next(iter(trainer.get_train_dataloader()))
            sample_batch = {k: v.to(self.model.device) if hasattr(v, 'to') else v for k, v in sample_batch.items()}
            
            with torch.no_grad():
                outputs = self.model(**sample_batch)
                logger.info(f"Forward pass ì„±ê³µ! Loss: {outputs.loss:.4f}")
        except Exception as e:
            logger.error(f"Forward pass ì‹¤íŒ¨: {e}")
            return None
        
        # ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
        total_steps = len(train_dataset) // self.config.per_device_train_batch_size * self.config.num_train_epochs
        logger.info(f"ì´ ìŠ¤í…: {total_steps}")
        logger.info(f"ì˜ˆìƒ ì‹œê°„: ì•½ 4-6ì‹œê°„ (H100, ë°°ì¹˜í¬ê¸° {self.config.per_device_train_batch_size}, 30ë§Œ ìƒ˜í”Œ, 3 ì—í¬í¬)")  # 
        
        # í›ˆë ¨ ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ë¶€í„° ì¬ê°œ)
        try:
            # ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
            checkpoints = [d for d in os.listdir(self.config.output_dir) 
                          if d.startswith('checkpoint-')]
            if checkpoints:
                last_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))
                resume_path = os.path.join(self.config.output_dir, last_checkpoint)
                logger.info(f"ì²´í¬í¬ì¸íŠ¸ë¶€í„° ì¬ê°œ: {resume_path}")
                trainer.train(resume_from_checkpoint=resume_path)
            else:
                logger.info("ìƒˆë¡œìš´ í›ˆë ¨ ì‹œì‘")
                trainer.train()
            
            # ëª¨ë¸ ì €ì¥
            final_path = os.path.join(self.config.output_dir, "final")
            trainer.save_model(final_path)
            self.tokenizer.save_pretrained(final_path)
            
            logger.info(f"í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {final_path}")
            return final_path
            
        except Exception as e:
            logger.error(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print(" SOLAR-10.7B í•œêµ­ì–´ íŒŒì¸íŠœë‹ (5ì‹œê°„ ì œí•œ ë²„ì „)")
    print("=" * 80)
    print("ëª©í‘œ: ë¹ ë¥¸ í•œêµ­ì–´ ëŠ¥ë ¥ í–¥ìƒ")
    print(f"ë°ì´í„°: ìµœëŒ€ {config.max_samples:,}ê°œ ìƒ˜í”Œ")
    print(f"ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„")
    print(f"í™˜ê²½: KT Cloud H100E")
    print("-" * 80)
    
    start_time = datetime.now()
    logger.info(f" íŒŒì¸íŠœë‹ ì‹œì‘: {start_time}")
    
    # Step 1: í™˜ê²½ ì²´í¬
    logger.info("Step 1: KT Cloud H100E í™˜ê²½ ì²´í¬")
    if not check_kt_cloud_environment():
        logger.error("í™˜ê²½ ì²´í¬ ì‹¤íŒ¨")
        return False
    
    # Step 2: í•œêµ­ì–´ ë°ì´í„° ë¡œë“œ
    logger.info("Step 2: í•œêµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„")
    korean_loader = SOLARKoreanDataLoader(config)
    korean_dataset = korean_loader.load_korean_datasets()
    
    if korean_dataset is None:
        logger.error("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return False
    
    # Step 3: ëª¨ë¸ ì„¤ì • ë° í›ˆë ¨
    logger.info("Step 3: SOLAR ëª¨ë¸ í›ˆë ¨")
    trainer = SOLARTrainer(config)
    trainer.setup_model()
    
    processed_dataset = trainer.process_dataset(korean_dataset)
    model_path = trainer.train(processed_dataset)
    
    # ì™„ë£Œ
    end_time = datetime.now()
    duration = end_time - start_time
    
    if model_path:
        logger.info("íŒŒì¸íŠœë‹ ì„±ê³µ!")
        logger.info(f"ëª¨ë¸ ìœ„ì¹˜: {model_path}")
        logger.info(f"ì´ ì†Œìš”ì‹œê°„: {duration}")
    else:
        logger.error("íŒŒì¸íŠœë‹ ì‹¤íŒ¨")
    
    return model_path is not None

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)