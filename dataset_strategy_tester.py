#!/usr/bin/env python3
"""
SOLAR checkpoint-1385 ê¸°ë°˜ ë°ì´í„°ì…‹ ì „ëµ ìˆ˜ë¦½ ë„êµ¬
ë¹ ë¥¸ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ íŒ¨í„´ í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings
import time
import json
warnings.filterwarnings('ignore')

# CUDA ìµœì í™”
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

class DatasetStrategyTester:
    def __init__(self, checkpoint_path="/home/work/tesseract/solar-korean-output/checkpoint-1385"):
        self.checkpoint_path = checkpoint_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def load_model(self):
        """ëª¨ë¸ ë¡œë”©"""
        print("ğŸš€ ë°ì´í„°ì…‹ ì „ëµ í…ŒìŠ¤í„° ë¡œë”©...")
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "upstage/SOLAR-10.7B-v1.0",
            device_map="auto",
            quantization_config=bnb_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16
        )
        
        self.model = PeftModel.from_pretrained(base_model, self.checkpoint_path)
        self.model.eval()
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # ì›Œë°ì—…
        self._warmup()
        print("âœ… í…ŒìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ!")
        
    def _warmup(self):
        """ì›Œë°ì—…"""
        dummy = self.tokenizer("í…ŒìŠ¤íŠ¸", return_tensors="pt").to(self.device)
        with torch.no_grad():
            self.model.generate(**dummy, max_new_tokens=3, do_sample=False)
    
    def test_data_patterns(self):
        """ë‹¤ì–‘í•œ ë°ì´í„° íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        
        print("\nğŸ¯ ë°ì´í„°ì…‹ íŒ¨í„´ë³„ ì„±ëŠ¥ ë¶„ì„")
        print("="*60)
        
        # ë°ì´í„° íŒ¨í„´ë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_patterns = {
            "ë‹¨ë‹µí˜• QA": [
                "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
                "1+1ì€?", 
                "íŒŒì´ì¬ì´ë€?",
                "ê¹€ì¹˜ëŠ” ë¬´ì—‡?",
                "AI ëœ»ì€?"
            ],
            
            "ì„¤ëª…í˜• QA": [
                "í•œêµ­ì˜ ìˆ˜ë„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì¸ê³µì§€ëŠ¥ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ì¢‹ì€ í•™ìŠµ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ],
            
            "ëŒ€í™”í˜•": [
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?",
                "ìš”ì¦˜ ë­˜ í•˜ë©° ì§€ë‚´ì‹œë‚˜ìš”?",
                "ì·¨ë¯¸ê°€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì¢‹ì•„í•˜ëŠ” ìŒì‹ì´ ìˆë‚˜ìš”?",
                "ì£¼ë§ì—ëŠ” ë­˜ í•˜ì‹œë‚˜ìš”?"
            ],
            
            "ì „ë¬¸ì ": [
                "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì€?",
                "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
                "ë”¥ëŸ¬ë‹ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì›ë¦¬ëŠ”?",
                "ìì—°ì–´ì²˜ë¦¬ì—ì„œ í† í°í™”ë€ ë¬´ì—‡ì¸ê°€?",
                "ì»´í“¨í„° ë¹„ì „ì—ì„œ CNNì˜ ì—­í• ì€?"
            ],
            
            "ì°½ì˜ì ": [
                "ë¯¸ë˜ì˜ AI ì„¸ìƒì„ ìƒìƒí•´ì„œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”",
                "ë§Œì•½ í•˜ë£¨ê°€ 48ì‹œê°„ì´ë¼ë©´ ì–´ë–¨ê¹Œìš”?",
                "ë¡œë´‡ê³¼ ì¸ê°„ì´ í•¨ê»˜ ì‚¬ëŠ” ì„¸ìƒì„ ê·¸ë ¤ë³´ì„¸ìš”",
                "100ë…„ í›„ ê¸°ìˆ ì€ ì–´ë–»ê²Œ ë°œì „í• ê¹Œìš”?",
                "ë§Œì•½ ì‹œê°„ì—¬í–‰ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ì–´ë””ë¡œ ê°€ê³ ì‹¶ë‚˜ìš”?"
            ]
        }
        
        results = {}
        
        for pattern_name, questions in test_patterns.items():
            print(f"\nğŸ”¸ {pattern_name} íŒ¨í„´ í…ŒìŠ¤íŠ¸")
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
            answers, batch_time = self._batch_generate(questions, max_tokens=50)
            
            # í’ˆì§ˆ ë¶„ì„
            quality_score = self._analyze_quality(questions, answers)
            
            results[pattern_name] = {
                "ì²˜ë¦¬ì‹œê°„": batch_time,
                "í‰ê· ì‹œê°„": batch_time / len(questions),
                "í’ˆì§ˆì ìˆ˜": quality_score,
                "ìƒ˜í”Œ": list(zip(questions[:2], answers[:2]))  # ìƒ˜í”Œ 2ê°œë§Œ
            }
            
            print(f"   ì²˜ë¦¬ì‹œê°„: {batch_time:.2f}ì´ˆ ({batch_time/len(questions):.2f}ì´ˆ/ì§ˆë¬¸)")
            print(f"   í’ˆì§ˆì ìˆ˜: {quality_score:.1f}/10")
            
            # ìƒ˜í”Œ ì¶œë ¥
            for q, a in list(zip(questions, answers))[:2]:
                print(f"   Q: {q}")
                print(f"   A: {a[:100]}..." if len(a) > 100 else f"   A: {a}")
                print()
        
        return results
    
    def _batch_generate(self, questions, max_tokens=50):
        """ë°°ì¹˜ ìƒì„±"""
        prompts = [f"ì§ˆë¬¸: {q}\në‹µë³€:" for q in questions]
        
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            truncation=True,
            max_length=400,
            padding=True
        ).to(self.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.3,
                    top_p=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    num_beams=1
                )
        
        batch_time = time.time() - start_time
        
        # ì‘ë‹µ ì¶”ì¶œ
        answers = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            if "ë‹µë³€:" in response:
                response = response.split("ë‹µë³€:")[-1].strip()
            answers.append(response)
            
        return answers, batch_time
    
    def _analyze_quality(self, questions, answers):
        """ë‹µë³€ í’ˆì§ˆ ë¶„ì„ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        total_score = 0
        
        for q, a in zip(questions, answers):
            score = 5  # ê¸°ë³¸ ì ìˆ˜
            
            # ê¸¸ì´ ì²´í¬
            if len(a.strip()) < 5:
                score -= 3
            elif len(a.strip()) < 20:
                score -= 1
            elif len(a.strip()) > 200:
                score += 1
                
            # í•œêµ­ì–´ ë¹„ìœ¨ ì²´í¬ (ê°„ë‹¨íˆ)
            korean_chars = sum(1 for c in a if '\uac00' <= c <= '\ud7af')
            if len(a) > 0:
                korean_ratio = korean_chars / len(a)
                if korean_ratio > 0.3:
                    score += 2
                    
            # ë°˜ë³µ ì²´í¬
            words = a.split()
            if len(words) > len(set(words)) * 1.5:
                score -= 2
                
            # URLì´ë‚˜ íŠ¹ìˆ˜ë¬¸ì ì²´í¬
            if 'http' in a or '@@' in a or '##' in a:
                score -= 2
                
            total_score += max(0, min(10, score))
            
        return total_score / len(questions) if questions else 0
    
    def recommend_dataset_strategy(self, results):
        """ë°ì´í„°ì…‹ êµ¬ì„± ì „ëµ ì¶”ì²œ"""
        
        print("\nğŸ¯ ë°ì´í„°ì…‹ êµ¬ì„± ì „ëµ ì¶”ì²œ")
        print("="*60)
        
        # íŒ¨í„´ë³„ ì„±ëŠ¥ ìˆœìœ„
        performance_ranking = sorted(
            results.items(), 
            key=lambda x: x[1]['í’ˆì§ˆì ìˆ˜'], 
            reverse=True
        )
        
        print("ğŸ“Š íŒ¨í„´ë³„ í’ˆì§ˆ ìˆœìœ„:")
        for i, (pattern, data) in enumerate(performance_ranking, 1):
            print(f"   {i}. {pattern}: {data['í’ˆì§ˆì ìˆ˜']:.1f}ì  ({data['í‰ê· ì‹œê°„']:.2f}ì´ˆ)")
        
        # ì „ëµ ì¶”ì²œ
        best_pattern = performance_ranking[0][0]
        worst_pattern = performance_ranking[-1][0]
        
        print(f"\nğŸ’¡ ì¶”ì²œ ì „ëµ:")
        print(f"   âœ… ê°•í™”í•  íŒ¨í„´: {best_pattern} (í˜„ì¬ ìµœê³  ì„±ëŠ¥)")
        print(f"   âš ï¸  ê°œì„  í•„ìš”: {worst_pattern} (ì¶”ê°€ ë°ì´í„° í•„ìš”)")
        
        # ë°ì´í„°ì…‹ ë¹„ìœ¨ ì¶”ì²œ
        total_quality = sum(data['í’ˆì§ˆì ìˆ˜'] for data in results.values())
        
        print(f"\nğŸ“‹ ê¶Œì¥ ë°ì´í„°ì…‹ ë¹„ìœ¨:")
        for pattern, data in results.items():
            ratio = (data['í’ˆì§ˆì ìˆ˜'] / total_quality) * 100 if total_quality > 0 else 20
            print(f"   {pattern}: {ratio:.0f}%")
            
        print(f"\nâš¡ ì²˜ë¦¬ íš¨ìœ¨ì„±:")
        avg_time = sum(data['í‰ê· ì‹œê°„'] for data in results.values()) / len(results)
        print(f"   í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.2f}ì´ˆ/ì§ˆë¬¸")
        print(f"   ë°°ì¹˜ ì²˜ë¦¬ ê¶Œì¥: 5-10ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    
    print("ğŸ¯ SOLAR-1385 ë°ì´í„°ì…‹ ì „ëµ ìˆ˜ë¦½ ë„êµ¬")
    print("="*60)
    
    tester = DatasetStrategyTester()
    tester.load_model()
    
    # íŒ¨í„´ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.test_data_patterns()
    
    # ì „ëµ ì¶”ì²œ
    tester.recommend_dataset_strategy(results)
    
    # ê²°ê³¼ ì €ì¥
    with open('/home/work/tesseract/dataset_strategy_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: dataset_strategy_analysis.json")
    print("ğŸ‰ ë°ì´í„°ì…‹ ì „ëµ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
