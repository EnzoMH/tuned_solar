{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLAR-10.7B 한국어 WMS 파인튜닝 (KT Cloud H100E 최적화)\n",
    "\n",
    "이 노트북은 Upstage의 SOLAR-10.7B 모델을 KT Cloud H100E 환경에서 한국어 WMS(창고관리시스템) 도메인에 특화하여 파인튜닝하는 전체 과정을 담고 있습니다.\n",
    "\n",
    "### 주요 단계\n",
    "1. **환경 설정**: 필요한 라이브러리 설치 및 환경 체크\n",
    "2. **데이터 준비**:\n",
    "   - 대규모 한국어 데이터셋 로드\n",
    "   - Gemini 1.5 Flash API를 활용한 WMS 전문 데이터 생성\n",
    "3. **모델 설정**: SOLAR-10.7B 모델 로딩 및 LoRA 설정\n",
    "4. **훈련**: KT Cloud H100E에 최적화된 설정으로 모델 파인튜닝\n",
    "5. **평가 및 업로드**: (옵션) 훈련된 모델 테스트 및 Hugging Face Hub에 업로드\n",
    "\n",
    "### 최적화 환경\n",
    "- **GPU**: NVIDIA H100E\n",
    "- **Framework**: PyTorch 2.6, CUDA 12.8\n",
    "- **Techniques**: Flash Attention 2, BF16, Fused AdamW, LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환경 안정화 완료!\n"
     ]
    }
   ],
   "source": [
    "# <<< 반드시 가장 첫 셀에 실행 >>>\n",
    "import os, sys\n",
    "base = \"/home/work\"  # 쓰기 가능한 경로\n",
    "\n",
    "pkg_root = os.path.join(base, \"transformer_engine\")\n",
    "pkg_common = os.path.join(pkg_root, \"common\")\n",
    "\n",
    "os.makedirs(pkg_common, exist_ok=True)\n",
    "\n",
    "# 빈 모듈들 생성: __init__.py, pytorch.py, common/__init__.py\n",
    "open(os.path.join(pkg_root, \"__init__.py\"), \"w\").close()\n",
    "open(os.path.join(pkg_root, \"pytorch.py\"), \"w\").close()\n",
    "open(os.path.join(pkg_common, \"__init__.py\"), \"w\").close()\n",
    "\n",
    "# 이 경로를 파이썬 검색 경로 최우선으로 (시스템 TE 대신 우리가 만든 더미를 잡게)\n",
    "if base not in sys.path:\n",
    "    sys.path.insert(0, base)\n",
    "\n",
    "# (권장) device_map 환경변수 오염 방지 + 단일 GPU 고정\n",
    "for k in (\"ACCELERATE_USE_DEVICE_MAP\", \"HF_ACCELERATE_USE_DEVICE_MAP\"):\n",
    "    os.environ.pop(k, None)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"환경 안정화 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 설정 및 임포트\n",
    "\n",
    "파인튜닝에 필요한 모든 라이브러리를 임포트하고, 기본 설정을 정의합니다.\n",
    "- `KTCloudH100Config` 클래스를 통해 모든 하이퍼파라미터와 설정을 관리합니다.\n",
    "- Gemini API 키, Hugging Face 사용자명 등은 이 단계에서 설정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers>=4.45.0 in /home/work/.local/lib/python3.12/site-packages (4.56.2)\n",
      "Requirement already satisfied: peft==0.17.1 in /home/work/.local/lib/python3.12/site-packages (0.17.1)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/work/.local/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.1 in /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg (0.45.4.dev0)\n",
      "Collecting bitsandbytes>=0.43.1\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.4 in /home/work/.local/lib/python3.12/site-packages (0.6.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/work/.local/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.16.0 in /home/work/.local/lib/python3.12/site-packages (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/work/.local/lib/python3.12/site-packages (from peft==0.17.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/work/.local/lib/python3.12/site-packages (from peft==0.17.1) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (2.6.0a0+ecf3bae40a.nv25.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.17.1) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (3.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/work/.local/lib/python3.12/site-packages (from transformers>=4.45.0) (0.22.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/work/.local/lib/python3.12/site-packages (from datasets>=2.0.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/work/.local/lib/python3.12/site-packages (from huggingface_hub>=0.16.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/work/.local/lib/python3.12/site-packages (from huggingface_hub>=0.16.0) (1.1.10)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (3.11.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.45.0) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.17.1) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.0.0) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.0.0) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.0.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.1) (2.0.1)\n",
      "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.47.0\n",
      "패키지 설치 완료! 커널을 재시작해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 필수 패키지 설치\n",
    "!pip install --user -U \\\n",
    "\"transformers>=4.45.0\" \\\n",
    "\"peft==0.17.1\" \\\n",
    "\"accelerate>=0.34.0\" \\\n",
    "\"bitsandbytes>=0.43.1\" \\\n",
    "\"safetensors>=0.4.4\" \\\n",
    "\"datasets>=2.0.0\" \\\n",
    "\"huggingface_hub>=0.16.0\"\n",
    "\n",
    "print(\"패키지 설치 완료! 커널을 재시작해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 라이브러리 임포트 완료!\n",
      "PyTorch 버전: 2.6.0a0+ecf3bae40a.nv25.01\n",
      "CUDA 사용 가능: True\n",
      "GPU 디바이스: NVIDIA H100 80GB HBM3\n",
      "GPU 메모리: 79.2GB\n",
      "H100 감지! 최적화 모드 활성화\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "print(\"모든 라이브러리 임포트 완료!\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 디바이스: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU 메모리: {gpu_memory:.1f}GB\")\n",
    "    \n",
    "    # H100 확인\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"H100\" in gpu_name:\n",
    "        print(\"H100 감지! 최적화 모드 활성화\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정 완료!\n",
      "기반 모델: upstage/SOLAR-10.7B-v1.0\n",
      "출력 디렉토리: /home/work/solar-korean-output\n",
      "배치 크기: 8 x 2 = 16\n",
      "LoRA 설정: r=32, alpha=64\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SOLARKoreanConfig:\n",
    "    \"\"\"SOLAR-10.7B 한국어 파인튜닝 설정\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    base_model: str = \"upstage/SOLAR-10.7B-v1.0\"\n",
    "    model_name: str = \"SOLAR-10.7B-Korean-Instruct\"\n",
    "    \n",
    "    # Training settings\n",
    "    output_dir: str = \"/home/work/solar-korean-output\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 8  # H100E에서 안전한 배치 크기\n",
    "    gradient_accumulation_steps: int = 2  # effective batch size = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.03\n",
    "    max_length: int = 2048\n",
    "    \n",
    "    # LoRA settings - SOLAR에 최적화\n",
    "    lora_r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # Hardware settings\n",
    "    use_4bit: bool = False  # H100은 메모리 충분\n",
    "    use_bf16: bool = True\n",
    "    device_map: str = \"auto\"\n",
    "    \n",
    "    # Performance settings\n",
    "    dataloader_num_workers: int = 8\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "# 설정 인스턴스 생성\n",
    "config = SOLARKoreanConfig()\n",
    "\n",
    "print(\"설정 완료!\")\n",
    "print(f\"기반 모델: {config.base_model}\")\n",
    "print(f\"출력 디렉토리: {config.output_dir}\")\n",
    "print(f\"배치 크기: {config.per_device_train_batch_size} x {config.gradient_accumulation_steps} = {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"LoRA 설정: r={config.lora_r}, alpha={config.lora_alpha}\")\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 파일 로드 중: /home/work/tesseract/korean_base_dataset.json\n",
      "총 데이터 개수: 173,785개\n",
      "\n",
      "데이터 구조:\n",
      "- instruction: 32\n",
      "- output: 342\n",
      "- messages: 2\n",
      "\n",
      "데이터 전처리 중...\n",
      "훈련 데이터: 156,406개\n",
      "검증 데이터: 17,379개\n",
      "\n",
      "데이터 전처리 완료!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 파일 확인 및 로드\n",
    "data_path = \"/home/work/tesseract/korean_base_dataset.json\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"데이터 파일 로드 중: {data_path}\")\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        korean_data = json.load(f)\n",
    "    \n",
    "    print(f\"총 데이터 개수: {len(korean_data):,}개\")\n",
    "    \n",
    "    # 데이터 구조 확인\n",
    "    sample = korean_data[0]\n",
    "    print(\"\\n데이터 구조:\")\n",
    "    print(f\"- instruction: {len(sample.get('instruction', ''))}\")\n",
    "    print(f\"- output: {len(sample.get('output', ''))}\")\n",
    "    print(f\"- messages: {len(sample.get('messages', []))}\")\n",
    "    \n",
    "    # 전처리 함수\n",
    "    def preprocess_korean_data(data_list):\n",
    "        \"\"\"한국어 데이터를 chat 형태로 전처리\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in data_list:\n",
    "            # messages 필드가 있으면 사용, 없으면 instruction-output으로 생성\n",
    "            if 'messages' in item and item['messages']:\n",
    "                messages = item['messages']\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": item['instruction']},\n",
    "                    {\"role\": \"assistant\", \"content\": item['output']}\n",
    "                ]\n",
    "            \n",
    "            processed.append({\n",
    "                \"messages\": messages,\n",
    "                \"source\": \"korean_base\"\n",
    "            })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # 전처리 실행\n",
    "    print(\"\\n데이터 전처리 중...\")\n",
    "    processed_data = preprocess_korean_data(korean_data)\n",
    "    \n",
    "    # 훈련/검증 분할 (90%/10%)\n",
    "    train_size = int(0.9 * len(processed_data))\n",
    "    train_data = processed_data[:train_size]\n",
    "    eval_data = processed_data[train_size:]\n",
    "    \n",
    "    print(f\"훈련 데이터: {len(train_data):,}개\")\n",
    "    print(f\"검증 데이터: {len(eval_data):,}개\")\n",
    "    \n",
    "    # Dataset 객체 생성\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    eval_dataset = Dataset.from_list(eval_data)\n",
    "    \n",
    "    print(\"\\n데이터 전처리 완료!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"데이터 파일을 찾을 수 없습니다: {data_path}\")\n",
    "    train_dataset = None\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KT Cloud 환경 체크\n",
    "\n",
    "현재 실행 환경이 H100E에 최적화되어 있는지 확인합니다.\n",
    "- PyTorch, CUDA 버전 체크\n",
    "- GPU 종류 및 메모리 확인\n",
    "- Flash Attention 지원 여부 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR 모델 로딩: upstage/SOLAR-10.7B-v1.0\n",
      "토크나이저 로드 완료\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb512cc87684b028787c09d407ee109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLAR 모델 로드 완료 (Full precision)\n",
      "모델 파라미터 수: 10,731,524,096\n"
     ]
    }
   ],
   "source": [
    "print(f\"SOLAR 모델 로딩: {config.base_model}\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 패딩 토큰 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(\"토크나이저 로드 완료\")\n",
    "\n",
    "# 모델 로드 (H100은 메모리가 충분하므로 양자화 없이)\n",
    "if not config.use_4bit:\n",
    "    # Full precision 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model,\n",
    "        device_map=config.device_map,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False  # 훈련 시 메모리 절약\n",
    "    )\n",
    "    print(\"SOLAR 모델 로드 완료 (Full precision)\")\n",
    "else:\n",
    "    # 양자화 버전 (필요시)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.base_model,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=config.device_map,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"SOLAR 모델 로드 완료 (4-bit 양자화)\")\n",
    "\n",
    "print(f\"모델 파라미터 수: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gemini API를 활용한 WMS 데이터 생성\n",
    "\n",
    "Gemini 1.5 Flash 모델을 사용하여 WMS 도메인에 특화된 고품질 질의응답 데이터셋을 생성합니다.\n",
    "- **API 키 로테이션**: 4개의 API 키를 번갈아 사용하여 API 제한(rate limit)을 최소화합니다.\n",
    "- **비동기 처리**: `asyncio`를 사용하여 데이터 생성을 병렬로 처리하고 속도를 높입니다.\n",
    "- **전문 프롬프트**: 재고관리, 물류운영, WMS 기술 등 세분화된 프롬프트를 사용하여 데이터의 전문성을 높입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 어댑터 적용 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/work/.local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 125,829,120 || all params: 10,857,353,216 || trainable%: 1.1589\n",
      "LoRA 설정 완료!\n"
     ]
    }
   ],
   "source": [
    "# 일곱 번째 셀 수정버전\n",
    "# QLoRA/LoRA 준비 - gradient checkpointing 비활성화 버전\n",
    "if config.use_4bit:\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "    print(\"4-bit 모델 훈련 준비 완료\")\n",
    "\n",
    "# SOLAR에 최적화된 LoRA 설정  \n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    use_rslora=False  # RSLoRA도 비활성화해서 안정성 확보\n",
    ")\n",
    "\n",
    "print(\"LoRA 어댑터 적용 중...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Enable gradient for LoRA parameters explicitly\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param.retain_grad()  # 명시적으로 gradient 유지\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "print(\"LoRA 설정 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 한국어 데이터 로더\n",
    "\n",
    "SOLAR 모델 파인튜닝에 사용할 대규모 한국어 데이터셋을 로드하고 전처리합니다.\n",
    "- **다양한 데이터셋 활용**: KoAlpaca (Instruction), Ko-Ultrachat (대화), KULLM (고품질 대화) 등 여러 데이터셋을 통합하여 모델의 한국어 능력을 종합적으로 향상시킵니다.\n",
    "- **병렬 처리**: `num_proc` 옵션을 사용하여 데이터 전처리 속도를 높입니다.\n",
    "- **일관된 형식**: 모든 데이터를 `messages` 형식으로 통일하여 훈련 데이터 구조를 일관성 있게 유지합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 토크나이징 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654e2fe17286442eab9dbc3fd48ab449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/156406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979fdf6935064db88263dd668edb4b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/17379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징 완료!\n",
      "훈련 샘플: 156406\n",
      "검증 샘플: 17379\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"데이터셋 토크나이징 함수\"\"\"\n",
    "    batch_input_ids = []\n",
    "    batch_attention_masks = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for messages in examples['messages']:\n",
    "        # Chat template 적용\n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "        except:\n",
    "            # Chat template이 없으면 직접 포맷\n",
    "            formatted_text = \"\"\n",
    "            for msg in messages:\n",
    "                if msg['role'] == 'user':\n",
    "                    formatted_text += f\"사용자: {msg['content']}\\n\"\n",
    "                elif msg['role'] == 'assistant':\n",
    "                    formatted_text += f\"어시스턴트: {msg['content']}\\n\"\n",
    "        \n",
    "        # 토크나이징\n",
    "        tokenized = tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        # 레이블 = input_ids (causal LM)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        batch_input_ids.append(input_ids)\n",
    "        batch_attention_masks.append(attention_mask)\n",
    "        batch_labels.append(labels)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": batch_input_ids,\n",
    "        \"attention_mask\": batch_attention_masks,\n",
    "        \"labels\": batch_labels\n",
    "    }\n",
    "\n",
    "print(\"데이터셋 토크나이징 중...\")\n",
    "if train_dataset is not None:\n",
    "    # 토크나이징 실행\n",
    "    train_tokenized = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    eval_tokenized = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        num_proc=4,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    print(f\"토크나이징 완료!\")\n",
    "    print(f\"훈련 샘플: {len(train_tokenized)}\")\n",
    "    print(f\"검증 샘플: {len(eval_tokenized)}\")\n",
    "else:\n",
    "    print(\"데이터셋이 없어 토크나이징을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SOLAR 최적화 트레이너\n",
    "\n",
    "본격적인 모델 훈련을 담당하는 `SOLARTrainer` 클래스입니다.\n",
    "- **모델 및 토크나이저 설정**: `setup_model`\n",
    "  - H100E 환경에 맞춰 `bfloat16`과 `Flash Attention 2`를 사용하여 모델을 로드합니다.\n",
    "  - `torch.compile`을 적용하여 PyTorch 2.x의 최신 성능 최적화를 활용합니다.\n",
    "  - `LoRA` 설정을 통해 효율적인 파인튜닝을 준비합니다.\n",
    "- **데이터셋 전처리**: `process_dataset`\n",
    "  - 모든 텍스트 데이터를 모델이 이해할 수 있는 토큰 ID로 변환합니다.\n",
    "- **훈련 실행**: `train`\n",
    "  - `TrainingArguments`를 통해 H100E에 최적화된 하이퍼파라미터를 설정합니다.\n",
    "  - `Trainer`를 사용하여 실제 훈련을 진행하고, 최종 모델을 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 가능한 파라미터 상태 확인...\n",
      "훈련 가능한 파라미터: 125,829,120 / 10,857,353,216 (1.16%)\n",
      "첫 번째 배치로 forward pass 테스트...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Forward pass 성공! Loss: 1.9100\n",
      "SOLAR 한국어 파인튜닝 시작!\n",
      "총 스텝: 29325\n",
      "예상 시간: 약 6-8시간 (H100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='29328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   87/29328 12:38 < 72:26:29, 0.11 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_dataset is not None:\n",
    "    # 모델 trainable 파라미터 확인 및 수정\n",
    "    print(\"훈련 가능한 파라미터 상태 확인...\")\n",
    "    \n",
    "    # PEFT 모델의 경우 훈련 모드 명시적 설정\n",
    "    model.train()\n",
    "    \n",
    "    # LoRA 파라미터가 제대로 훈련 가능한지 확인\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for param in model.parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"훈련 가능한 파라미터: {trainable_params:,} / {all_params:,} ({trainable_params/all_params*100:.2f}%)\")\n",
    "    \n",
    "    if trainable_params == 0:\n",
    "        print(\"❌ 훈련 가능한 파라미터가 없습니다! LoRA 설정을 다시 확인하세요.\")\n",
    "        # LoRA 파라미터 강제 활성화\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora_' in name:\n",
    "                param.requires_grad = True\n",
    "                print(f\"LoRA 파라미터 활성화: {name}\")\n",
    "    \n",
    "    # 훈련 인자 설정 (API 변경사항 반영)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        \n",
    "        # H100 최적화\n",
    "        bf16=config.use_bf16,\n",
    "        gradient_checkpointing=config.gradient_checkpointing,\n",
    "        dataloader_num_workers=config.dataloader_num_workers,\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        # 로깅 및 저장\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        \n",
    "        # 성능 최적화\n",
    "        remove_unused_columns=False,\n",
    "        optim=\"adamw_torch_fused\",  # PyTorch 2.6 최적화\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # 로깅\n",
    "        report_to=[],  # 빈 리스트로 설정\n",
    "        logging_dir=f\"{config.output_dir}/logs\",\n",
    "        \n",
    "        # 추가 안정성 설정\n",
    "        save_safetensors=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # 데이터 콜레이터\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM이므로 MLM 사용하지 않음\n",
    "    )\n",
    "    \n",
    "    # 트레이너 생성 (API 변경사항 반영)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        processing_class=tokenizer,  # tokenizer 대신 processing_class 사용\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # 추가 검증: 첫 번째 배치로 forward pass 테스트\n",
    "    print(\"첫 번째 배치로 forward pass 테스트...\")\n",
    "    try:\n",
    "        sample_batch = next(iter(trainer.get_train_dataloader()))\n",
    "        sample_batch = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in sample_batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**sample_batch)\n",
    "            print(f\"✅ Forward pass 성공! Loss: {outputs.loss:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Forward pass 실패: {e}\")\n",
    "        print(\"데이터 또는 모델 설정을 확인하세요.\")\n",
    "    \n",
    "    print(\"SOLAR 한국어 파인튜닝 시작!\")\n",
    "    print(f\"총 스텝: {len(train_tokenized) // (config.per_device_train_batch_size * config.gradient_accumulation_steps) * config.num_train_epochs}\")\n",
    "    print(f\"예상 시간: 약 6-8시간 (H100)\")\n",
    "    \n",
    "    # 훈련 실행\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"훈련 완료!\")\n",
    "    except Exception as e:\n",
    "        print(f\"훈련 중 오류 발생: {e}\")\n",
    "        print(\"모델 상태와 데이터를 다시 확인하세요.\")\n",
    "        \n",
    "else:\n",
    "    print(\"데이터셋이 없어 훈련을 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 통합 파이프라인 실행\n",
    "\n",
    "이제 위에서 정의한 모든 컴포넌트(데이터 로더, 데이터 생성기, 트레이너)를 하나로 묶어 전체 파인튜닝 파이프라인을 실행합니다.\n",
    "\n",
    "- **실행 순서**:\n",
    "  1. 환경 체크\n",
    "  2. 한국어 데이터 로드\n",
    "  3. (API 키가 있을 경우) Gemini로 WMS 데이터 생성\n",
    "  4. 데이터셋 통합\n",
    "  5. 모델 설정 및 훈련\n",
    "\n",
    "- **실행 방법**:\n",
    "  - 아래 셀을 실행하면 전체 파이프라인이 동작합니다.\n",
    "  - `config` 객체의 값을 수정하여 훈련 설정을 변경할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dataset is not None:\n",
    "    # 모델 저장\n",
    "    final_path = os.path.join(config.output_dir, \"final\")\n",
    "    print(f\"모델 저장 중: {final_path}\")\n",
    "    \n",
    "    trainer.save_model(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    \n",
    "    print(f\"모델 저장 완료: {final_path}\")\n",
    "    \n",
    "    # 간단한 테스트\n",
    "    def generate_korean_response(prompt, max_length=200):\n",
    "        \"\"\"한국어 응답 생성 함수\"\"\"\n",
    "        # Chat format으로 입력 구성\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except:\n",
    "            formatted_text = f\"사용자: {prompt}\\n어시스턴트:\"\n",
    "        \n",
    "        inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 프롬프트 부분 제거하고 응답만 추출\n",
    "        if \"어시스턴트:\" in response:\n",
    "            return response.split(\"어시스턴트:\")[-1].strip()\n",
    "        else:\n",
    "            return response[len(formatted_text):].strip()\n",
    "    \n",
    "    # 테스트 질문들\n",
    "    test_prompts = [\n",
    "        \"안녕하세요! 자기소개를 해주세요.\",\n",
    "        \"Python과 JavaScript의 차이점을 설명해주세요.\",\n",
    "        \"건강한 식단을 위한 조언을 해주세요.\",\n",
    "        \"효과적인 학습 방법에 대해 알려주세요.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== SOLAR 한국어 모델 테스트 ===\")\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n[테스트 {i}]\")\n",
    "        print(f\"질문: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            response = generate_korean_response(prompt)\n",
    "            print(f\"답변: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\n✅ SOLAR-10.7B 한국어 파인튜닝 완료!\")\n",
    "    print(f\"📁 모델 위치: {final_path}\")\n",
    "    print(f\"🎯 훈련 데이터: {len(train_data):,}개 한국어 샘플\")\n",
    "    \n",
    "else:\n",
    "    print(\"데이터셋이 없어 모델 테스트를 건너뜁니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (선택) Hugging Face 업로드 및 추론 테스트\n",
    "\n",
    "훈련이 완료된 모델을 Hugging Face Hub에 업로드하고, 간단한 추론 테스트를 통해 성능을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_and_test(model_path):\n",
    "    # Hugging Face 로그인\n",
    "    if config.hf_token:\n",
    "        login(token=config.hf_token)\n",
    "    \n",
    "    # 모델 병합 및 업로드\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(config.base_model, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    peft_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    \n",
    "    repo_name = f\"{config.hf_username}/{config.model_name}\"\n",
    "    merged_model.push_to_hub(repo_name)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.push_to_hub(repo_name)\n",
    "    \n",
    "    logger.info(f\"✅ 모델을 {repo_name}에 업로드했습니다.\")\n",
    "    \n",
    "    # 추론 테스트\n",
    "    logger.info(\"🔍 추론 테스트 시작...\")\n",
    "    pipe = pipeline(\"text-generation\", model=repo_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": \"창고에서 피킹 효율을 높이는 방법 3가지를 알려주세요.\"}]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    \n",
    "    print(\"\\n--- 추론 결과 ---\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"-----------------\")\n",
    "\n",
    "# 훈련 완료 후 실행\n",
    "# final_model_path = os.path.join(config.output_dir, \"final\")\n",
    "# upload_and_test(final_model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.6 (NGC 25.01/Python 3.12) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
