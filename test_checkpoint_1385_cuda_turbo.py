#!/usr/bin/env python3
"""
SOLAR-10.7B checkpoint-1385 CUDA í„°ë³´ ìµœì í™” ë²„ì „
ë°ì´í„°ì…‹ ì „ëžµ ìˆ˜ë¦½ì„ ìœ„í•œ ê³ ì† í…ŒìŠ¤íŠ¸
"""

import torch
import torch.cuda
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import warnings
import time
import gc
warnings.filterwarnings('ignore')

# CUDA ìµœì í™” ì„¤ì •
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

class CUDATurboSOLAR:
    def __init__(self, checkpoint_path="/home/work/tesseract/solar-korean-output/checkpoint-1385"):
        self.checkpoint_path = checkpoint_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(0.95)
            
    def load_model_turbo(self):
        """CUDA í„°ë³´ ëª¨ë¸ ë¡œë”©"""
        print("ðŸš€ CUDA í„°ë³´ ë¡œë”© ì‹œìž‘...")
        print(f"GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}")
        
        load_start = time.time()
        
        # ê³µê²©ì ì¸ ì–‘ìží™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            "upstage/SOLAR-10.7B-v1.0",
            device_map="auto",
            quantization_config=bnb_config,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        self.model = PeftModel.from_pretrained(
            base_model, 
            self.checkpoint_path,
            torch_dtype=torch.bfloat16
        )
        
        # ì¶”ë¡  ìµœì í™”
        self.model.eval()
        
        # CUDA ìµœì í™” ì„¤ì •
        if hasattr(self.model, 'config'):
            self.model.config.use_cache = True
            
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # ì²« ë²ˆì§¸ ë”ë¯¸ ì‹¤í–‰ (CUDA ì›Œë°ì—…)
        self._warmup()
        
        load_time = time.time() - load_start
        print(f"âœ… í„°ë³´ ë¡œë”© ì™„ë£Œ! ({load_time:.1f}ì´ˆ)")
        
        return load_time
        
    def _warmup(self):
        """CUDA ìºì‹œ ì›Œë°ì—…"""
        print("ðŸ”¥ CUDA ì›Œë°ì—… ì¤‘...")
        dummy_input = self.tokenizer("ì•ˆë…•", return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            _ = self.model.generate(
                **dummy_input,
                max_new_tokens=5,
                do_sample=False,
                use_cache=True
            )
        print("ðŸ”¥ ì›Œë°ì—… ì™„ë£Œ!")
        
    def turbo_generate(self, question, max_tokens=50):
        """CUDA í„°ë³´ ìƒì„±"""
        
        # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"ì§ˆë¬¸: {question}\në‹µë³€:"
        
        # í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„)
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=400,
            padding=False
        ).to(self.device)
        
        gen_start = time.time()
        
        # í„°ë³´ ìƒì„± (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):  # Mixed precision
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.3,
                    top_p=0.7,
                    top_k=25,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    use_cache=True,
                    num_beams=1,  # ë¹” ì„œì¹˜ ë¹„í™œì„±í™”
                )
        
        gen_time = time.time() - gen_start
        
        # ì‘ë‹µ ì¶”ì¶œ
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # ì‘ë‹µ ì •ë¦¬
        if "ë‹µë³€:" in response:
            response = response.split("ë‹µë³€:")[-1].strip()
        if "ì§ˆë¬¸:" in response:
            response = response.split("ì§ˆë¬¸:")[0].strip()
            
        return response, gen_time
    
    def batch_generate(self, questions, max_tokens=50):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì§ˆë¬¸ ë™ì‹œ ì²˜ë¦¬"""
        
        prompts = [f"ì§ˆë¬¸: {q}\në‹µë³€:" for q in questions]
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            truncation=True,
            max_length=400,
            padding=True
        ).to(self.device)
        
        batch_start = time.time()
        
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.3,
                    top_p=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    num_beams=1
                )
        
        batch_time = time.time() - batch_start
        
        # ë°°ì¹˜ ì‘ë‹µ ë””ì½”ë”©
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            if "ë‹µë³€:" in response:
                response = response.split("ë‹µë³€:")[-1].strip()
            responses.append(response)
            
        return responses, batch_time

def cuda_performance_test():
    """CUDA ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    print("ðŸš€ SOLAR-1385 CUDA í„°ë³´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    turbo = CUDATurboSOLAR()
    load_time = turbo.load_model_turbo()
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸
    print("\nðŸ”¸ ê°œë³„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    questions = [
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
        "íŒŒì´ì¬ í”„ë¡œê·¸ëž˜ë°ì´ëž€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ê¹€ì¹˜ì°Œê°œ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì¸ê³µì§€ëŠ¥ì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¢‹ì€ ì±…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”."
    ]
    
    individual_times = []
    
    for i, question in enumerate(questions, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {question}")
        
        try:
            answer, gen_time = turbo.turbo_generate(question, max_tokens=80)
            individual_times.append(gen_time)
            
            print(f"ë‹µë³€: {answer}")
            print(f"â±ï¸ ìƒì„±ì‹œê°„: {gen_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
        
        print("-" * 50)
    
    # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    print("\nðŸ”¸ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    batch_questions = [
        "ì„œìš¸ì€?", "íŒŒì´ì¬ì€?", "ê¹€ì¹˜ëŠ”?", "AIëž€?", "ì±… ì¶”ì²œ?"
    ]
    
    try:
        batch_answers, batch_time = turbo.batch_generate(batch_questions, max_tokens=30)
        
        print(f"ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ({len(batch_questions)}ê°œ ì§ˆë¬¸):")
        for q, a in zip(batch_questions, batch_answers):
            print(f"  Q: {q} â†’ A: {a}")
        print(f"â±ï¸ ë°°ì¹˜ ì²˜ë¦¬ì‹œê°„: {batch_time:.2f}ì´ˆ")
        print(f"ðŸ“Š ì§ˆë¬¸ë‹¹ í‰ê· : {batch_time/len(batch_questions):.2f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    # ì„±ëŠ¥ ìš”ì•½
    if individual_times:
        avg_individual = sum(individual_times) / len(individual_times)
        print(f"\nðŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ëª¨ë¸ ë¡œë”©: {load_time:.1f}ì´ˆ")
        print(f"   ê°œë³„ í‰ê· : {avg_individual:.2f}ì´ˆ")
        print(f"   ìµœê³ ì†ë„: {min(individual_times):.2f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {1/avg_individual:.1f} ì‘ë‹µ/ì´ˆ")

def interactive_turbo_chat():
    """í„°ë³´ ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸"""
    
    turbo = CUDATurboSOLAR()
    turbo.load_model_turbo()
    
    print("\nðŸš€ CUDA í„°ë³´ ì±„íŒ… (ì¢…ë£Œ: 'quit')")
    print("ðŸŽ¯ ë°ì´í„°ì…‹ ì „ëžµ ìˆ˜ë¦½ìš© ê³ ì† í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    while True:
        question = input("\nðŸ’­ ì§ˆë¬¸: ").strip()
        
        if question.lower() in ['quit', 'ì¢…ë£Œ', 'q', 'exit']:
            print("ðŸ í„°ë³´ ì±„íŒ… ì¢…ë£Œ!")
            break
            
        if not question:
            continue
            
        start_time = time.time()
        
        try:
            answer, gen_time = turbo.turbo_generate(question, max_tokens=100)
            total_time = time.time() - start_time
            
            print(f"ðŸ¤– ë‹µë³€: {answer}")
            print(f"âš¡ ì²˜ë¦¬ì‹œê°„: {total_time:.2f}ì´ˆ (ìƒì„±: {gen_time:.2f}ì´ˆ)")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "chat":
        interactive_turbo_chat()
    else:
        cuda_performance_test()
